id	sentence1	sentence2	label
train_81078	The models of the previous sections provide a variety of options for representing the meaning of a verb from its arguments.	"none of these constructions takes into account the distributional vector of the verb itself, which includes valuable information that could further help in entailment tasks."	contrasting
train_115708	"One concern to the bootstrapping approach in this paper is that it takes time to work with, which will make it difficult to be incorporated into language applications that need to responses in real time."	we believe that such an approach can be used in offline contexts.	contrasting
train_85809	"Modern RNN architectures have been very successful in solving character-level NLP tasks (Chung et al., 2016b;Golub and He, 2016;Mikolov et al., 2012)."	they do not make the learned linguistic structure explicit: rather it can be presumed to be cryptically encoded in the states of the hidden layers.	contrasting
train_215132	"The recall increases with decreasing the sentence length, while the false discovery rate (FDR) decreases."	the recall for the grammatical patterns is low for all instructions.	contrasting
train_85987	In case of baseline B2 we used rather a crude way of capturing the stance wisdom by counting different stance types.	"collective stance might obey some specific patterns of development, as indicated by Mendoza et al. (2010), and capturing these patterns is an important factor."	contrasting
train_12959	"Simple-Expansion Min-Expansion could, to some degree, describe the syntactic relationships between the connective and arguments."	"the syntactic properties of the argument pair might not be captured, because the tree structure surrounding the argument is not taken into consideration."	contrasting
train_204820	The wheat straw in (9) changes its state when it is shattered.	"""shattered"" is not an absolute specification, but can be specified ""in greater degrees of accuracy"" (Gruber 1985: 82)."	contrasting
train_159949	"The selection of a summary-worthy subset of all extracted concepts and relations was largely ignored in previous work, as many studies did not have a focus on summarization."	"when dealing with larger document clusters, this step becomes inevitable."	contrasting
train_55859	We addressed the normalization problem using a constrained linear solver and the crossentropy problem using numerical optimization.	our experiments showed the difference in WSD performance to be less than 1% in each case.	contrasting
train_154358	It is obvious that the concepts related to airplane should have the same relation with plane.	it is not the case in ConceptNet.	contrasting
train_146654	"A recent empirical study (Siddhant and Lipton, 2018) investigating active learning in NLP suggests that Bayesian active learning outperforms classical uncertainty sampling across all settings."	the approaches have been limited to rel-atively small datasets.	contrasting
train_78646	AnalysePoems is another tool for identification of metrical patterns written by Plamondon (2006).	"with other programs, its main goal is not to perform a perfect scansion, but to only identify the predominant meter in a poem."	contrasting
train_107560	"Thus, a spell checker built according to this formulation could suggest the correction detroittigers because this alternative occurs frequently enough in the employed query log."	"detroittigers itself could be corrected to detroit tigers if presented as a stand-alone query to this spell checker, based on similar query-log frequency facts, which naturally leads to the idea of an iterative correction approach."	contrasting
train_77446	"Now that we have explored the role of syntax in this project, our next step is try to further improve our QE system by adding semantic information."	there are many other ways in which the research in this paper could be further extended.	contrasting
train_89926	Our experiments were limited by the size of the labeled data.	"the results support the theoretical predictions, and demonstrate the advantage of posterior-decoding over Viterbi decoding."	contrasting
train_36060	"Second, T-SRI has certain effectiveness for learning prerequisite relations, with F 1 ranging from 62.1 to 65.2%."	"t-SRI only considers relatively simple features, such as the sequential and co-occurrence among concepts."	contrasting
train_151301	"For well-defined IRs such as relational database retrieval (E. Levin et al., 2000), significant words (=keywords) are obvious."	"determining significant words for more general IR task (T. Misu et al., 2004) (C.Hori et al., 2003) is not easy."	contrasting
train_19334	"For example, after decoding with BMES, 4 consecutive characters associated with the tag sequence BMME compose a word."	"after decoding with IB, characters associated with BIII may compose a word if the following tag is B or only form part of a word if the following tag is I."	contrasting
train_11172	"Interestingly, (political catoons) are among these Chinese queries improved most by English ranking, which is believed as rare (or sensitive) content on Chinese web."	top English queries are short of this type of queries.	contrasting
train_21240	We assumed that the type and number of emotions are pre-defined and our approach was based on this assumption.	"in previous research, there is little agreement about the number and types of basic emotions."	contrasting
train_28850	"In existing parsers, features are commonly exploited from the parsing history, such as the top k elements on the stack."	such features are expensive in terms of search efficiency.	contrasting
train_9164	"These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles ."	a mention of an entity contains more information than just its head and syntactic role.	contrasting
train_187439	We assume the following ordering exists: authoritative behavior > motivational behavior > negative deference > positive deference in the opposite direction > closeness.	we do not assume such an ordering for the SC Leadership.	contrasting
train_124775	"Since the exact boundaries of the spans are hard to define even for human annotators (Wiebe et al., 2005a;Yang and Cardie, 2013), the target span in MPQA 2.0 could be a single word, an NP or VP, or a text span covering more than one constituent."	"in MPQA 3.0, each target is anchored to the head of an NP or VP, which is a single word."	contrasting
train_131798	"We also try to choose more effective examples for self-training and tri-training, by selecting training instances according to the base segmentation model score."	the segmentation performances do not get improved.	contrasting
train_109247	"This work bears some similarity to the substantial literature on automatic subcategorization frame acquisition (see, e.g., Manning (1993), Briscoe and Carroll (1997), and Korhonen (2002))."	"that research is focused on acquiring verbs' syntactic behavior, and we are focused on the acquisition of verbs' linking behavior."	contrasting
train_123164	"Note that our method is similar to Active Learning (Tong and Koller, 2001), in that both automatically identify which unlabeled instances the human should annotate next."	"in active learning, the goal is to find instances that are difficult for a supervised learning system."	contrasting
train_165032	Incorporating this feedback in the development process may have influence on the success of the app.	one challenge for the developers is to deal with the overwhelming amount of reviews.	contrasting
train_71748	Existing systems which evaluate each passage separately against the question would view each passage as having a similar degree of support for either hypertrophic cardiomyopathy or aortic stenosis as the answer to the question.	"these systems lose sight of a crucial fact, namely, that even though each passage covers half of the facts in the question, (2.1 a) and (2.1 b) cover disjoint subsets of the facts, while (2.2 a) and (2.2 b) address the same set of facts."	contrasting
train_219494	We therefore regard these as parsing errors that we will train our model to avoid.	avoiding the Left-Arc transitions would require the parser to predict that the head is disfluent when it has not necessarily seen any evidence indicating that.	contrasting
train_15503	"However, Snow et al. assume the existence of an oracle which provides the senses of each word."	conceptResolver automatically determines the number of senses for each word.	contrasting
train_102501	"At the largest training data sizes, modeling all 4 features together results in the best predictions of inflection."	"using 4 separate models is worth this minimal decrease in performance, since it facilitates experimentation with the CRF framework for which the training of a single model is not currently tractable."	contrasting
train_9167	"Their method of combination is quite different from ours; they use the system's judgements to define the ""entities"" whose repetitions the system measures."	"we do not attempt to use any proposed coreference links; as Barzilay and Lapata (2005) point out, these links are often erroneous because the disorded input text is so dissimilar to the training data."	contrasting
train_96492	"Taghipour and Ng (2016) tried to attend over words on their one-layer LSTM model, but failed to beat the baseline model that employs mean-over-time pooling, because of that text-level model contains a quite long sequence of words, which may weaken the effect of attention."	"sentence-level model contains relatively short sequences of words, which makes attention more effective."	contrasting
train_154362	"Therefore some recent work (Mihalcea, 2007; Ponzetto&Navigli, 2010) exploits Wikipedia, a large collaborative Web encyclopedia, to extract the knowledge for WSD."	the type of semantic relations extracted from Wikipedia is uncertain.	contrasting
train_206368	"The argument is of a simple, brute-force nature and takes into account various alternatives including even the ones which might be dismissed as implausible without a moment's thought."	"it clearly shows, by utilizing the locality requirements imposed on phrasal idioms, that the causative and inchoative verbs in the causative alternation are best analyzed in terms of the common base approach."	contrasting
train_33238	"The input gate by allowing incoming signal to alter the state of the memory cell, regulates proportion of history information memory cell will keep."	the output gate regulates what proportion of stored information in the memory cell will influence other neurons.	contrasting
train_71753	"A merger strategy that takes maximum across passages will choose M AX (s 1.1 , s 1.2 ) as the optimal supporting passage."	"since these passages have complementary information to offer, it would be better to somehow aggregate this information."	contrasting
train_183420	"First, the model learns that high edit distance is predictive of a mismatch."	singleton strings that do not match often have a lower edit distance than longer strings that do match.	contrasting
train_169460	"This pipeline architecture is appealing for various reasons, including modularity, modeling convenience, and manageable computational complexity."	it suffers from the error propagation problem: errors made in one sub-task are propagated to the next sub-task in the sequence.	contrasting
train_38579	"All previous works converged to a shared assessment: both CNNs and RNNs provide relevant, but different kinds of information for text classification."	"though several works have studied linguistic structures inherent in RNNs, to our knowledge, none of them have focused on CNNs."	contrasting
train_31737	"Another avenue for providing user confidence is probabilistic calibration (Platt, 1999), which has been explored more recently for structured prediction (Kuleshov and Liang, 2015)."	these methods do not guarantee precision for any training set and test input.	contrasting
train_194614	"The broad success of CWRs indicates that they encode useful, transferable features of language."	their linguistic knowledge and transferability are not yet well understood.	contrasting
train_155139	"This means that, for instance, any occurrence of the verb charge, such as in the expressions charge a fee or charge a battery, is assigned the same vector representation, ignoring the difference of word sense."	"the fact that charge and impose are near-synonyms in charge/impose a fee will not be properly reflected in their respective meaning vectors, since the former, but not the latter, includes (context words reflecting) the ""supply electricity"" sense of charge."	contrasting
train_150979	Its adaptation-guided retrieval makes it ultimately similar to our system.	our approach differs from it in two respects.	contrasting
train_187440	We achieved a very high recall (close to 1.0) for most indicators with these rules on test data.	"in few cases, the frequency of such indicators (such as politeness) were very low deeming the set of regular expressions as incomplete."	contrasting
train_48632	"This provides clear evidence of M-BERT's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data."	"cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-BERT's multilingual representation is not able to generalize equally well in all cases."	contrasting
train_115340	"Indeed we find that this distribution is too sharp and overemphasises short phrases, so we use f C = 1."	it does allow us to rank target phrases as possible translations.	contrasting
train_10121	"Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007)."	"the CRF may consider the distance between head and child, whereas DMV does not model distance."	contrasting
train_11253	"We can retrieve positive examples from Web archive with high precision (but low recall) by manually augmenting queries with hypernyms or semantically related words (e.g., ""Loft AND shop"" or ""Loft AND stationary"")."	it is often costly to create negative examples.	contrasting
train_174390	These distinctions would be much more difficult to account for in textual format.	"it should not replace the textual resource altogether since other forms of information are better represented in textual format (definitions, contexts, etc.)."	contrasting
train_112699	"For instance, despite the fact that the sentences ""he is affected by AIDS"" and ""HIV is a virus"" express concepts closely related, their similarity is zero in the VSM because they have no words in common (they are represented by orthogonal vectors)."	"due to the ambiguity of the word ""virus"" , the similarity between the sentences ""the laptop has been infected by a virus"" and ""HIV is a virus"" is greater than zero, even though they convey very different messages."	contrasting
train_136568	"It is only recently that any work has introduced larger-scale novel metaphor annotations (Parde and Nielsen, 2018)."	"to our approach, they collect annotations on a relation level (see also Section 2)."	contrasting
train_103536	"Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally."	they do not enforce user consistency throughout the dialog.	contrasting
train_100786	"The ATS models do provide an integrated approach, but their lexical translation is limited to the word level."	"to prior work, we present a integrated approach that allows POS-based reordering and phrase translation."	contrasting
train_215978	Table 2 presents that combination of frequency based vectorizer and bigram model provided the highest F1 score result of 95.47 percent.	the lowest F1 score result we got among the above combinations was 90.9 percent and was obtained from tf-idf and trigram model.	contrasting
train_194717	"Back-translation has been dominantly used in these approaches, where pseudo sentence pairs are generated to train the translation systems with a reconstruction loss."	it is inefficient because the generated pseudo sentence pairs are usually of low quality.	contrasting
train_98519	Their ability to understand user spoken commands and identify the user's intent(s) is one of their main benefits.	this is a very challenging task due to the diversity of domains and languages they are required to support.	contrasting
train_109245	"The two main handtagged corpora are PropBank (Palmer et al., 2003) and FrameNet (Baker et al., 1998), the former of which currently has broader coverage."	"even PropBank, which is based on the 1M word WSJ section of the Penn Treebank, is insufficient in quantity and genre to exhibit many things."	contrasting
train_115882	"Clearly, this baseline is expected to perform worse than both our model and the universal types one since those are able to cover most of the sentences and thus, they are likely to produce more correct dependency relations."	it gives us an idea how much extra quality is gained when coverage improves.	contrasting
train_69247	"In fact, Fellbaum (1998) allows for more than one unique beginner per verb category."	cases where there is a large number of unique beginners in one category merit investigation.	contrasting
train_62311	"From the computational perspective, ε-entries represent the boundary between the results in Section 3 and Section 4."	"because we do not know whether the classes NP and EXPTIME can be separated, we cannot draw any precise conclusion about the role of ε-entries in the parsing problem."	contrasting
train_209172	"While the Air Traffic Services (ATS) of the CAAP agreed to accommodate interviews with the pilots and ATCS, it could not provide or release copies of the conversation transcripts."	"due to strong requisition of the study, the ATS released only three transcripts, ensuring that the airline companies remained anonymous."	contrasting
train_167935	Secondary data usually works as prime data in publication.	"in a language study, primary data itself is prime data in publication."	contrasting
train_165037	The number of subjective phrases within the different application categories is relatively constant.	the number of application aspects (from 149 to 287) and relations (104 to 237) varies much more.	contrasting
train_3575	"In contrast, RULE, SYM, and LE features did not affect the accuracy."	"if each of them was removed together with another feature, the accuracy decreased drastically."	contrasting
train_103158	Approaching temporal link labelling as a classification task has already been explored in several works.	"choosing the right feature vectors to build the classification model is still an open issue, especially for event-event classification, whose accuracy is still under 50%."	contrasting
train_35402	"The lexicon designers then manually contructed lexicons for each category, augmenting their intuitions by using distributional statistics to suggest words that may have been missed (Pennebaker et al., 2015)."	"we follow the approach of Biber (1991), using multidimensional analysis to identify latent groupings of markers based on co-occurrence statistics."	contrasting
train_45456	"In the left path, the model resolves adjacent arcs first."	"in the right path, distant arcs that rely on the global structure are resolved first."	contrasting
train_127181	"In this variant, word embeddings are used, as we are proposing in this paper, to map text content within generated summaries to SCUs."	"the SCUs still need to be manually identified, limiting this variant's scalability and applicability."	contrasting
train_203056	"Putting all of this evidence together, most transformational approaches claim that Japanese epistemic verb construction involves the RTO movement."	each piece of evidence that they adduce is indirect and shows that the accusative argument may undergo raising from an embedded clause.	contrasting
train_146662	"We observe that across queries (âˆ©Q), FTZ with entropy strategy has a balanced representation from all classes (high mean) with a high probability (low std) while Multinomial Naive Bayes (MNB) results in more biased queries (lower mean) with high probability (high std) as studied previously."	we did not find evidence of class bias in the resulting sample (âˆ©S) in both models: Fast-Text and Naive Bayes (column 5 and 6 from Table  2).	contrasting
train_9220	"Efficiency is maintained because such arbitrary disjunction is not needed to encode the most common forms of uncertainty, and thus the number of MDP states in the set can be kept small without losing accuracy."	allowing multiple MDP states provides the representational mechanism necessary to incorporate multiple speech recognition hypotheses into the belief state representation.	contrasting
train_85973	The dataset consists of rumours that emerged during eight different events.	three of these events evoked less than five rumours consisting of five or more tweets.	contrasting
train_182390	"In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes."	human judgment still plays a large role in the context of evaluating MT systems.	contrasting
train_118307	"We present the complete approach in three parts by describing the factored representation of the lexicon (Section 5), techniques for proposing potential new lexemes and templates (Section 6), and finally a complete learning algorithm (Section 7)."	the next section first reviews the required background on semantic parsing with CCG.	contrasting
train_218070	"For example, the WASH event in the TAKING A BATH scenario is often broken up into three mentions: wetting the hair, applying shampoo, and washing it again."	"because there is only one event type for the three mentions, this sequence is never observed in DeScript."	contrasting
train_16915	These methods usually learn a single model given a training set.	single models cannot deal with words from multiple language origins.	contrasting
train_39062	"Indeed, we can also observe that another CNN-based baseline, i.e., CNN-ASP implemented by us, also obtains good results on TWITTER."	the performance of those comparison methods is mostly unstable.	contrasting
train_85968	"In the early stages of a rumour, its actual veracity tends to be unknown."	"as new evidence emerges over time, Twitter users take more pronounced and continuously evolving stance towards the information asserted in the rumour."	contrasting
train_15204	"For example, (Hu and Liu, 2004;Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text."	opinion summarization in spontaneous conversation is seldom studied.	contrasting
train_215131	"For example, in the sentences “Simmer (the sauce) until thickened. Add the pork, mix well for one minute.”using a grammatical pattern we will discover that simmer causes thickened (through the causal link until)."	"it will not discover that the sauce has to thicken, in order to add the pork."	contrasting
train_146649	"Deep neural networks (DNNs) trained on large datasets provide state-of-the-art results on various NLP problems (Devlin et al., 2019) including text classification (Howard and Ruder, 2018)."	the cost and time needed to get labeled data and to train models is a serious impediment to creating new and/or better models.	contrasting
train_136575	"For example, in this way the metaphor ""[...] the artistic temperament which kept her tight-coiled as a spring [...]"" (0.514) is treated as novel, while ""To quench [thirst] is more than to refresh [...]"" (0.424) is treated as conventionalized."	"since we provide the scores, this threshold can be adjusted to suit a given application."	contrasting
train_219475	It is encouraging to note that the new system achieves this performance without using any of the carefully-chosen heuristics employed by the previous method.	we do note that some of these techniques can be easily combined with our method to produce further improvements.	contrasting
train_91712	"Even though in the Wikipedia domain the TK+BF score is less than the baseline score, still the performance of the classifiers do not fall much in any of the in-domain and cross-domain experiments."	bF does not have a good performance in 5 of 6 the experiments.	contrasting
train_19398	"The most related work to ours is the boostVSM introduced by (He et al., 2007b), it proposes to weight different term dimensions with corresponding bursty scores."	it is still based on term dimensions and fails to deal with terms with multiple bursts.	contrasting
train_76485	One approach for building such a multilingual semantic parsing system is to develop a joint generative process from which both the semantic representations and the sentences in different languages are generated simultaneously.	building such a joint model is non-trivial.	contrasting
train_89918	"We tried numerous features that compare MeSH terms based on their distance in the ontology, and other features that indicate whether a word is part of a longer term."	none of these feature were selected for the final system.	contrasting
train_153282	Early studies on PPI extraction employ feature-based methods.	"the featurebased methods often fail to effectively capture the structured information, which is essential to identify the relationship between two proteins in a constituent or dependency-based syntactic representation."	contrasting
train_26480	Most previous topic labelling approaches focus on topics derived from well formatted and static documents.	"in contrast to this type of content, the labelling of topics derived from tweets presents different challenges."	contrasting
train_112153	Filtered Espresso halted after the seventh iteration (Filtered Espresso (optimal stopping)) is comparable to the proposed methods.	"in bootstrapping, not only the number of iterations but also a large number of parameters must be adjusted for each task and domain."	contrasting
train_89915	"Other utility functions, such as Dice, Jaccard and Hamming can be used as U set agreement ."	only metric-based utility functions will result in a metric-based U AM A utility function.	contrasting
train_62310	"The computational effect of grammar structure and grammar size on the parsing problem is rather well understood for several formalisms currently used in computational linguistics, including context-free grammar and TAG."	"to the best of our knowledge, this problem has not been investigated before for VW-CCG or other versions of CCG; see, for instance, Kuhlmann and Satta (2014) for discussion."	contrasting
train_174553	Systems capable of summarizing live streams of heterogeneous content can be directly beneficial to users and even assist journalists during their daily work.	this new task also comes with new challenges.	contrasting
train_194718	"Recently, neural-based methods (Chu et al., 2016; Grover and Mitra, 2017; GrÃ©goire and Langlais, 2018) aim to select potential parallel sentences from monolingual corpora in the same domain."	"these neural models need to be trained on a large parallel dataset first, which is not applicable to language pairs with limited supervision."	contrasting
train_36259	"The recently released MS Marco dataset (Nguyen et al., 2016) also contains independently authored questions and documents drawn from the search results."	the questions in the dataset are derived from search logs and the answers are crowdsourced.	contrasting
train_17739	Van den Bosch 2005 proposes a decision-tree classifier which has been applied to training datasets with more than 100M words.	his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn't possible.	contrasting
train_85984	Results in Table 2 are obtained using gold stance labels.	this restricts the idea of stance-based rumour verification to only data where human stance labels are available.	contrasting
train_114004	"For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in Figure 2b) in which the two instances of Jeep are aligned."	"if Hypothesis 1 is aligned to the backbone first, we would still get the CN in Figure 2a)."	contrasting
train_219498	"If it turned out that almost all of the former leftward children of disfluent words are subsequently marked as disfluent, there would be little point in returning them to the stack -we could just mark them as disfluent in the original Edit transition."	"if they are almost all marked as fluent, perhaps they can just be attached as children to the first word of the buffer."	contrasting
train_213636	"In examining the results, we find that the correlation of our experiments with PYRAMID, at the micro-evaluation level, is not high enough; in spite of this, it is greater than standard metrics and ROUGE-1."	our experiments give a good correlation with PYRAMID at the macroevaluation level.	contrasting
train_93758	"From the above experiments, we conclude that tasks sensitive to under-estimation should use the CM-CU sketch, which guarantees over-estimation."	"if we are willing to make some underestimation error with less over-estimation error, then LCU-WS and LCU-SWS are recommended."	contrasting
train_145225	"Various statistical methods have also been proposed to filter out the mistakes or (spamming) random responses of the â€  Corresponding author crowdsource workers (Liu et al., 2012;Hovy et al., 2013;Nguyen et al., 2017)."	the way to filter out the mistakes or the random responses through statistical means is difficult to utilize for a fundamentally subjective annotation task.	contrasting
train_48633	This provides further evidence that M-BERT uses a representation that is able to incorporate information from multiple languages.	"m-BERT is not able to effectively transfer to a transliterated target, suggesting that it is the language model pre-training on a particular language that allows transfer to that language."	contrasting
train_120732	"To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012)."	combining the sparse modeling and spectral graphical modeling approaches in a principled way is challenging.	contrasting
train_170610	"Generally speaking, only the factual information with high credibility has the value for use."	most expressions in social media are published with the hypothesis and episteme which cannot be decided whether it is true or false at that moment.	contrasting
train_135179	"Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a;Cotterell and Duh, 2017;Lin et al., 2018)."	"such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language."	contrasting
train_88784	Figure 2 shows that the performance is almost constant on ungrammatical data in the important sentence length range from 5 to 40.	there is a negative correlation of accuracy and sentence length for grammatical sentences.	contrasting
train_135824	Word-by-word translation always outputs a target word for every position.	"there are a plenty of cases that multiple source words should be translated to a single target word, or that some source words are rather not translated to any word to make a fluent output."	contrasting
train_192652	"Example 11 shows the advantage of combining the vector representation with orthographic distance, i.e., our model could find translations of sleddogs that have similar meaning, while in examples 12 and 13 orthographic distance helped to pick the correct translation which is the closest in terms of edit distance."	in example 14 orthographic distance caused an error because the incorrect prediction is too close to the source word in orthographic distance.	contrasting
train_17717	"There are several different theories about relative prominence assignment in noun-noun (henceforth, NN) compounds, such as the structural theory (Bloomfield, 1933;Marchand, 1969;Heinz, 2004), the analogical theory (Schmerling, 1971;Olsen, 2000), the semantic theory (Fudge, 1984;Liberman and Sproat, 1992) and the informativeness theory (Bolinger, 1972;Ladd, 1984)."	"in most studies, the different theories are examined and applied in isolation, thus making it difficult to compare them directly."	contrasting
train_174555	"We took a snapshot of this page that provided us with 16,246 unique live blogs."	the BBC website has no such live blog archive.	contrasting
train_198950	"For instance, the English word 'wear' could be used to discover the seeds in an English-Chinese aligned corpus."	gaps between English and Chinese may produce incomplete sets of seeds.	contrasting
train_203051	"Chomsky, on the other hand, argues that the derivation is either by the Exceptional Case-marking (ECM) process under S-deletion (Chomsky, 1981) or by IPcomplementation (Chomsky, 1986)."	his motivation for not accepting the RTO analyses is mainly theoretical.	contrasting
train_32114	"This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations."	"the previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity."	contrasting
train_53935	"Without SVD (compare column 1 to 2 in Table 17), performance drops, but the drop is not statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990)."	we hypothesize that the drop in performance would be significant with a larger set of word pairs.	contrasting
train_159232	"Word representations based on the distributional hypothesis of Harris (1954) have become a dominant approach including word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which show remarkable performances in a wide spectrum of natural language processing."	a question arises about a relationship between a true word meaning and its distributed representation.	contrasting
train_55516	"As can be seen, some of the top features are either too specific (landlocked, airspace), and are thus less reliable, or too general (destination, ambition), thus not indicative and may co-occur with many different types of words."	"intuitively more characteristic features of country, like population and governor, occur further down the sorted feature list, at positions 461 and 832."	contrasting
train_32108	"The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014)."	these models do not show particular advantages on other datasets.	contrasting
train_137509	"As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD."	the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.	contrasting
train_156851	Dependency parsing based methods have achieved much success in SRL.	"due to errors in dependency parsing, there remains a large performance gap between SRL based on oracle parses and SRL based on automatic parses in practice."	contrasting
train_73910	"When a voting strategy is used, ACA is ahead with 1.75% compared to Degree."	"it is important to note that when looking a the scores per part of speech, Degree exhibits notably higher results for nouns (85% versus 76.35%), while ACA performs much better for adverbs adjectives and verbs (83.98%, 82.44%, 74.16%)."	contrasting
train_104515	"The content selection discussed here is analogous to the selection of semantic attributes (type, color, size, etc) when generating a description of an entity (Dale and Haddock, 1991; Dale and Reiter, 1995)."	"instead of attributes, the content selection step in our model aims to choose the form of a proper name reference (which kind(s) of name and modifier(s) are part of the proper name reference)."	contrasting
train_64441	"Since large tagged corpora in Bulgarian are not widely available, the development of a corpus-based probabilistic tagger was an unrealistic goal for us."	"as some studies suggest (Voutilainen, 1995), the precision of rule-based taggers may exceed that of the probabilistic ones."	contrasting
train_46109	"Previous work in the context of phrase-based statistical machine translation (DaumÃ© III and Jagarlamudi, 2011) has noted that unseen (OOV) words account for a large portion of translation errors when switching to new domains."	"this problem of OOV words in cross-domain transfer is under-examined in the context of NMT, where both training methods and experimental results will differ greatly."	contrasting
train_81271	"More recently, Gong and Zhang (2016) propose an attention-based convolutional neural network, which incorporates a local attention channel and global channel for hashtag recommendation."	"to the best of our knowledge, there is no work yet on employing both topic models and deep neural networks for this task."	contrasting
train_97431	Entity linking in long text has been well studied in previous work.	short text entity linking is more challenging since the texts are noisy and less coherent.	contrasting
train_180705	"Because the CMQD model can easily hypothesize implausible degradations, we see the MAP increases modestly with a few degradations, but then MAP decreases."	the MAP of the phrase-based system (PBQD-Fac) increases through to 500 query degradations using multigrams.	contrasting
train_198040	"The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowledge from the parent task, thereby improving the performance on the child task."	the divergence between the source and the assisting language can adversely impact the benefits obtained from transfer learning.	contrasting
train_99483	"Surprisingly, students had initiative more of the time in the didactic dialogues (21% of the turns) than in the Socratic dialogues (10% of the turns), and there was no direct relationship between student initiative and learning."	"socratic dialogues were more interactive than didactic dialogues as measured by percentage of tutor utterances that were questions and percentage of words in the dialogue uttered by the student, and interactivity had a positive correlation with learning."	contrasting
train_74554	"Furthermore, in the second example, our PASbased method successfully recognizes the [AM-TMP] argument ""2005å¹´"" and move it to the end of sentence."	the BTG system only performs translation without any reordering.	contrasting
train_97451	"However, the soft-TF information in the descriptions of the two entities is similar."	m-CNN captures the whole meaning of the text and links the mention to the correct entity.	contrasting
train_210381	"Generally, languages acquired after the first language (L1) are usually called second languages (L2)."	"consensus has not yet been reached on the definition of the term 'the third language (L3)' (De Angelis, 2007)."	contrasting
train_205673	"Moreover, due to difficulties in utterance disentanglement similarily shown in Elsner and Charniak (2008)), we expect reduced effectiveness over our data of such information (although some degree of interaction exists)."	"to partly help with disentanglement, we noticed that some users mentioned the user name(s) of the users they are responding to in their posts, which allows us to identify the utterances they link to."	contrasting
train_223497	"As a result, their model needs to re-encode the partial sequence at every step, which is computationally more expensive."	our approach does not necessitate reencoding the entire sentence during generation.	contrasting
train_122591	"Motivated by this observation, we hypothesize that the reasons mentioned in the preceding post could be useful for predicting the reasons in the current post."	none of the models we have presented so far makes use of the reasons predicted for the preceding post.	contrasting
train_62935	"In this work we follow up on the findings of Rabinovich, Ordan, and Wintner (2017), who, by using language representations consisting of manually specified feature vectors, find that the structure of a language representation space is approximately preserved by translation."	"their analysis only stretches as far as finding a correlation between their language representations and genetic distance, even though the latter is correlated to several other factors."	contrasting
train_108971	We observe that backward constituent alignment-based models (1–4) perform similarly to word-based projection models (the F-score ranges between 0.40 and 0.45).	they obtain considerably higher precision (albeit lower recall) than the word-based models.	contrasting
train_33	"This apparent progress in spoken language technology has been fuelled by a number of developments: the relentless increase in desktop computing power, the introduction of statistical modelling techniques, the availability of vast quantities of recorded speech material, and the institution of public system evaluations."	"our understanding of the fundamental patterning in speech has progressed at a much slower pace, not least in the area of its high-level linguistic properties."	contrasting
train_152047	The experimental results show that target language side information gives the best performance in the experimental setting.	there are no large differences among the different selection results.	contrasting
train_37674	An ever increasing amount of data is becoming available for NMT training.	only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance.	contrasting
train_115701	The unsupervised nature of the approach means good ability to deal with domain variation.	the approach did not show a segmentation performance as good as that of the supervised approach.	contrasting
train_65770	"Newswire text has long been a primary target for natural language processing (NLP) techniques such as information extraction, summarization, and question answering (e.g MUC (1998); NIS (2003); DUC (2003))."	"newswire does not offer direct access to facts, events, and opinions; rather, journalists report what they have experienced, and report on the experiences of others."	contrasting
train_155143	"Although all models have been evaluated on test-sets derived from the LST dataset in essentially the same way, the datasets differ slightly due to technical details, so strictly speaking the results cannot be compared directly."	"since all authors report similar scores for the random baselines, we assume that the complexity of the subsets used in previous work is more or less comparable."	contrasting
train_71759	"In this paper, we only considered merging evidence across passages and question terms."	this may be easily extended to merging evidence across passage scorers.	contrasting
train_100567	"First, as can be expected the ASR 1-best output is typically error-prone especially when a user query originates from a noisy environment."	aSR word confusion networks which compactly encode multiple word hypotheses with their probabilities have the potential to alleviate the errors in a 1-best output.	contrasting
train_5513	"In principle, w could have been tuned by maximizing conditional probability or maximizing margin."	"these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h."	contrasting
train_178382	"Given that storing all of these phrases leads to very large phrase tables, many research systems simply limit the phrases gathered to those that could possibly influence some test set."	"this is not feasible for true production MT systems, since the data to be translated is unknown."	contrasting
train_59588	"Eschewing theorizing to stay close to data permits a remarkably wide range of linguistic phenomena to be covered, and it is this that is the book's greatest strength."	"in a few places, a seemingly arbitrary theoretical perspective is assumed rather more tacitly than one might hope, with few hints as to alternative analyses (e.g., see the following remarks about parts of speech in Chapter 6)."	contrasting
train_49678	The DL model improves dramatically as the data size increases and achieves the best performance among the 7 models when 7000 training examples are available.	the other models suffer much less from data scarcity with an exception of Random Forest.	contrasting
train_6086	"Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004;Wu, 1997) and empirically successful (Chiang, 2005;Galley et al., 2006)."	"despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment."	contrasting
train_66747	"Finding the four nearest neighbors for each word in the collection, we calculated the average minimum similarity score that a pair of words must have in order to be considered related."	"since words vary a lot in terms of the amount of corpus data available on them, the average similarity threshold might be inappropriate for many words."	contrasting
train_98110	"These studies rely on human annotation of timestamped subtask goals, e.g., timed captions created through crowdsourcing."	humanin-the-loop annotation is infeasible to deploy for popular video sharing platforms like YouTube that receive hundreds of hours of uploads per minute.	contrasting
train_128801	"Automatically identifying users' emotional states from their texts and classifying emotions into finite categories such as joy, anger, disgust, etc., can be considered as a text classification problem."	it introduces a challenging learning scenario where multiple emotions with different intensities are often found in a single sentence.	contrasting
train_217587	"Although this task has been addressed in literature, most of the publications report analyses based on the written text only, usually at the phrase or sentence level."	a written text is not merely a set of words or sentences.	contrasting
train_91266	"In English, the capitalization of the phrase European Court of Auditors helps identify the span as a named entity."	"in German, all nouns are capitalized, and capitalization is therefore a less useful cue."	contrasting
train_23956	A simple lexicon-based constraint during inference time may also correct this case.	hard-constraint baselines can hardly improve the performance in general because the contributions of different constraints are not learned and their combination may not lead to better predictions.	contrasting
train_97966	"Difficulties in handling LDD have motivated the development of syntax-based MT (Yamada and Knight, 2001), that can effectively represent reordering at the phrase level, such as when translating between VSO and SOV languages."	"syntaxbased MT models remain limited in their ability to map between arbitrarily different word orders (Sun et al., 2009;Xiong et al., 2012)."	contrasting
train_4087	"Collins et al. (1997) and Miller et al. (2000) used statistical parsing models to extract relational facts from text, which avoided pipeline processing of data."	"their results are essentially based on the output of sentence parsing, which is a deep processing of text."	contrasting
train_130266	"To this end, we need to improve the quantitative empirical understanding of such reuse accompanied by qualitative empirical studies."	only few such works exist.	contrasting
train_155512	These context vectors are clustered and the resulting clusters are taken to represent the induced senses.	"when constructing context vectors, the approaches based on VSM assume that the words occurring in the contexts are independent and do not exploit semantic relevance between words."	contrasting
train_128793	"We note that the Topic Coherence (TC) metric in (Mimno et al., 2011) which is often used to approximate coherence in unigram topic models as it correlates with human notions of coherence, uses codocument frequency of individual words in topics."	"in our problem as phrases are sparse, their co-document frequency is far lower than words."	contrasting
train_90804	The simplest implementation would be to check the most recent model with the previous model and stop if their agreement exceeds the intensity cutoff.	"independent of wanting to provide users with a longevity control, this is not an ideal approach because there's a risk that these two models could happen to highly agree but then the next model will not highly agree with them."	contrasting
train_180764	The work in this phase is cubic in sentence length.	"lexical rules in LNF can be applied without binarization, because they only apply to particular spans that contain the appropriate lexical items."	contrasting
train_220099	"One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009)."	"while sense and topic are related, they are distinct linguistic phenomena."	contrasting
train_143698	These actions can be regarded as users' feedbacks that reflect users' interest.	"due to its implicitness, such feedback can only reflect a part of users' interest, causing inaccuracy in recommendation."	contrasting
train_200290	"With the rapid increase of students learning Japanese, there is a growing need for a quick but reliable and accurate assessment instrument in the field of teaching Japanese."	"at present, no such test exists."	contrasting
train_97967	"String-similarity metrics against a reference are known to be partial and coarsegrained aspects of the task (Callison-Burch et al., 2006), but are still the common practice in various text generation tasks."	"their opaqueness and difficulty to interpret have led to efforts to improve evaluation measures so that they will better reflect the requirements of the task (Anderson et al., 2016;Sulem et al., 2018;Choshen and Abend, 2018b), and to increased interest in defin-ing more interpretable and telling measures (Lo and Wu, 2011;Hodosh et al., 2013;Choshen and Abend, 2018a)."	contrasting
train_42155	"The proposed approach is close to (Liu et al., 2015), where only the annotated data for aspect extraction is used."	we will show that our approach is more effective even compared with baselines using additional supervisions and/or resources.	contrasting
train_160297	Word embeddings are a relatively new addition to the modern NLP researcher's toolkit.	"unlike other tools, word embeddings are used in a black box manner."	contrasting
train_123685	"An alternate technique is to use word-classing (Goodman, 2001;Mikolov et al., 2011), which can reduce the cost of exact normalization to O"	"our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation."	contrasting
train_62316	"Because we can also translate any TAG into an equivalent VW-CCG without blowing up the size of the grammar, following the construction by Vijay-Shanker and Weir (1994), we conclude that VW-CCG is more succinct than TAG."	the price we have to pay for this gain in expressivity is the extra parsing complexity of VW-CCG.	contrasting
train_183414	"One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler."	transliteration systems are complex and require significant training resources.	contrasting
train_166815	"FrameNet (Baker et al., 1998) is such a resource and provides fine-grained semantic relations of predicates and their arguments."	frameNet does not provide an explicit link to real-world fact types.	contrasting
train_67550	An obvious case is that of synonyms.	there are cases when different words are used as synonyms only in certain contexts.	contrasting
train_219785	Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired.	"it is not clear whether the semantic expressiveness of these approaches can eventually scale up to the casual, but highly expressive language people naturally use in their online activities."	contrasting
train_5514	"The problem with (c) is that the correspondence h contains an incorrect alignment (', a)."	"since h is unobserved, the training procedure has no way of knowing this."	contrasting
train_150755	"Normally, this task would be performed by a parser."	"since the CoNLL dataset contains no parsing information 6 and we did not want to use any resources not explicitly provided in the CoNLL data, we had to construct a PPA classifier to specifically perform this task."	contrasting
train_88529	"Tests on the PDT (Böhmovà et al., 2003) show that the added actions are sufficient to handle all cases of non-projectivity."	"since the cases of non-projectivity are quite rare in the corpus, the general learner is not supplied enough of them to learn how to classify them accurately, hence it may be worthwhile to exploit a second classifier trained specifically in handling non-projective situations."	contrasting
train_3388	Almost all of sense disambiguation methods are heavily dependant on manually compiled lexical resources.	"these lexical resources often miss domain specific word senses, even many new words are not included inside."	contrasting
train_155955	"As depicted in Graph (Figure 4), there is a average increase of 0.38 (LAS) on all datasets using the first sense strategy from the baseline."	using WSD the accuracy decreased across all datasets.	contrasting
train_195378	Globally normalized neural sequence models are considered superior to their locally normalized equivalents because they may ameliorate the effects of label bias.	"when considering high-capacity neural parametrizations that condition on the whole input sequence, both model classes are theoretically equivalent in terms of the distributions they are capable of representing."	contrasting
train_35752	"NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs (Junczys-Dowmunt et al., 2016) and becomes the new de facto method in practical MT systems ."	there still remains a severe challenge: it is hard to interpret the internal workings of NMT.	contrasting
train_94497	The baseline system uses an identical model for coreference resolution on both pronouns and nonpronominal mentions.	"in the literature (Bengtson and Roth, 2008;Rahman and Ng, 2011;Denis and Baldridge, 2007) the features for coreference resolution on pronouns and nonpronouns are usually different."	contrasting
train_198069	"Recent work has shown that supervision available in English for this task (e.g., lexical resources) can be transferred to other languages via crosslingual word embeddings."	"this kind of transfer misses monolingual distributional information available in a target language, such as contrast relations that are indicative of antonymy (e.g., hot...while...cold)."	contrasting
train_123149	"Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion."	"in most such work, the lexicons are word-level rather than sense-level."	contrasting
train_99491	These results may be good news for system builders; one possible Socratic teaching strategy would be to ask sequences of targeted questions where strong expectations about plausible answers make it easier to interpret student input.	"we must be mindful of the fact that, even in Socratic interaction, students sometimes do take initiative rather than simply answering the sequence of questions posed by the tutor."	contrasting
train_3660	"If we had a large collection of sense-tagged text, then we could extract disambiguated feature vectors by collecting co-occurrence features for each word sense."	"since there is little sense-tagged text available, the feature vectors for a random WordNet concept would be very sparse."	contrasting
train_45183	"We adapt the domain-adversarial method for feature alignment in an encoder proposed by (Ganin et al., 2016)."	"for text generation, a domain-independent representation from the encoder, as used in domain adaptation for classification, is not adequate."	contrasting
train_81	Spoken dialogue managers have benefited from using stochastic planners such as Markov Decision Processes (MDPs).	"so far, MDPs do not handle well noisy and ambiguous speech utterances."	contrasting
train_122758	"CRF has been used for Chinese word segmentation (Tseng, 2005;Shi and Wang, 2007;Zhao and Kit, 2008;Wang et al., 2011)."	most previous work train a CRF by using full annotation only.	contrasting
train_77760	We will see that the word representation approach can capture contextual information through combinations of vectors in a window.	it only produces local features around each word of the sentence.	contrasting
train_62300	"When the machine is in an existential state, it accepts the input if there is at least one transition that eventually leads to an accepting state."	"when the machine is in a universal state, it accepts input only if every possible transition eventually leads to an accepting state."	contrasting
train_98526	"A recent approach proposed by (Le and Mikolov, 2014) generates both word-level and paragraph-level representations."	"the approach relies on training the network with a large corpus with billions of tokens, which is rarely available in the conversational text domain."	contrasting
train_163973	"For example, the syntax-based grammar correctly finds two ubiquitination events and two negative regulations in the sentence ""CYLD inhibits the ubiquitination of both TRAF2 and TRAF6"" because the dependency graph correctly connects ""ubiquitination"" to ""TRAF2"" and ""TRAF6"", as seen in Figure 2."	"the surface-based grammar misses the ubiquitination event involving ""TRAF6"" (and the negative regulation of this ubiquitination), because the last two tokens of the sentence are not explicitly handled by the rules."	contrasting
train_16916	This method requires training sets of transliterated word pairs with language origin.	"it is difficult to obtain such tagged data, especially for proper nouns, a rich source of transliterated words."	contrasting
train_170465	"Another difficult choice is the distinction between the information status category unused (sometimes called mediated) and bridging, i.e. in a case like where some people might consider this a bridging case, as the foreign secretary Mottaki is probably not interpretable alone for a typical WSJ reader without the mentioning of Iran first."	others might argue that his discourse referent might already be identified by his name.	contrasting
train_72349	"For example, the contraction I'm was parsed as two lexical items by MICA: I and 'm."	'm was not recognized as a form of the word am-itself a form of the verb to be-but rather was analyzed as a distinct verb.	contrasting
train_4028	"It is often simply an unstated assumption that any full translation system, to achieve full performance, will sooner or later have to incorporate individual WSD components."	"in some translation architectures and particularly in statistical machine translation (SMT), the translation engine already implicitly factors in many contextual features into lexical choice."	contrasting
train_223489	"As for decoding, we directly follow Algorithm 1 to sample or decode greedily from the proposed model."	in practice beam-search is important to explore the output space for neural autoregressive models.	contrasting
train_215129	there is no explicit causal relation between add and fry.	"we implicitly know, that without adding the pork pieces, we cannot fry them."	contrasting
train_115873	Unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language.	words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar.	contrasting
train_36054	"For example, ""data set"" and ""training set"" have learning dependencies and the latter concept is more advanced than the former one."	"""test set"" and ""training set"" have no such relation when their complexity levels are similar."	contrasting
train_66663	Jing (Jing 00) developed a method to remove extraneous phrases from sentences by using multiple sources of knowledge to decide which phrases could be removed.	"while this method exploits a simple model for sentence reduction by using statistics computed from a corpus, a better model can be obtained by using a learning approach."	contrasting
train_165808	"We propose an unsupervised system for a variant of cross-lingual lexical substitution (CLLS) to be used in a reading scenario in computer-assisted language learning (CALL), in which single-word translations provided by a dictionary are ranked according to their appropriateness in context."	"to most alternative systems, ours does not rely on either parallel corpora or machine translation systems, making it suitable for low-resource languages as the language to be learned."	contrasting
train_45181	"Vocabulary expansion may be used to address the different vocabularies in source and target domains, and adversarial domain adaptation (ADA) may be used to merge the embedded feature representations across domains."	aDa does not adapt the decoder in an encoder-decoder generation model.	contrasting
train_122752	"There has been work on making use of both unlabeled data (Sun and Xu, 2011;Wang et al., 2011) and Wikipedia (Jiang et al., 2013) to improve segmentation."	no empirical results have been reported on a unified approach to deal with different types of free data.	contrasting
train_130950	"To alleviate such drawback, attempts have been made to build relation extractors with a small set of seed instances or humancrafted patterns (Nakashole et al., 2011;Carlson et al., 2010), based on which more patterns and instances will be iteratively generated by bootstrap learning."	"these methods often suffer from semantic drift (Mintz et al., 2009)."	contrasting
train_195656	This word-level loss ensures efficient and scalable training of seq2seq models.	"this word-level training objective suffers from a few crucial limitations, namely the label bias, the exposure bias, and the loss-evaluation mismatch (Lafferty et al., 2001; Bengio et al., 2015a; Venkatraman et al., 2015). "	contrasting
train_165202	"Of the 30 sampled tasks, ""buy movie tickets early"", ""buy a car"", and ""write a cover letter for a recruitment consultant"", which achieve accuracies of 0.3000, 0.3200, and 0.2800, respectively, perform worse than the other tasks."	"""download music safely"" and ""play experimental music"", which have accuracies of 0.8200  and 0.7000, perform better."	contrasting
train_141801	"For example, a token can be tagged with B-P ER, where B indicates the boundary of an entity and P ER indicates the corresponding entity categorical label."	"when entities are nested within one another, single-layer sequence labeling models can not ex-tract both entities simultaneously."	contrasting
train_167802	"The average number of inflected forms collected per paradigm differs across systems, as Liebeck and Conrad (2015) only considered forms which fit certain Wiktionary templates and Durrett and DeNero (2013) extracted only paradigms for which they could obtain a fixed set of forms (their software requires all training paradigms to be equal in size)."	"our system extracts all paradigms, regardless of completeness."	contrasting
train_129654	"In practice, a simple beam search procedure that explores K prospective histories at each time-step has proven to be an effective decoding approach."	"as noted above, decoding in this manner after conditional languagemodel style training potentially suffers from the is-sues of exposure bias and label bias, which motivates the work of this paper."	contrasting
train_81266	Most of these methods depend on sparse lexical features including bag-of-word (BoW) models and exquisitely designed patterns.	feature engineering is labor-intensive and the sparse and discrete features cannot effectively encode semantic and syntactic information of words.	contrasting
train_150758	"The reason for the worse results is that in our experiments, the oracled PPA always identifies more prepositions attached to verbs than the PPA classifier, therefore more prepositions will be given semantic roles by the SRD classifier."	"since the performance of the SRD classifier is not high, and the segmentation subsystem does not always produce the same semantic role boundaries as the CoNLL data set, most of these additional prepositions would either be given a wrong semantic role or wrong phrasal extent (or both), thereby causing the overall performance to fall."	contrasting
train_53925	"Like Hearst (1992) and Berland and Charniak (1999), they use manually generated rules to mine text for their desired relation."	"they supplement their manual rules with automatically learned constraints, to increase the precision of the rules."	contrasting
train_184655	We have found that the clustering-based procedure is very competitive when presented with gold chunks.	the iterative learning procedure performs very well when presented with automatic chunks in all tested domains and the two languages.	contrasting
train_5332	"Interestingly, Buckley et al., (2000) points out that ""English query words are treated as potentially misspelled French words"" and attempts to treat English words as variations of French words according to lexicographical rules."	"when two languages are very distinct, e.g., English-Korean, English-Chinese, transliteration from English words is utilized for cognate matching."	contrasting
train_123485	"That is, previous studies focused only on text printed on paper."	"with the increasing use of hand-held devices, people in these days use various reading devices such as a tablet and a smart phone as well as a paper."	contrasting
train_191963	(2017) also experimented with reconstructing implicit knowledge in short German argumentative essays.	"to our work, they used expert annotators who iteratively converged to a single proposition."	contrasting
train_160435	"Empirical studies such as (Kita andÃ–zyÃ¼rek, 2003;Kita et al., 2007) analysed speech and gesture semantics with statistical methods and show that the semantics of speech and gestures coordinate with each other."	it remains unclear how to computationally derive the semantics of iconic gestures and build corresponding multimodal semantics together with the accompanying verbal content.	contrasting
train_87828	Part of the problem is that words in the PP are replaced independently and without consideration to the remaining context.	we had hoped the specialist thesaurus might alleviate this problem by providing neighbours that are more appropriate for this specific task.	contrasting
train_149590	"Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks."	"for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry."	contrasting
train_69688	"Thus, TF-IDF performs relatively well because of the high-quality domain corpus."	"tF-IDF, as a statistics based algorithm suffers from similar problem as others based on statistics."	contrasting
train_61340	Lazaridou et al. (2013) also use a specific scheme of discourse relations.	"rather than relying on gold discourse annotations, they jointly predict sentiment, aspect, and discourse relations and show that the model improves accuracy of both aspect and sentiment polarity at the sub-sentential level."	contrasting
train_170613	"Similar work for Chinese has been reported by Ji (2010), who constructed the Chinese corpus from newspapers for Chinese uncertainty identification."	their data was annotated exactly based on the cue-phrases and size of corpus was a bit small for some uncertainty detection systems.	contrasting
train_200295	"Therefore, SJT test items employ the -desu and -masu forms more than any other forms of honorifics."	"the other forms of honorifics, ""respectful"" form (Sonkeigo) and ""humble"" form (Kenjogo) and also informal sentence-finals (e.g. -da), were also used to reflect reality of language use."	contrasting
train_58299	"This is to be expected, as most core arguments fall under the Arg0 and Arg1 classes, which can typically be disambiguated based on syntactic information (i.e., subject vs. object)."	"there are no syntactic hints for adjunct arguments, so the system learns to rely more on SP information in this case."	contrasting
train_29447	"However, with this regularizer, EA++ does not strictly restrict either the source classifier or the target classifier to lie in the target subspace X t ."	"as we have pointed out above, when only the induced features are used, our method leverages the unlabeled target instances to force the learned classifier to lie in X t ."	contrasting
train_84	"There are several POMDP algorithms that may be the natural choice for policy generation (Sondik, 1971;Monahan, 1982;Parr and Russell, 1995;Cassandra et al., 1997;Kaelbling et al., 1998;Thrun, 1999)."	"solving real world dialogue scenarios is computationally in-tractable for full-blown POMDP solvers, as the complexity is doubly exponential in the number of states."	contrasting
train_82207	"We can see that in this example the premise entails the hypothesis, and in order to correctly identify this relation, one has to know that the word kettle entails the word pot."	"if we train a neural network model on a set of labeled sentence pairs, and if the training dataset does not contain the word pair kettle and pot anywhere, it would be hard for the learned model to know that kettle entails pot and subsequently predict the relation between the premise and the hypothesis to be entailment."	contrasting
train_48004	" When the size of memory is small, the gap between different models are larger, with EMR-Transformer obtain-ing the best accuracy, which may be due to its ability to capture global relative importance of each memory entry."	"the gap between EMR-Transformer and EMR-biGRU diminishes as the size of memory increases, since then the size of the memory becomes large enough to contain all the frames necessary to answer the question."	contrasting
train_26241	"To date, most of the work presented on deception detection has focused on the identification of deceit clues within a specific language, where English is the most commonly studied language."	"a large portion of the written communication (e.g., e-mail, chats, forums, blogs, social networks) occurs not only between speakers of English, but also between speakers from other cultural backgrounds, which poses important questions regarding the applicability of existing deception tools."	contrasting
train_91645	"(b) CUE: either-or SCOPE: either a seqeuncing error or a pseudogene By handling this class to some extent, we could have increased our recall, and therefore, F-score (65 out of 1,044 cues in the evaluation data for biological text involved this class)."	"we decided against treating this class, as we believe it requires a slightly different treatment due to its special semantics."	contrasting
train_206357	"Under the causativization approach, he proposes a set tions alone suggest that the common base approach must be taken to account for equipollent alternations, as in (2)a-(2)j, which involve overt affixes for both the causative and the inchoative alternants."	"the same line of reasoning also suggests that causativization of inchoative verbs and anticausativization of causative verbs may be involved in (2)k-(2)m and (2)n-(2)o, respectively."	contrasting
train_85986	In our results in Table 2 we showed that overall collective stance indeed is an important feature to consider for the purpose of veracity prediction.	this depends on how this collective feature is used.	contrasting
train_156727	"(Uszkoreit 2011) introduced a bootstrapping system for relation extraction rules, which achieved good performance under some circumstances."	"most previous semi-supervised methods have large performance gaps from supervised systems, and their performance depends on the choice of seeds (Vyas et al., 2009;Kozareva and Hovy, 2010)."	contrasting
train_3571	The above method allows for the tractable estimation of log-linear models on exponentially-many HPSG parse trees.	"despite the development of methods to improve HPSG parsing efficiency (Oepen et al., 2002a), the exhaustive parsing of all sentences in a treebank is still expensive."	contrasting
train_120582	"Passive-to-active voice transformation in English can be performed systematically, which does not depend on lexical information in most cases."	"in Japanese, the method of transformation depends on lexical information."	contrasting
train_220112	"This could be because (4) and (5) both relate to child development, and therefore LDA considers them as sharing the same topic."	"topic is not the same as sense, especially when larger contexts are available."	contrasting
train_111165	"Previous phrase alignment work has primarily mitigated this tendency by constraining the inference procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007;Zhang et al., 2008)."	"the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure."	contrasting
train_219474	They also use an auto-supervision technique to smooth counts learnt from EM onto new words encountered during testing.	we do not apply any such technique for unknown words and allow them to be mapped uniformly to all possible tags in the dictionary.	contrasting
train_125636	"To motivate this method, recall that one important property of a sieve-based approach is that later sieves can exploit earlier sieves' decisions when making their own decisions."	our first method of using sieves makes limited use of the decisions made by earlier sieves.	contrasting
train_47993	"Differentiable Neural Computer (DNC)  extends the NTM to address the issue by introducing a temporal link matrix, replacing the least used memory when the memory is full."	this method is a rule-based one that cannot maximize the performance on a given task.	contrasting
train_129998	"These results clearly demonstrate that the OOV issue is much more severe for entities than predicates, and the difficulty word-level models have when generalizing to new entities."	"character-level models have no such issues, and achieve a 96.6% accuracy in predicting the correct entity on the mixed set."	contrasting
train_202508	They recognize coordinate structures in a Japanese sentence by constructing a similarity matrix between bunsetsus and searching a path with the highest parallelism in the similarity matrix using a dynamic programming method.	that method is inadequate to apply to patent documents which have usually a large number of coordinate nodes and sometimes complex modification such as an inserted clause.	contrasting
train_37933	"Therefore, we expect to see the same benefit as BPE with the unigram language model."	"the unigram language model is more flexible as it is based on a probabilistic language model and can output multiple segmentations with their probabilities, which is an essential requirement for subword regularization."	contrasting
train_141134	"Probably because of this, there is very little literature or understanding of the effect of order among the training examples-strict ordering of examples has simply been known to be undesirable."	"as neural network approaches increasingly dominate in performance across many NLP tasks, the notion of random shuffling has become overshadowed by that of computational efficiency."	contrasting
train_73912	"The estimation of the parameters, whether manual or through an automated learning algorithm prevent these algorithms from being entirely unsupervised."	the degree of supervision remains far below supervised approaches that use training corpora approximately 1000 times larger.	contrasting
train_202551	"They have proved useful for various important tasks and applications, including e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004;Dang, 2004;Shi and Mihalcea, 2005;Kipper et al., 2008;Zapirain et al., 2008)."	"useful are classes which capture generalizations over a range of (cross-)linguistic properties, such as the ones proposed by Levin (1993)."	entailment
train_2768	We have examined two largely unexplored issues in computing and using anaphoricity information for improving learning-based coreference systems: representation and optimization.	we have systematically evaluated all four combinations of local vs. global optimization and constraint-based vs. feature-based representation of anaphoricity information in terms of their effectiveness in improving a learning-based coreference system.	entailment
train_119638	"Hence, we propose to exploit a question focus along with the related named entities (according to the mapping from Table 1) of the answer sentence to establish relational links between the tree fragments."	"once the question focus and question category are determined, we link the focus word wf ocus in the question, with all the named entities whose type matches the question class (Table 1)."	entailment
train_83238	"In order to evaluate the quality of the summaries produced by our model and the other baselines, we ask annotators to do a score evaluation on some aspects of the summaries."	we ask them to score the grammaticality and non-redundancy of the summaries.	entailment
train_11166	"As Figure 1 shows as the number of times a query is issued increases, so does the chance of it being bilingual."	nearly 45% of the highest-frequency English queries and 35% of the highest-frequency Chinese queries are bilingual.	entailment
train_55084	This is not to say that going electronic-only makes the journal free of production costs.	"although the journal's editor, editorial board, and external reviewers provide their labor free of charge, it still costs real money to produce the journal: CL will continue to use the professional services of the MIT Press in managing the high standards of copyediting and typesetting to which our readers have become accustomed."	entailment
train_61333	It is clear from their results that using adjectives that can be classified as conveying appraisal values is beneficial in sentiment analysis.	"and unsurprisingly, adjectives labeled as Appreciation are some of the most useful features for the classifier."	entailment
train_123506	"Therefore, readability is calculated as a weighted linear sum of all readability factors."	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability. 	entailment
train_23937	"In contrast, our approach allows structured modeling of sentiment while taking into account both local and global contextual information."	we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization.	entailment
train_37045	"For ZvdP2016, most errors can be attributed to oversplitting."	25 out of its 37 (67.5%) misclassifications compared to FF2010 can be attributed to this problem.	entailment
train_94920	We use the POS tag and the shallow parser output to identify the set of candidates that are input to the classifiers.	"for nouns, we collect all words tagged as NN or NNS."	entailment
train_71687	In this paper we focus on the problem of resolving references of named-entities and concepts in natural language through their textual surface forms.	we present a method of resolving surface forms in general text documents to Wikipedia entries.	entailment
train_55533	These figures reflect a desired behavior of the bootstrapping function which concentrates most of the prominent common features for all the distributionally similar words (whether entailing or not) at the lower ranks of their vectors.	"this explains the ability of our method to perform a massive feature reduction as demonstrated in Section 5, and to produce more informative vectors, while demoting and eliminating much of the noise in the original vectors."	entailment
train_6389	"Please note that, different from the previous tree kernel, here we loosen the condition for the occurrence of a subtree by allowing both original and reduced rules (Improvement 1) and node feature mutations (Improvement 2)."	we modify the criteria by which a subtree is said to occur.	entailment
train_156244	"In this paper, we analyze political debates where the power differential is dynamic."	we analyze the 2012 Republican presidential primary debates.	entailment
train_11394	"Somers (2005) therefore states: ""To be really sure of our results, we should like to replicate the experiments evaluating the translations using a more old-fashioned method involving human ratings of intelligibility."""	"apparently nobody has ever seriously compared backtranslation scores to human judgments, so the belief about their inutility seems not sufficiently backed by facts."	entailment
train_102954	"In recent years, approaches that aim at extracting a subset of all relations have achieved great success."	"previous research (Carreras and MÃrquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments."	entailment
train_122596	"Note that since P1 and P2 are pipeline systems, the binary classifiers they use to predict a test post's reasons depend on the post's predicted stance."	"if a test post is predicted to have a positive (negative) stance, then only the reason classifiers associated with the positive (negative) stance will be used to predict the reasons it contains."	entailment
train_208710	"(2) Among the five faculties employed by human beings to interact with the world-vision, hearing, smell, taste and touch-we hypothesize vision is consequential."	the visibility of human body parts is important for the selection of a body part to be used metaphorically.	entailment
train_112053	Some human supervision can be valuable particularly in the form of semisupervised learning and active learning.	a process that integrates human input at appropriate times (for example seeding or correction) is likely to be part of a successful approach.	entailment
train_200291	"For example, both the SET-10 and the SST score report consist of one Overall score and four subscores: Sentence Mastery, Vocabulary, Fluency, and Pronunciation."	the SET-10 measures two aspects of the spoken skills: what the test-taker said and how the test-taker said it.	entailment
train_198553	"Thus, the two disjunctive situations in R, namely R, and R2, corresponds to indeterminacy of possibilities that are associated with the two situations shown in (18) above."	"use of disjunction with ""ka"" which expresses indeterminacy among two potentials is quite informative in such a situation, because this indeterminacy relates directly to the following course of events."	entailment
train_195749	"we view this problem as an interactive process, in which the human operator can observe the agents' response to their instruction and adjust it by providing advice, a form of online feedback."	"the advice consists of a short sentence, simplifying the user's intent."	entailment
train_29918	The matching score of a source sentence and a target phrase can be measured as the similarity between their feature vectors.	"we use the multi-layer perceptron (MLP), a nonlinear function for similarity, to compute their matching score."	entailment
train_52839	"However, the RNP is observed only in a subset of the cases that would be considered as CB mentions according to the definition provided by Constraint 3, and in the example we are discussing (10), neither Branicki nor the cupboard occur in (u229) in a position that would be subject to RNP effects according to Gordon et al."	(some) evidence used by Grosz et al. in support of CB uniqueness cannot be used to argue that (u229) in (10) has a single CB.	entailment
train_115702	"Aiming at preventing the OOV recall from dropping sharply and still maintaining an overall performance as good as that of the state-of-art segmenter when working with heterogeneous test sets, we propose in this paper to use a semisupervised way for Chinese word segmentation task."	we propose to use Ï‡ 2 statistics together with bootstrapping strategies to build Chinese word segmentation model.	entailment
train_176662	"As for compositional noun phrases, if the translation preserves normal word order, then the PicmreQuest-internal noun phrase recognition will take effect."	"ifjeune fille translates as young girl, then PictureQuest will understand that young is an adjective modifying girl."	entailment
train_219419	Our purpose in this section is to investigate the ability of automatic systems to select this subset of bilingual documents.	"given a collection of documents retrieved for a target language, the task is to identify the documents that contain text in English in addition to the target language."	entailment
train_209171	"Although there is a need to revisit Kachru's three-circle model in this regard, it is still vital to be taken into account since pilots and ATCs either native or nonnative speakers of English coming from different nations speak different varieties of English."	there is a need to understand the World Englishes paradigm and use it as a theoretical underpinning in describing the lexical items in standard phraseology having non-standard definition.	entailment
train_89089	"With the larger goal of automatically constructing a Pan-Chinese lexical resource, this work aims at taking an existing semantic classificatory structure as leverage and incorporating new words into it."	"it is important to see if the classification could accommodate new words from heterogeneous data sources, and whether simple similarity measures and clustering methods could cope with such variation."	entailment
train_196573	"For human, writing down an equation that solves a math word problem requires the ability of reading comprehension, reasoning, and sometimes real world understanding."	"to solve a math word problem, we first need to know the goal of the given problem, then understand the semantic meaning of each numerical number in the problem, perform reasoning based on the comprehension in the previous step, and finally decide what to write in the equation."	entailment
train_27553	"Our contention, and the main motivation behind our work, is that we can do better by leveraging explicit mechanisms adopted by authors in keyphrase generation."	"we focus on a tendency to expand keyphrases by adding terms, coupled with a pressure to abbreviate to retain succinctness."	entailment
train_152282	This implies that the ratio of trigram to quad-gram perplexities would be lower for a fake article than for a real article.	this ratio is similar to computing the likelihood ratio of an article w.r.t the trigram and quad-gram models.	entailment
train_200662	All SJT test items are integrated 'listen-then-speak' type requiring the test-taker to employ real-time receptive and productive spoken language skills.	the SJT is designed to measure the test taker's control of these core language processing components.	entailment
train_180707	"Comparing the MAP for PBQD-Fac with MAP using the generative baseline for the most improved indexing system (the word system), we find that this degradation approach again statistically significantly improved MAP."	these two strategies for handling recognition errors in RUR appear to work well in combination.	entailment
train_115134	Notice that the rational speaker as defined so far does not make full use of our grammar.	"the rational speaker will never use the ""wildcard"" noun something nor the relativization rule in the grammar because an NP headed by the wildcard something can always be replaced by the object ID to obtain a higher utility."	entailment
train_82670	"It integrates both local contextual information and global interdependence of mentions in a document, and is efficiently trainable in an end-to-end fashion."	we introduce attention mechanism to robustly model local contextual information by selecting informative words and filtering out the noise.	entailment
train_83230	"The Copy-Generate decoder produces output the compressed sentence word by word, and the output words are either copied from the input words which are filtered by the delete decoder, or generated with a fixed vocabulary."	"our model integrates copy, generate, delete operations together to produce the output sequence."	entailment
train_187251	Another related field aims to learn continuous vector representations for various abstraction levels of natural language.	"the creation of socalled word embeddings has attracted a lot of attention in the past years, often by implementing neuralnetwork language models."	entailment
train_133446	"While the goal of ALA is to generate annotations of all types, it is evident from our analysis that CS annotations can not be generated by models trained solely on parallel text."	these annotations cannot be generated without background knowledge or added context.	entailment
train_65368	"The structure of our belief network, which represents the constraints is a bipartite graph."	"the variable E's and R's are the nodes in the network, where the E nodes are in one layer, and the R nodes are in the other."	entailment
train_123487	The usefulness of the device-dependent readability is proven by applying it to news article recommendation.	different importance weights for readability factors are considered according to device type when recommending news articles.	entailment
train_120453	"Below both MERT and PRO tune weights on the dev set, while our method on the training set."	our method only uses the dev set to know when to stop training.	entailment
train_125631	1 This Baseline system performs joint role labeling and relation classification using an ensemble of classifiers.	"it trains one classifier for extracting QSLINKs and OLINKs, and seven classifiers for extracting MOVELINKs."	entailment
train_153394	"We group these two models together, selecting which one to use based on the referring expression."	"the pronoun model is selected if a referring expression is a pronoun, and the nonpronoun model otherwise."	entailment
train_184259	One immediate question raised in this process is whether one needs to actually see the image to perform the annotation.	"if we expect an NLP system to be able to classify noun phrases as visual or non-visual, we need to know whether people can do this task sans image."	entailment
train_145996	We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.	"our contribution is two-fold: 1. We construct context-gloss pairs and propose three BERT-based models for WSD. 2. We fine-tune the pre-trained BERT model, and the experimental results on several English all-words WSD benchmark datasets show that our approach significantly outperforms the state-of-the-art systems."	entailment
train_166109	In this paper we present the datasets created by CLEF eHealth Lab from 2013-2015 for evaluation of search solutions to support common people finding health information online.	"the CLEF eHealth information retrieval (IR) task of this Lab has provided the research community with benchmarks for evaluating consumer-centered health information retrieval, thus fostering research and development aimed to address this challenging problem."	entailment
train_17741	"In order to improve generalization, we estimate the categorical parameters based on the counts from all instances, except the one whose gradient is being computed for the online update (leave-one-out)."	we subtract the counts for a particular instance before computing the update (Equation 8) and add them back when the update has been executed.	entailment
train_55505	"In addition, a major property of lexical collocations is their ""non-substitutability"", as termed in Manning and Schutze (1999)."	typically neither a headword nor a modifier in the collocation can be substituted by their synonyms or other related terms.	entailment
train_12318	This paper shows that -with new techniques self-supervised learning of relation-specific extractors from Wikipedia infoboxes does scale.	"we present LUCHS, a selfsupervised IE system capable of learning more than an order of magnitude more relation-specific extractors than previous systems."	entailment
train_154875	This paper describes efforts of NLP researchers to create a system to aid the relief efforts during the 2011 East Japan Earthquake.	"we created a system to mine information regarding the safety of people in the disaster-stricken area from Twitter, a massive yet highly unorganized information source."	entailment
train_137870	To overcome this we artificially increase the softmax probabilities (dashed edges in Figure 3) so that they reflect the DFS path decoded up until that point.	"we override the predicted word according to the previous index, and backtrack ""up"" the corresponding edge."	entailment
train_27585	"Compared to standard RBMs, a crucial difference is that hidden units now have a heterogeneous structure instead of being homogeneous as in the standard basic RBM model."	"we rely on three types of hidden units, representing aspect, sentiment, and background, respectively."	entailment
train_39061	The hyper-parameters of TNet-LF and TNet-AS are listed in Table 2.	all hyperparameters are tuned on 20% randomly held-out training data and the hyper-parameter collection producing the highest accuracy score is used for testing.	entailment
train_57769	This improvement persists even when the original LMs are up to ten times larger than the translated ones.	one has to collect ten times more original material in order to reach the same quality as is provided with translated material.	entailment
train_29612	We propose a new approach to the task of fine grained entity type classifications based on label embeddings that allows for information sharing among related labels.	we learn an embedding for each label and each feature such that labels which frequently co-occur are close in the embedded space.	entailment
train_132174	The final hidden state of the question-encoding LSTMs naturally cluster based on question type (Table 1).	the task induces a question encoding that superficially respects type information.	entailment
train_140849	We first evaluate embeddings quantitatively with respect to retrieval performance.	"we assess whether the induced representations afford improved retrieval of abstracts relevant to a particular systematic review (Cohen et al., 2006; Wallace et al., 2010)."	entailment
train_174186	The platform is semi-autonomous because some modules of the system are automatic (for example the dialogue generation) where some others are manual.	the speech recognition and the comprehension modules are simulated by a human: the doctor verbal production is interpreted in real time by the operator which selects the adequate input signal to be transmitted to the dialogue system.	entailment
train_2212	Our evaluation metric is human assessment: Can the translation provided by the system be part of an acceptable translation of the whole sentence?	the noun phrase has to be translated correctly given the sentence context.	entailment
train_129016	"However, in the RA-CNN model this is a weighted sum."	weights are set to the estimated probabilities that corresponding sentences are rationales in the most likely direction.	entailment
train_47221	The advantage is that our model is less sensitive to the quality of individual q-r pairs and hence increases the robustness of response generation.	"we divide the training corpus into multiple groups by clustering, extract common characteristics of each group, and learn to utilize the characteristics to assist generation."	entailment
train_160318	Table 1 shows that 1-LAYER-BISEQ2SEQ obtains better results than the strong BASELINE.	"1-LAYER-BISEQ2SEQ improves the baselines with 0.3+ BLEU in both cases of ATT and NON-ATT, indicating the usefulness of using bidirectional architecture."	entailment
train_48614	We have to find a tradeoff: take as many Bibles as possible that share as many verses as possible.	"we cast this selection process as an optimization problem: select Bibles such that the number of verses overall (i.e., the number of verses shared times the number of Bibles) is maximal, breaking ties in favor of including more Bibles and ensuring that we have at least 20000 verses overall to ensure applicability of neural language models."	entailment
train_83233	The alignment information together with the pair of source text and compressed text are used for generating operation sequences for our experiments.	"for each pair of the source and compressed text, a delete operation sequence and a copy/generate sequence are generated."	entailment
train_215626	"Since SKD distills knowledge from the current training model, at the beginning of the training process, the model does not contain relevant information."	we cannot extract any knowledge from the training model at the beginning.	entailment
train_151299	"In this paper, we propose an automatic estimation method for word significance (weights) based on its influence on IR."	weights are estimated so that evaluation measures of ASR and IR are equivalent.	entailment
train_129651	"Once the input sequence is encoded, seq2seq models generate a target sequence using a decoder."	the decoder is tasked with generating a target sequence of words from a target vocabulary V.	entailment
train_76907	"Our experiments are designed to explore two main questions: (a) the value of symmetric patterns as semantic classification features, compared to state-of-the-art word clustering and embedding methods; and (b) the required complexity of an algorithm that can propagate information about semantic similarity."	we test the value of our simple I-k-NN algorithm compared to more sophisticated alternatives.	entailment
train_107890	The inference-based features are computed by attempting to infer an underlying semantic property of a given mention.	"we attempt to identify gender and semantic number (e.g., ""group"" is semantically plural although it is syntactically singular)."	entailment
train_89096	"To evaluate the effectiveness of the automatic classification, we adopted a simple baseline measure by ranking the 94 subclasses in descending order of the number of words they cover."	"assuming the bigger the subclass size, the more likely it covers a new term, thus we compared the top-ranking subclasses with the classifications obtained from the automatic method using the cosine measure."	entailment
train_13557	"To perform belief update given a new visual input, we create a new wFSA to represent the likelihood of each character from the sample."	"this wFSA has only a single chain of states, where, e.g., the first and second state in the chain are connected by 27 (or fewer) arcs, which emit each of the possible characters for w 1 along with their respective likelihoods given the visual input (as in the inner term of Equation 3)."	entailment
train_66083	"In addition, the verb sense disambiguation results are cached and applied globally in the same text."	"the selected case frames are cached for each verb, and only the case frames that are similar to the cache are used for the same verb following in the same text."	entailment
train_17269	"On the other hand, our choice was driven by the availability of large corpora for English."	we used the New York Times Annotated Corpus.	entailment
train_5107	We propose a segment retrieval approach for performing the first subtask by also using HMMs.	"it trains an HMM from labelled segments in texts, and then use the learned HMM to determine whether a segment is relevant or not with regard to a specific extraction task."	entailment
train_542	Some of the complexity in the RICHES system is there to demonstrate the potential for different types of control strategies.	"we do not make use of the possibilities offered by the interleaving of the RE and LC, as the examples we cover are too simple."	entailment
train_169478	"However, existing weight learning algorithms cannot take advantage of such prior information during the training process (Venugopal et al., 2014)."	prior information can only be applied during test time.	entailment
train_65771	"However, newswire does not offer direct access to facts, events, and opinions; rather, journalists report what they have experienced, and report on the experiences of others."	"facts, events, and opinions are filtered by the point of view of the writer and other sources."	entailment
train_76490	Next we move to the multilingual setting where we would like to simultaneously process more than two languages.	"we considered multilingual semantic parsing where there are two, three, four and five input languages."	entailment
train_155474	We experiment with the parallel Chinese-Spanish corpus (United Nations) to explore alternatives of SMT strategies which consist on using a pivot language.	two well-known alternatives are shown for pivoting: the cascade system and the pseudo-corpus.	entailment
train_45511	"In general, our proposed method achieves significant improvements over our baseline biaffine parser and matches state-of-the-art models."	"it achieves 0.29 percent UAS and 0.35 percent LAS improvement over the baseline parser, and 0.1 percent UAS and 0.12 percent LAS improvement over the strong transition-based parser (Ma et al., 2018)."	entailment
train_72180	"For example, we assume the adversary is a human acting passively rather than actively."	"we have ignored the possibility of computational steganalysis and steganographic attacks, such as detecting, extracting and destroying the hidden message (Fridrich, 2009)."	entailment
train_23941	"In this paper, we propose a sentence-level sentiment classification method that can (1) incorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning."	"we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization (PR) (Ganchev et al., 2010)."	entailment
train_113472	In the future we plan to investigate how the different models fare against one another in parsing different languages.	"we wish to examine whether parsing different languages should be pursued by different models, or whether the RR strategy can effectively cope with different languages types."	entailment
train_53236	"To us, the work we present here seems to be the crucial first step in avoiding arbitrary constraints on inferences for building discourse structures."	"the point we wish to make here is that although there might be other constraints on possible discourse annotations that will have to be identified in future research, tree structure constraints do not seem to be the right kinds of constraints."	entailment
train_204824	"However, unlike English, in addition to the delimiter that further specifies the endpoint/endstate, Chinese allows another type of delimiter to follow an existing delimiter and function to reinforce the attainment of the endpoint/endstate denoted by first delimiter."	the two delimiters in a clause provide the same degree of specification of an endpoint/endstate.	entailment
train_39054	"Moreover, it is hard for vanilla CNN to differentiate opinion words of multiple targets."	"multiple active local features holding different sentiments (e.g., ""great food"" and ""service was dreadful"") may be captured for a single target, thus it will hinder the prediction."	entailment
train_208510	"To integrate CSLM more efficiently into decoding, some existing approaches calculate the probabilities of the n-grams before decoding and store them (Wang et al., 2013;Wang et al., 2014;Arsoy et al., 2013;Arsoy et al., 2014) in n-gram format."	"n-grams from BNLM are used as the input of CSLM, and the output probabilities of CSLM together with the corresponding n-grams of BNLM constitute converted C-SLM."	entailment
train_215618	It shows that there is knowledge not only in the target probability corresponding to the target class but also in the other class probabilities in the estimation of the trained model.	the other class probabilities can contain additional information describing the input data samples differently even when the samples are in the same class.	entailment
train_2448	"However, these methods are not appropriate for chunking Korean and Japanese, because such languages have a characteristic of partially free wordorder."	there is a very weak positional constraint in these languages.	entailment
train_206243	The mother node has a non-empty C-CONT feature which specifies the semantic relation between the two events.	"the RELS feature inside of C-CONT accommodates a relation of the type svcrelation, which has the subtypes causative, purpose, manner-or-instrument and consecutive."	entailment
train_61363	"Prefix contextual similarity is calculated in the same way as the contextual similarity score, but we use source and target word stems, or word prefixes up to five characters long, instead of full words."	"the word prefix contextual similarity score for the word pair (blanco, white) is the same as that of (blanca, white)."	entailment
train_166110	"The second CLEFeHealth (Kelly et al., 2014) expanded our year-one efforts and again organized three tasks."	"the first task aimed to help patients (or their nextof-kin) by addressing visualisation and readability issues related to their hospital discharge documents and related information search on the Internet (Suominen et al., 2014)."	entailment
train_58292	"In this case each word is represented by a vector which contains all similar words with weights, where those weights come from first order similarity."	"in order to obtain the second-order vector for word w, we need to compute its first order similarity with all other words in the vocabulary."	entailment
train_49092	"In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model."	we propose three different syntax encoding methods: a) a full constituency tree representation (Full-C); b) an SRLspecific span representation (SRL-C); and c) a dependency tree representation (Dep).	entailment
train_184257	"In both corpora, nouns tend not to have any modifiers, but modifiers are still more prevalent in Flickr than in generic text."	"60% of nouns in Flickr have zero modifiers, but 70% of nouns in generic text have zero modifiers."	entailment
train_73388	"Before we go into the details about the algorithm, one thing should be mentioned."	"for dependency parsing, POS tag serves as indispensable information (Li et al., 2010)."	entailment
train_206205	Reduction of non-projective dependencies is not allowed.	"a dependent word in a distant position cannot be deleted (with the only exception of limited technical non-projectivities caused, e.g., by prepositions)."	entailment
train_121523	"As a second transformation, we apply dimensionality reduction to the two matrices."	"we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original data in lower dimensionality space (SchÃ¼tze, 1997), and reduce the vectors to 50 dimensions."	entailment
train_167103	"We used such subdivision as a starting point, and departed from it only when the syntactic properties defining an English class do not apply to Italian (e.g.: subclasses based on the presence vs. absence of a syntactic construction that is not possible in Italian, like the Double Object construction), or when other solutions were regarded to better describe the verb behavior."	"during our analysis we established several mismatches between the English and the Italian models, due to syntactic or semantic differences between the two languages, or discrepancies deriving from the construction methodologies."	entailment
train_2754	Inspection of this definition shows that cr provides a means of adjusting the relative misclassification penalties placed on training instances of different classes.	"the larger cr is, the more conservative the classifier is in classifying an instance as negative (i.e., non-anaphoric)."	entailment
train_113469	"Firstly, RR models significantly outperform HD models (about 2 points absolute improvement in F 1 ) in parsing the Modern Hebrew treebank."	rr models show better performance in identifying the constituents for which syntactic positions are relatively free.	entailment
train_66078	"If <priest> is used for ""gobou"" in a cooking domain text, though ""gobou"" means 'burdock' (a genus of coarse biennial herbs) in its context, ""gobou"" is identified as an agent."	"""gobou"" is incorrectly analyzed as antecedents of the following nominative zero pronouns."	entailment
train_148993	Most of the previous RST parsers have been based on supervised learning approaches.	"they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus."	entailment
train_129992	"Our novel, character-level encoder-decoder model is compact, requires significantly less data to train than previous work, and is able to generalize well to unseen entities in test time."	"without use of ensembles, we achieve 70.9% accuracy in the Freebase2M setting and 70.3% accuracy in the Freebase5M setting on the SimpleQuestions dataset, outperforming the previous state-of-arts of 62.7% and 63.9% (Bordes et al., 2015) by 8.2% and 6.4% respectively."	entailment
train_82678	"Compared with NCEL-local, the global module in NCEL brings more improvements in the ""hard"" case than that for ""easy"" dataset, because local features are discriminative enough in most cases of TAC2010, and global information becomes quite helpful when local features cannot handle."	our propose collective model is robust and shows a good generalization ability to difficult EL.	entailment
train_44530	"We therefore annotated Spanish and Hebrew sentences ourselves, with annotations made by native speakers of each language."	"for each language, we extracted sentences containing animate nouns from that language's UD treebank."	entailment
train_38442	"We identify probable subevents by using surface syntactic cues corresponding to identifying a sequence of events in a sentence (Badgett and Huang, 2016)."	a sequence of two or more verb event mentions in a conjunction structure are extracted as subevents.	entailment
train_49671	The resultant attribute representations are then used to compare attributes of each entity record pair.	we compute the element-wise absolute difference between the two attribute vectors for each attribute and construct attribute similarity vectors (sim 1 and sim 2 in Fig.	entailment
train_16879	"However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago."	"modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model."	entailment
train_118212	"In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques."	"we show how to formulate the task of learning a semantic parser as discussed by Chen, Kim andMooney (2008, 2010) as the task of learning a PCFG from strings."	entailment
train_1567	The second implementation is different from the first one only in (1).	it employs as a classifier a decision list instead of an ensemble of NBCs.	entailment
train_55525	"We suggest, on the other hand, that the desired behavior of effective feature weighting is that the common features of truly similar words would be concentrated at the top ranks of both word vectors."	"if the two words are semantically similar then we expect them to share their most characteristic features, which are in turn expected to appear at the higher ranks of each feature vector."	entailment
train_202552	The most common evaluation resource in English verb classification has been that of Levin (1993) supplemented with additional information from VerbNet or WordNet.	"two gold standards based on (Levin, 1993) have been used to evaluate much of the recent research: GS1 The gold standard of Joanis et al. (2008) provides a classification of 835 verbs into 15 (some coarse, some fine-grained) Levin classes."	entailment
train_2459	The reason why the performance on Korean is lower than that on English is the curse of dimensionality.	"the wider context is required to compensate for the free order of Korean, but it hurts the performance (Cherkassky and Mulier, 1998)."	entailment
train_52807	"Given the many ways in which the parameters of centering can be set, the only feasible way to make a systematic comparison between the theory's ""instantiations"" is by computational means."	running computer simulations of the process of local focus update using an annotated corpus and comparing the results obtained under different instantiations.	entailment
train_17270	"In order to be able to explore our approach in a fruitful manner, we decided to concentrate on words which have acquired a new dimension of use due to the introduction of computing and the internet, e.g., to browse, to surf, bookmark."	the Netscape Navigator was introduced in 1994 and our data show that this does indeed correlate with a change in use of these words.	entailment
train_95114	"Our feature set, summarized in table 2, contains features which are strongly related to many of the structural features in WALS."	we use features derived from labeled dependency parses.	entailment
train_58297	"The Selectional Preferences implemented in this study are not able to deal with non-nominal argument heads, such us those of NEG, DIS, MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate the same evaluation conditions of typical PropBank-based SRL experiments all arguments are evaluated."	"our SP models don't return any prediction for those, and the evaluation penalizes them accordingly."	entailment
train_123514	These results imply that readers are negatively sensitive to the length of an article because of the small display size of a smart phone.	"in the smart phone, longer articles are recognized as difficult to read compared to shorter ones."	entailment
train_149017	"In this model, we aim to utilize latent representations to characterize the flow between sentences."	"we define delta, subtractions of hidden represenations of adjacent sentences as such latent information."	entailment
train_156250	"Building on insights from these studies, we investigated if the modes of addressing candidates change with respect to their power."	"we looked at four modes of addressing -FN (First Name), LN (Last Name) FLN (First and Last Name) and TN (Title followed by Name, first, last or full)."	entailment
train_166239	"Finally, following the famous zoo introduced in (Doddington et al., 1998), one can notice that several speakers appear to be wolves, meaning that they are particularly successful at imitating other speakers."	their speech is very likely to be accepted as that of another speaker.	entailment
train_65354	"For the sake of the experimental comparison, we define six feature sets, each of which is an incremental combination of the primitive feature types."	"feature set 1 (denoted by Word) contains word features; feature set 2 (Pos) contains features composed of words and pos tags and so on; The final feature set, feature set 6 (RelWord) contains all the feature types and is the only one that contains the related words lists."	entailment
train_121524	"Representational similarity analysis circumvents the previous problems by abstracting each fMRI/distributional data source to a common structure capturing the interrelationships between each pair of data items (e.g., words)."	"for each model/participant's fMRI data/anatomical region, the similarity structure was evaluated by taking the pairwise correlation (Pearson's correlation coefficient) between all unique category or word combinations."	entailment
train_206363	The anticausativization approach predicts exactly the opposite to those made by the causativization approach.	"it predicts that, if an idiomatic interpretation is possible with a causative, then it should be possible with its inchoative counterpart, as shown in Table 2 below."	entailment
train_41164	"Instead of predicting the overall sentiment polarity, fine-grained aspect based sentiment analysis (ABSA) (Liu and Zhang, 2012) is proposed to better understand reviews than traditional sentiment analysis."	we are interested in the sentiment polarity of aspect categories or target entities in the text.	entailment
train_61371	We computed how frequently each signal ranks the correct translation higher than any other signal.	we computed how often each signal is a better predictor of how to translate a given word than all other signals.	entailment
train_221873	"We find that the prediction of these constituent types requires, explicitly, modeling of bottom-up structures."	"bottom-up information is necessary for us to know if the span can be a noun phrase (NP) or sentence (S), for example."	entailment
train_28853	Sagae and Lavie ascribe the empirical success of their BFS to the sparseness of the distribution over subsequent actions in the MaxEnt model.	"bFS is very efficient when only a few actions have dominant probabilities in each step, and the Max-Ent model facilitates this with its exponential operation (2)."	entailment
train_95112	This correlation has broad implications on the ability to perform inference from native language structure to second language performance and vice versa.	it paves the way for a novel and powerful framework for comparing native languages through second language performance.	entailment
train_170362	The class selection depends on the outcome of the Intervention compared to the Comparison.	"if an Intervention is better (e.g. more effective, less adverse effects) than its Comparison, the sentiment is positive."	entailment
train_125627	This novel combination of sieves and tree-based structured features results in what we call an expanding parse tree.	"as a spatial element for a MOVELINK is extracted by a (role-specific) sieve, it will be added to the structured feature for the classifier associated with the following sieve."	entailment
train_218175	To obtain the triples for this task we use the test set of the CoNLL 2008 shared task data.	for every verbal predicate mention in the data we select a random nominal word from each of its arguments phrase chunks to obtain a true predicate-argument word pair.	entailment
train_79864	"Here, we propose to examine the impact of these problems by also using other evaluation protocols, based on other types of reference datasets, and by third-party tasks."	"following the work of Claveau and Kijak (2016), we rely on information retrieval (IR) as a realistic evaluation use case."	entailment
train_58296	"For that, SP models will be used in isolation, according to the classification rule in Equation 11, to predict role labels for a set of (predicate, argument-head) pairs."	"we are interested in the discriminative power of the semantic information carried by the SPs, factoring out any other feature commonly used by the state-of-theart SRL systems."	entailment
train_101121	"SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. (Collins and Duffy, 2002;Kudo and Matsumoto, 2003;Cumby and Roth, 2003;Shen et al., 2003;Moschitti and Bejan, 2004;Culotta and Sorensen, 2004;Kudo et al., 2005;Toutanova et al., 2004;Kazama and Torisawa, 2005;Zhang et al., 2006;."	"in question classification, tree kernels, e.g. (Zhang and Lee, 2003), have shown accuracy comparable to the best models, e.g. (Li and Roth, 2005)."	entailment
train_107889	"Based on the hypothesis that many name to nominal coreference chains are best understood in terms of background knowledge (for instance, that ""George W. Bush"" is the ""President""), we have attempted to take advantage of recent techniques from large scale data mining to extract lists of such pairs."	"we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman’s web page to generate features between names and nominals (this list contains noU pairs mined from pI`GBs of news data)."	entailment
train_204822	"This study shows that although further study is necessary for the inconsistency between the SDC/UPC and English data, Mandarin Chinese conforms to these constraints in that a verb can have only one delimiting expression."	the event denoted by the verb can only have one endpoint or endstate.	entailment
train_74187	The main motivation behind our study of causal relations comes from Sanders's cognitive theory of discourse representation.	"the causality-by-default hypothesis states: ""because experienced readers aim at building the most informative representation, they start out assuming the relation between two consecutive sentences is a causal relation"" - Sanders (2005) ."	entailment
train_2765	"As discussed above, we optimize the anaphoricity model for coreference performance via the conservativeness parameter."	we will use this parameter to maximize the F-measure score for a particular data set and learner combination using held-out development data.	entailment
train_153299	"In our experimentation, we select Support Vector Machines (SVM) as the classifier since SVM represents the state-of-the-art in the machine learning research community."	"we use the SVM light (Joachims, 1998) with the convolution tree kernel function SVM light -TK (Moschitti, 2004) 2 to compute the similarity between two constituent parse trees."	entailment
train_55515	"On the other hand, our analysis revealed that many candidate word similarity pairs suggested by distributional similarity measures do not correspond to ""tight"" semantic relationships."	"many word pairs suggested by the LIN measure do not satisfy the lexical entailment relation, as demonstrated in Table 2."	entailment
train_215346	"After collecting the maximum possible number of tweets for each user, we filtered them."	"we removed the non-Arabic tweets, retweets, and short tweets that contain less than 3 words."	entailment
train_160752	"In this study we find that a simple retrieval model based on latent semantic analysis (Deerwester et al., 1990, LSA) works relatively well."	"we get semantic vector representations for each document and the given topical word by performing singular value decomposition on the term-document matrix, where each element corresponds to the tf-idf value for a particular word in the document."	entailment
train_122588	"In P2, on the other hand, we recast SC as a sequence labeling task."	"we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence."	entailment
train_167101	"The greater or lesser degree of prototypicality is given by the number of typical constructions of the class the verbs participate in: each class contains a group of ""core"" verbs that share all the syntactic and semantic characteristics of the general class, and fuzzy boundaries of less prototypical members."	"we argue that specific semantic properties of a specific verb meaning may ""block"" the realization of some constructions that are instead shared by most of the other verbs of a class, or instead add some idiosyncratic constructions as well."	entailment
train_132666	"Even with substantially restricted granularity, with high variance feedback, or with skewed rewards, this combination improves pre-trained models ( §6)."	"under realistic settings of our noise parameters, the algorithm's online reward and final heldout accuracies do not significantly degrade from a noise-free setting."	entailment
train_141081	"For example, measures such as first fixation duration were not included in any of the models, while revisits as late as the third run (the third time the eyes pass over the region of interest) occurred in these regions and provided a strong signal."	"useful features were the durations of the second and last fixations, as well as the information about the run (pass) during which they occur."	entailment
train_83840	"In this paper, we study the problem of multi-class fake news detection with multiple sources."	we aim to answer two major research questions -(1) how to effectively combine information from multiple sources for fake news detection and (2) how to discriminate between degrees of fakeness mathematically.	entailment
train_170619	"11,071 out of 40,168 Chinese Sina Microblogs were labeled uncertain following our proposed scheme with the Kappa value over 0.85 that indicated substantially reliable."	we define six sub-classes for the uncertain classification which made the corpus more detailed and useful for other related NLP applications in the future.	entailment
train_36042	We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically.	what kinds of information can be leveraged to uncover the potential prerequisite relation between knowledge concepts.	entailment
train_151305	A word weight should be defined based on its influence on IR.	weights are estimated so that WWER will be equivalent to an IR performance degradation.	entailment
train_2762	"Moreover, CBLO does not seem to be very effective in improving the baseline, in part due to the dramatic loss in recall."	"although we see improvements in F-measure in five of the 12 experiments in this group, only one of them is statistically significant."	entailment
train_137861	"While syntactic grammars (Marcus et al., 1993;Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives)."	"we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN BiLexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012)."	entailment
train_129014	"As a first step, we extend the CNN model above to explicitly account for document structure."	we apply a CNN to each individual sentence in a document to obtain sentence vectors independently.	entailment
train_76489	These voting-based approaches yielded better results over the first voting-based approach.	"we compared this new voting-based approach against the previous best model reported in Lu et al. (2008) – mixgram model, which was also based on a combination of unigram and bigram models."	entailment
train_2751	"To achieve global optimization, we construct a parametric anaphoricity model with which we optimize the parameter 1 for coreference accuracy on heldout development data."	we tighten the connection between anaphoricity determination and coreference resolution by using the parameter to generate a set of anaphoricity models from which we select the one that yields the best coreference performance on held-out data.	entailment
train_104025	"A recent related task is the SemEval 2016 tweets stance classification (Mohammad et al., 2016)."	"in its weakly supervised subtask (Task B), no labeled training data was provided for the single assessed topic (Donald Trump)."	entailment
train_65774	"Wiebe (1994), studies methods to track the change of ""point of view"" in narrative text (fiction)."	"the ""writer"" of one sentence may not correspond to the writer of the next sentence."	entailment
train_120597	"(2006)'s method, which is based on SVMs, we added the output of Model 2 as a new feature."	we first tuned the parameter vector x on the training data and acquired the knowledge for case alternation with the tuned parameter.	entailment
train_28508	The set of FEs is further split into core FEs and non-core FEs.	the core FEs are the essential components of a frame and can be defined by themselves.	entailment
train_153300	"Evaluation is done using 10-fold documentlevel cross-validation, each of which contains 90% of documents as the training data and 10% as the test data."	"for the AIMed corpus we apply the exactly same 10-fold split as widely used in a series of relevant studies (e.g., Bunescu et al., 2005a;Giuliano et al., 2006)."	entailment
train_188177	We view the probability computation of a dependency tree as a generation process.	"we assume dependency trees are constructed top-down, in a breadth-first manner."	entailment
train_135171	Cross-lingual NER attempts to address this challenge by transferring knowledge from a high-resource source language with abundant entity labels to a low-resource target language with few or no labels.	"in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language."	entailment
train_49780	"Then, we find the total number of unique templates each model produces, finding that each ""more informed"" model produces more unique templates: BASE produces 18k, +ADJ produces 22k, +SENT produces 23k, and +STYLE produces 26k unique templates."	"given the test set of 30k, +STYLE produces a novel templated output for over 86% of the input MRs."	entailment
train_49679	"First, deep transfer active learning (DTAL) achieves the best performance in the low-resource setting of each dataset."	dTAL outperforms the others to the greatest degree in Cora (97.68 F1 points) probably because Cora is the most complex dataset with 8 attributes in the schema.	entailment
train_75240	WePS is a competitive evaluation campaign that proposes several tasks including resolution of disambiguation on the Web data.	"wePS-1, wePS-2 and wePS-3 campaigns provide an evaluation framework consisting in several annotated data sets composed of English person names."	entailment
train_39908	"Such KG construction methods are schema-guided as they require the list of input relations and their schemata (e.g., playerPlaysSport(Player, Sport))."	knowledge of schemata is an important first step towards building such KGs.	entailment
train_61364	"Finally, we add a final feature indicating the target translation's monolingual frequency, which serves as a sort of prior probability that the target word is of interest at all."	we define this feature as the inverse of the log of the target word's frequency.	entailment
train_149412	"We expect the generated poems to not only keep the semantic of the original text, but also demonstrate terseness, rhythm and other characteristics of ancient poems."	"we chose 20 famous fragments from four types of Chinese literature (5 fragments for each of modern prose, modern poems, pop song lyrics and Song lyrics)."	entailment
train_3334	"Based on these benchmarks, using either manually truthed corpora or automatically constructed corpora, using either ambiguous corpora or unambiguous corpora, our algorithm consistently and significantly outperforms the existing algorithm."	our system achieves a very high precision (0.96 precision).	entailment
train_126001	"Our motivation is to apply a rough but robust reordering to make the source and target sentences have more similar word orders, where fast align can show its power."	seg rev first segments a source-target sentence pair into a sequence of minimal monotone chunk pairs 6 based on the automatically generated word alignment.	entailment
train_62284	"However, recent work has drawn attention to the fact that the specific formalism for which these results were obtained, and which we will refer to as VW-CCG (after Vijay-Shanker and Weir), differs from contemporary versions of CCG in several important aspects."	"it allows one to restrict and even ban the use of combinatory rules on a per-grammar basis, whereas modern CCG postulates one universal set of rules, controlled by a fully lexicalized mechanism based on typed slashes, as in other approaches to categorial grammar (Baldridge 2002;Steedman and Baldridge 2011)."	entailment
train_130970	"True label discovery methods have been developed to resolve conflicts among multi-source information under the assumption of source consistency (Li et al., 2016;Zhi et al., 2015)."	"in the spammer-hammer model (Karger et al., 2011), each source could either be a spammer, which annotates instances randomly; or a hammer, which annotates instances precisely."	entailment
train_129021	This differs from previous comparisons in the context of classifying shorter texts.	"in previous work (Zhang and Wallace, 2015) we observed that CNN outperforms SVM uniformly on sentence classification tasks (the average sentence-length in these datasets was about 10)."	entailment
train_204737	"Whatever sanctions the movement of the object past the verb, the same will sanction leftward movement of the object to any other clause-internal position."	"if the object can make the first move, then it can make the subsequent move."	entailment
train_115347	"This more exhaustive rule extraction method permits a grammar simplification, as expressed by the phrase movement allowed by its rules."	a simple grammar with rules of only one nonterminal is shown to outperform a more complex grammar built on rules extracted from Viterbi alignments.	entailment
train_129658	"Accordingly, we also found it useful to use a ""curriculum beam"" strategy in training, whereby the size of the beam is increased gradually during training."	"given a desired training beam size K tr , we began training with a beam of size 2, and increased it by 1 every 2 epochs until reaching K tr ."	entailment
train_210571	"Kilgarriff et al. (2004) describe an interesting approach for extracting collocations (i.e., word sketches), and to build a concordance and thesaurus."	"they extract grammatical relations of a lexicon using hand-crafted rules, count the frequency of a word which is connected to another word by a specific grammatical relation (e.g., SUBJ OF, OBJ OF) (Lin, 1998), and weigh its collocations by logDice (Rychly, 2008)."	entailment
train_169468	"Hypothesizing that some verb senses and argument roles defined in VerbNet (Schuler, 2005) are useful for identifying text spans corresponding to opinions and their arguments as well as their relationships 4 , we introduce three constraints that exploit the PropBank-style semantic role labels provided by Mate Tools (BjÃ¶rkelund et al., 2009)."	"constraint (6) states that (1) a span candidate associated with a particular verb sense should be assigned a non-none label, and (2) a span candidate associated with a verb argument should be assigned a non-none label."	entailment
train_90654	"For predicate, only nouns and verbs are considered possible candidates."	all words without a split PoS in these two categories are filtered out.	entailment
train_156246	"In this study, we explore how the power differential between the candidates manifests in these debates."	we use the 20 debates held between May 2011 and February 2012 as part of the 2012 Republican presidential primaries.	entailment
train_46107	"To remedy this problem, we propose an unsupervised adaptation method which finetunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus."	"we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences."	entailment
train_190163	"Before introducing the attr2vec model we briefly describe the GloVe factorization approach (Pennington et al., 2014)."	gloVe employs a matrix factorization model using as input the word-word co-occurrence count.	entailment
train_211047	"Therefore, the answer to why the A-scrambled non-subject in SpTP controls only certain subject properties could be related to the distinction between high/Pivot and low/PA subject properties in Korean."	"if reflexive binding and wide scope are properties available to a nominal in the high/derived subject position while HA and PC are properties of the low/thematic subject position, the fact that the A-scrambled object fails to control the latter two properties would not constitute a counterexample to the analysis that takes Ascrambling to involve movement to SpTP."	entailment
train_143581	"While outperforming end-to-end dialogue models on bAbI Dialog Tasks in a zero-shot setup  due to its prior linguistic knowledge in the form of a dialogue grammar, this method inherited the limitations of it as well."	it's limited to a single domain until a wide-coverage grammar is available.	entailment
train_198043	"Since the source language and the assisting language (English) have different word order, we hypothesize that it leads to inconsistencies in the contextual representations generated by the encoder for the two languages."	"given an English sentence (SVO word order) and its translation in the source language (SOV word order), the encoder representations for words in the two sentences will be different due to different contexts of synonymous words."	entailment
train_212691	We propose using similar information to predict the plausibility of arbitrary verb-noun pairs.	we aim to learn the distinguishing visual features of all nouns that are plausible arguments for a given verb.	entailment
train_18094	"We attempt the more ambitious goal of separately associating a vector to nouns and adjectives, and determining the color of an object by the nearness of the noun denoting the object to the color term."	"we are trying to model the meaning of color terms and how they relate to other words, and not to directly extract the color of an object from pictures depicting them."	entailment
train_66085	"On the other hand, our proposed method deals with only the similar case frames to the cached ""orosu (1)""."	"the case frame ""orosu (2)"", which means ""withdraw money from bank or post"", is not similar to ""orosu (1)"", and is not used."	entailment
train_94496	"In the current implementation, we focus on a subset of mentions to further improve the mention detection stage of the baseline system."	we fix mentions starting with a stop word and mentions ending with a punctuation mark.	entailment
train_140852	We now turn from the specialized domain of biomedical abstracts to more general applications.	"we consider learning disentangled representations of beer, hotel and restaurant reviews."	entailment
train_57986	We use the Columbia Arabic Treebank (CATiB) (Habash and Roth 2009).	"we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information."	entailment
train_98115	"The FASC baseline strengthens ASC by removing word types that are less likely to appear in groundtruth captions, e.g., ""ah"", ""he"", ""hello,"" or ""wish""."	"we only keep words with high P (w | GT ) P (w | ASR) values, i.e., words that would be indicative of the groundtruth class if we were to build a Naive-Bayes classifier with addone smoothing; probabilities are computed only over the training set to reduce the risk of overfitting."	entailment
train_7532	"In this study, we explore approaches that can segment multiparty conversational speech by integrating various knowledge sources (e.g., words, audio and video recordings, speaker intention and context)."	"we evaluate the performance of a Maximum Entropy approach, and examine the effectiveness of multimodal features on the task of dialogue segmentation."	entailment
train_197197	In this paper we follow the same line of study: We consider how to adapt RNN sequence model to multi-label set prediction without specifying the label order.	"we make the following contributions: 1. We analyze existing RNN models proposed for multi-label prediction, and show that existing training and prediction objectives are not well justified mathematically and have undesired consequences in practice."	entailment
train_107134	Sun et al. (2002) proposes to use a class-based model for Chinese NE recognition.	"it uses a character-based trigram model for the class person, a word-based model for the class location, and a more complicated model for the class organization."	entailment
train_83287	This work takes the lead to study the aspect/sentiment-aware abstractive review summarization in an end-to-end manner without hand-crafted features and templates by exploring the encoder-decoder framework and multi-factor attentions.	"we propose a mutual attention mechanism to interactively learns the representations of context words, sentiment words and aspect words within the reviews, acted as an encoder."	entailment
train_11164	We find that our bilingual rankings have good monolingual ranking properties.	"given an optimal pairwise bilingual ranking, we show that simple heuristics can effectively approximate the optimal monolingual ranking."	entailment
train_17740	We will therefore refer to our model as the variable mixture model (VMM).	"our model differs from linear or log-linear interpolation models (Klakow, 1998), which combine a typically small number of components that are common across instances."	entailment
train_103320	"Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations."	new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors.	entailment
train_132178	"In this work, we provide a methodology that overcomes several limitations of existing approaches to extractive question answering."	"our proposed model, the Globally Normalized Reader, reduces the computational complexity of previous models by casting the question answering as search and allocating more computation to promising answer spans."	entailment
train_61337	"In sentiment analysis, discourse structure provides a crucial link between the local sentence level and the entire document (article, conversation, blog post, tweet, headline) and is needed for a better understanding of the opinions expressed in text."	discourse can help in three main tasks: (1) identifying the subjectivity and polarity orientation of evaluative expressions; (2) furnishing important clues for recognizing implicit opinions; and (3) assessing the overall stance of texts.	entailment
train_105345	The objective is to optimise ROUGE-1 by maximising the number of word tokens paired up between system and reference summaries.	"one is trying to choose the sentences, within budget, that cumulatively maximise the number of unigram tokens that can be paired with those of reference summaries."	entailment
train_13552	"Specifically, the model includes a flag for postlexical integration failure, that -when triggered -will instruct the model to produce a between-word regression to the site of the failure."	between-word regressions in E-Z Reader 10 can arise because of postlexical processes external to the model's main task of word identification.	entailment
train_211048	"Yoon (2007, 2009) argued that among the subject diagnostics proposed for Korean (Yoon 1986, Youn 1990, Hong 1991, etc.), some are low/PA subject properties while others are high/Pivot subject properties."	"he argued that HA and PC are PA/lower, subject properties, which are not controlled by high/Pivot subject (cf. 2b, 2d), but only by the lower subject (cf. 2a, 2c). "	entailment
train_52804	"Strube and Hahn (1999) argue that in German, the rank of discourse entities is determined by the position they hold in Prince’s (1981, 1992) givenness hierarchy"	"strube and Hahn argue that hearer-old entities rank higher than mediated entities, which and in turn rank higher than hearer-new entities: HEARER-OLD â‰º MEDIATED â‰º HEARER-NEW."	entailment
train_123154	"Among them, 14 words (e.g., crush, order, etc.) are in both the +effect word set and the -effect word set."	these words have both +effect and -effect meanings.	entailment
train_142111	"Similar to prior work on sentence embeddings (Kiros et al., 2015;Hill et al., 2016), we use an encoded sentence representation to predict its surrounding sentences."	we predict the immediately preceding and succeeding sentences.	entailment
train_153284	This is done by taking advantage of the shortest dependency path between two involved proteins in the dependency parse tree structure of a sentence.	only the words appearing on the shortest dependency path and their associated constituents in the constituent parse tree are considered as necessary and thus kept as the essential part of the constituent parse tree.	entailment
train_169469	We create one ILP program for each test sentence.	"for each test sentence, let O be the set of opinion candidates (provided by the 30-best CRF output), A k be the set of argument candidates (also provided by the 30-best CRF output), where k denotes the relation type (Is about or Is from), and S be the union of O and A k ."	entailment
train_95113	"Our work incorporates an NLI component, but departs from the performance optimization orientation towards leveraging computational analysis for better understanding of the relations between native language typology and ESL usage."	our choice of NLI features is driven by their relevance to linguistic typology rather than their contribution to classification performance.	entailment
train_145231	"To make sure that the IAA of good squared and good annotations is not the result of a different sample size, we randomly sampled 10%, 20%, ..., and 90% of all the annotations."	"we adopted random sampling as a baseline method to compare with our proposed method of the twostep reason selection, and checked the effect of the sample size on IAA."	entailment
train_2766	"The experimental setting employed here is essentially the same as that in the CBGO setting, except that anaphoricity information is incorporated into the coreference system as a feature rather than as constraints."	"each training/test instance i(NPi,NPj ) is augmented with a feature whose value is the computed anaphoricity of NP j ."	entailment
train_169464	"Given this setup, if the opinion-argument classifier is highly confident that an Is about relation exists between two candidate entities, then these two entities will likely be extracted as an opinion and a target even if the entity extraction component fails to extract them or the target-implicit classifier erroneously determines that the opinion candidate is target-implicit."	"the final entity extraction decisions and relation extraction decisions will be made jointly by the entity extraction model, the argument-implicit classifiers, and the opinion-argument classifiers by considering the confidence values they individually assign to the extraction decisions."	entailment
train_115130	This section focuses on an orthogonal direction: improving literal strategies with learning.	we construct learned strategies from log-linear models trained on human annotations.	entailment
train_10120	"Unfortunately, there is no straightforward generalization of the method of  to the two edge marginal problem."	"laplace expansion generalizes to second-order matrix minors, but it is not clear how to compute secondorder cofactors from the inverse Kirchoff matrix alone (c.f. (Smith and Smith, 2007))."	entailment
train_98801	"Motivated by previous findings, which showed that direct authorship attribution is not feasible in a large scale scenario, we contribute to this field by proposing a two-step approach in this study."	we propose at first to reduce the number of candidate authors while keeping the correct author in the reduced set with reasonable accuracy.	entailment
train_198074	Our experiments test the performance of an off-the-shelf system for lexical relation classification under different cross-lingual settings.	"we evaluate the performance of unsupervised cross-lingual word embeddings and assess the benefits of translation and re-ranking, taking into account word co-occurrences in contrast relations."	entailment
train_125634	"As in the LINK classifier, we enforce global role constraints when creating training instances for the MOVELINK classifiers."	"the roles assigned to the spatial elements in each training instance of each MOVELINK classifier are subject to six constraints: (1) the trigger has type motion event; (2) the mover has type place, path, spatial entity, or non-motion event; (3) the source, the goal, and the landmark can be NULL or have type place, path, spatial entity, or non-motion event; (4) the midpoint can be NULL or have type place, path, or spatial entity; (5) the path can be NULL or have type path; and (6) the motion signal can be NULL or have type motion signal."	entailment
train_130968	"And our method infers born-in as the true label for the first two relation mentions; after replacing the matched contexts (born) with other words (elected and examined), our method no longer trusts born-in since the modified contexts are no longer matched, then infers None as the true label."	our proposed method infer the true label in a context aware manner.	entailment
train_218583	"In this work, we look beyond syntactic properties of the text and incorporate semantic properties to the tree-structured LSTM model."	we utilize the network topology offered by a tree-structured LSTM and incorporate semantic features induced by AMR formalism.	entailment
train_6857	"Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation."	"we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder."	entailment
train_75514	Evaluation on a largescale dataset shows that our joint model performs much better than several baselines.	it shows that the performance on personal skill inference can be greatly improved by incorporating skill connection information.	entailment
train_218067	"Our results substantially outperform informed baselines, in spite of the availability of only small amounts of training data."	we also demonstrate the relevance of event ordering information provided by script knowledge.	entailment
train_92599	Olsen et al. (2001) relied on LCS to generate appropriately-tensed English translations for Chinese.	they addressed tense reconstruction on a binary taxonomy (present and past) for Chinese text and reported that incorporating lexical aspect features of telicity can obtain a 20% to 35% boost in accuracy on tense realization.	entailment
train_66134	"It has been reported that stochastic parsers degrade in performance on domains different than what they were trained on (Hwa, 1999; Gildea, 2001), so there really was an issue whether the output would be good enough."	we are taking the Collins parser trained on the Wall Street Journal and applying it unchanged to spontaneous human-human dialog in an emergency rescue task domain.	entailment
train_182400	"This can be detected by not limiting our verification to comparing behavior to the professional editor's, but also by comparing submitted edits to the MT output itself and to the human reference."	"a worker's submission could be characterized in terms of its distance to the MT output and to the human reference, thus building a complete 'profile' of the worker, and adding another component to guard against poor data quality and to reward the desired behavior."	entailment
train_26487	Table 1 presents average results for ROUGE-1 in the four categories.	the SB and TFIDF summarisation techniques consistently outperform the TT baseline across all four categories.	entailment
train_103587	"Even though we focus on a specific domain, the induction method can be easily translated to other domains."	"other life-style domains, such as fashion, cosmetics or home & gardening, show parallels since comparable textual web data are available and similar relation types (e.g.that two items fit together or can be substituted by each other) exist."	entailment
train_218174	"In what follows, we detail experimental results on two quantitative evaluation tasks: at the local and global levels of predicate-argument structure."	"we evaluate on pseudo disambiguation of selectional preference, and semantic frame lexicon overlap."	entailment
train_36045	"In this paper, we attempt to figure out what kinds of information in MOOCs can be used to uncover the prerequisite relations among concepts."	"we consider it from three aspects, including course concept semantics, course video context and course structure."	entailment
train_83853	This function penalizes cross entropy by an intra-class sparsity.	it just tries to move samples toward their class centers.	entailment
train_118306	"Our approach for learning factored PCCGs extends the work of Kwiatkowski et al. (2010), as reviewed in Section 7."	"we modify the lexical learning, to produce lexemes and templates, as well as the feature space of the model, but reuse the existing parameter estimation techniques and overall learning cycle, as described in Section 7."	entailment
train_197170	"Following previous work (Zhang and Lapata, 2017;Zhao et al., 2018), we use SARI as our main automatic metric for evaluation (Xu et al., 2016)."	"11 SARI calculates how often a generated sentence correctly keeps, inserts, and deletes n-grams from the complex sentence, using the reference simple standard as the gold-standard, where 1 â‰¤ n â‰¤ 4."	entailment
train_37042	"The approach to RTE taken in this paper follows the Lexical Overlap Hypothesis (LOH), which states that the higher the number of lexical matches between T and H, the more likely the T-H pair is entailing rather than non-entailing (Zeller, 2016)."	H is more likely to be entailed by T if most of its lexical content also occurs in T.	entailment
train_196289	"Using more data for training – i.e., our artificial training data which does not include context errors – generally boosts performance on data with and without grammatical errors in the context."	"training with additional artificially generated errors seems, overall, to be making our model more robust."	entailment
train_218068	"To test the model in a scenarioindependent setting, we perform additional experiments based on a cross-validation with the 10 scenarios as one fold each and exclude the script features."	"we repeatedly train our model on 9 scenarios and evaluate on the remaining scenario, without using any information about the test scenario."	entailment
train_182398	"On that portion, we evaluate how closely the Turker matches the LDC editor, and weight them accordingly when predicting the number of edits of the rest of that group's segments."	the Turker's weight is the absolute difference between the Turker's edit count and the professional editor's edit count.	entailment
train_155098	"That is, TO was assigned to the cases when the accuracy of WSD when a classifier was trained with ten labeled word tokens of the target data and tested using a leaveone-out cross-validation method was higher than the accuracy of WSD when a classifier was trained with the source data and tested using ten labeled word tokens of the target data."	this indicates that the simulation using ten manually labeled word tokens of the target data was an important clue in predicting the optimal DA method.	entailment
train_6383	"Given a sentence, the task of Semantic Role Labeling (SRL) consists of analyzing the logical forms expressed by some target verbs or nouns and some constituents of the sentence."	for each predicate (target verb or noun) all the constituents in the sentence which fill semantic arguments (roles) of the predicate have to be recognized.	entailment
train_204819	"Following Gruber (1965), Tenny points out that a second delimiter is allowed only when it is to further specify the endpoint denoted by the first delimiter."	"when the first delimiter specifies a change of location and the resultant location can be ""refined or elaborated upon"" (Gruber 1965: 82, cf. Hay et al 1999, Kennedy & McNally 2005, among others), a second delimiter can occur in order to describe the location in more detail."	entailment
train_197199	"If only some sequence permutations receive high probabilities while others receive low probabilities, the set probability computed as the product of sequence probabilities will still be low."	"if for each document, RNN finds one good way of ordering relevant labels (such as hierarchically) and allocates most of the probability mass to the sequence in that order, the model still assigns low probabilities to the ground truth label sets and will be penalized heavily."	entailment
train_196811	"In addition, to facilitate attribute classification, we want to capture the semantic relations between words and their labels."	we want the words and their attribute labels to be close to each other in the embedding space and the embeddings of different labels to be far away from each other.	entailment
train_218063	Table 2 shows that our model outperforms the baseline Word2Vec Skip-gram model (in fifth row from bottom).	"on the RW dataset, MSWE obtains a significant improvement of 2.92 in the Spearman's rank correlation (which is about 8.5% relative improvement)."	entailment
train_46161	"might be seen as special cases in this framework, we note that only their decoders are prefix-to-prefix, while their training is still fullsentence-based."	"they use a fullsentence translation model to do simultaneous decoding, which is a mismatch between training and testing."	entailment
train_198078	"Compared to the NoTrans baseline, we find BestTrans to substantially increase the number of correctly classified synonyms (by almost 20%)."	"this affects words derived from the same stem (unzÃ¤hlig/zahllos 'countless'), synonymous verbs (pfuschen/schummeln 'cheat') as well as (near-)synonymous adjectives (verfÃ¼gbar/verwendbar 'available/usable')."	entailment
train_65778	We therefore include several features that encode the relative position of pse parent and pse target in the sentence.	we add a feature that is 1 if pse parent is the root of the parse (and similarly for pse target ).	entailment
train_136895	Restricting generalization One of the strengths of the transition-based approach is its ability to recognize previously unseen forms of causal language that resemble known connectives semantically and/ or linguistically.	the F 1 metrics reflect exact span matches; it is counted as a mismatch if even a single word is off.	neutral
train_83860	"In this setting, we use Center Loss and cross entropy."	it is crucial to classify fake news into multiple classes reflecting degrees of fakeness.	neutral
train_34	"Speech recognisers has reached the high-street store, and a significant proportion of the developed world's population has now been exposed to the possibility of controlling one's computer, or creating a document, by voice."	"spoken language understanding continues to be an elusive goal, and the prosodic linkage between acoustic and linguistic patterning is still something of a mystery."	neutral
train_107857	"It is the first in a series of experiments aiming to leverage redundancy across a large fact base extracted from text, to improve the quality of extracted data."	"we aim to use a much larger set of data to train the parameters, without manually tagging large training sets."	neutral
train_546	Some of the complexity in the RICHES system is there to demonstrate the potential for different types of control strategies.	"NLG systems, especially end-to-end, applied nLG systems, have many functionalities in common."	neutral
train_169894	"Our other serious interest is focused on the DNN-based LVCSR for Slovak using Kaldi (Povey et al., 2011)."	we report here the preliminary results (see Table 2.2.)	neutral
train_168414	"KorAP is developed at the Institute for the German Language (IDS), member of the Leibniz-Community 11 and supported by the KobRA 12 project, funded by the Federal Ministry of Education and Research (BMBF)."	it can be a good candidate for performance comparison in the future and may serve as an alternative search backend.	neutral
train_117296	"Finally, while the predictive keyboard is a commonly studied interface, it is not appropriate for all AAC users."	"in order to build a good long-span language model, we would require millions of such communications."	neutral
train_128800	"The thread on fine grained opinion expressions Choi et al., 2006;Breck et al., 2007) focus on subjective expression extraction which are generic instead of aspect specific."	"in opinion mining, it is often desirable to mine the aspect specific opinion expressions (or aspectsentiment phrases) containing both the aspect and the opinion."	neutral
train_69147	"Finally, the Hellinger distance and kernels squash the variance associated with c through the square root function."	"the first is the 90 million word written component of the British National Corpus (Burnard, 1995)."	neutral
train_131801	"In particular, our context window size is five."	"as shown in Table 5, using word-context character (WCC) embeddings and multi-view wordcontext character embeddings both give significantly higher accuracy improvements compared with other semi-supervised methods."	neutral
train_106444	Text markers and cue words are discussed in section 2.4.	"they include characters such as '(â€¦)', '[â€¦]', and '='."	neutral
train_85818	Learning-rate schedule and early-stopping -The code base did not include an implementation of the learning-rate schedule that was reported to be used in the original experiments.	"the exact configuration of the baseline is not discussed in the paper, however, the authors do share that they implement both layer-normalization and the output module."	neutral
train_150767	"With graph refinement frameworks such as the one presented here, many of these resources may be improved automatically."	"vERBOCEAN is a graph of semantic relations between verbs, with 3,477 verbs (nodes) and 22,306 relations (edges)."	neutral
train_155019	"How to conduct an efficient and effective method for sentences ordering is a difficult but important task for both multidocument summarization and other text processing job, e.g. Question Answering."	chronological information cannot be easily extracted from those non-news documents and constructing a large corpus also is not so easy.	neutral
train_192979	"Problem: In a stream decoding scenario, the entire source sequence is not readily available."	"we were not able to improve upon our incremental decoder, with the results deteriorating notably."	neutral
train_65550	"We followed the three-point grading scheme previously developed for the CSTAR consortium, as described in [Levin, L. & al. 2000]."	we have to develop the analysis from the textual output of an automatic speech recognition module towards the IF and the generation from the IF towards a text-to-speech input text.	neutral
train_2466	"If we use all data for both the rules and memory-based learning, we have to weight the methods to combine them."	"unless we concentrate on the postpositions, we must enlarge the neighboring window to get a good hypothesis."	neutral
train_388	Some work has also started on building search engines.	"besides the parsers mentioned above, a parsing formalism called UCSG identifies clause boundaries without using sub-categorization information."	neutral
train_4041	"Word sense disambiguation is often assumed to be an intermediate task, which should then help higher level applications such as machine translation or information retrieval."	"the first voting model is a naive Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data."	neutral
train_61349	"With the rise of social media, tracking follow-up evaluations and how they change towards a topic over time has become very popular (Wang et al. 2012;Farzindar and Inkpen 2015)."	"hunston and Thompson (2000a), in their Introduction, propose that there are two aspects to evaluative language: modality and something else, which is variously called evaluation, appraisal, or stance."	neutral
train_204915	Our parser shows a competitive performance in rule-based parsing.	the scalability problem indicates the difficulty in incorporating other types of knowledge for use in parsing.	neutral
train_220116	"We trained 100-dimensional skip-gram vectors (Mikolov et al., 2013) on English Wikipedia (tokenized/lowercased, resulting in 1.8B tokens of text) using window size 10, hierarchical softmax, and no downsampling."	"if the current word is a local context word, we sample a new topic/sense pair for it again conditioned on all other latent variable values."	neutral
train_26104	Natural language generation faces many linguistic challenges.	"most patterns use the lexical form of the main verb along with the appropriate form of the auxiliary do (do, does, did), for the subject-auxiliary inversion required in forming interrogatives."	neutral
train_153225	"In the GIVE scenario (Byron et al., 2009), users try to solve a treasure hunt in a virtual 3D world that they have not seen before."	"to the online experiment, 31% of participants were male and 65% were female (4% did not specify their gender)."	neutral
train_33702	Such clustering is usually performed through manually defined semantic and syntactic features defined over argument instances.	"similar to ours, L&G and Arg2vec both encode dependency relations in the embeddings."	neutral
train_15214	"For example, (Hu and Liu, 2004;Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text."	"We choose to investigate these two methods since they have been widely used in text and speech summarization, and perform competitively."	neutral
train_76496	"In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a model that can parse novel input sentences into their respective semantic representations."	"each semantic unit can be regarded as a function which takes in other semantic representations of specific types as arguments, and returns a new semantic representation of a particular type."	neutral
train_105352	"Because the ROUGE optimisation problem is NP-hard, one may suspect that exchanging a greedy strategy out for an exact global approach would lead to substantial improvements in system performance."	"by gold summaries, we refer to optimal summaries consisting of sentences from the input document."	neutral
train_192654	Accuracies are comparable to previous work (which was on different language pairs).	we design a new task to evaluate bilingual word embeddings on rare words in different domains.	neutral
train_158304	"To remedy this problem, we use a classifier (specifically logistic regression) to determine whether a modified tree should be used. "	building treebanks is expensive and timeconsuming for humans.	neutral
train_162498	Transforming it into a three-player game helps to improve the performance of the evaluation set while lower the accuracy of the complement predictor.	A rationalization scheme z(X) that simultaneously satisfies Eqs. (4)-(6) is the global optimizer of Eq. (8).	neutral
train_13646	"Besides, when users post messages in the online forums, they are accustomed to be casual and use some synonymous words interchangeably in the posts, which is believed to be a significant situation in Chinese forums especially."	a specially designed model may learn semantic knowledge by reconstructing a great number of questions using the information in the corresponding answers.	neutral
train_124072	"In SMT, maximum entropybased reordering model is often introduced as a better alternative to the commonly used lexicalized one."	"in this situation, using dual MLR is very beneficial."	neutral
train_73914	"In this graph, we distinguish two types of nodes: nests and plain nodes."	The initial amount of energy per node (E 0 ) and the ant life-span (ω) influence the number of ants that can be produced and therefore the probability of reinforcing less likely paths.	neutral
train_158772	"In the light of recent research (Volkova et al., 2013;Hovy, 2015;JÃ¸rgensen et al., 2015), we explore the hypothesis that these biases transfer to NLP tools induced from these resources."	we then map each review to the NUTS region corresponding to the location.	neutral
train_5518	We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited.	the last column of table 3 shows the performance of our methods on the final tESt set.	neutral
train_30438	"Span ""(C)"" corresponds to a subtree."	questions are paired with Prolog-style queries.	neutral
train_159959	"As the METEOR and ROUGE evaluations show, this is not a problem for the final result."	"if n concepts are selected, n units of flow are sent from the root over the edges of the graph and each selected concept consumes one of them."	neutral
train_217539	Only 109 of the 1453 senses identified in Portuguese did not have an equivalent verb sense in English identified in Propbank.	"this approach is part of a previous decision to invert the process of implementing a Propbank project, by first annotating a core corpus and only then generating a lexical resource to enable further annotation tasks."	neutral
train_116466	"Other papers use margin methods such as MIRA (Watanabe et al., 2007; Chiang et al., 2008), updated somewhat to match the MT domain, to perform incremental training of potentially large numbers of features."	"the global variable L stores hypotheses combination matrices, one matrix for each range of sentences (s, t) as shown in Fig. 4, to determine which combination to try next."	neutral
train_64445	"Next, the paper proceeds to describe in more detail the anaphora resolution module."	"the bracket crossing accuracy is 80%, which is comparable to some probabilistic approaches."	neutral
train_221883	"Figure 1: Syntactic trees of the sentence ""The little boy likes red tomatoes.""."	"such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages."	neutral
train_124479	The models are considerably better at resolving explicit referrals (both non-spatial and spatial) compared to implicit ones.	"the user refers to the item using distinguishing features other than the title, such as the release or publishing date, writers, actors, image content (describing the item image), genre, etc."	neutral
train_113473	"For a statistical parser to cope with nonconfigurational phenomena as observed in, for instance, Hebrew or German, it should allow for flexibility in the form of realization of the grammatical functions within the phrase-structure representation of trees."	"in our case, there is more statistical evidence in the data for, e.g., case marking patterns, than for association of grammatical relations with structurally-marked positions."	neutral
train_23931	"For example, ""price"" and ""expensive"" are topically related in phone's domain, so they are a word preference for each other."	we perform experiments on real-world datasets from different languages and different domains.	neutral
train_196679	"While most pure character-level models cannot ensure consistent labels for each character of a token, our semi-CRF outputs correct segments in most cases (tokenization F 1 is 98.69%, see Table 4), and ensures a single label for all characters of a segment."	we calculate joint tokenization and UPOS (universal POS) F 1 scores.	neutral
train_174061	Formants in unvoiced or silent parts of the signal were either interpolated linearly from the two adjacent voiced parts or horizontally extended at the initial or final voiced portion of the recording.	when comparing the average RMSE error value for female speakers in PRAAT (194Hz) with male speakers in SNACK/ASSP (234/177Hz) we can see that the performance was in the same range (underlined values in Table 2).	neutral
train_162704	"Other examples, an analysis of the US Presidential Election in 2016 (Allcott and Gentzkow, 2017) revealed that fake news was widely shared during the three months prior to the election with 30 million total Facebook shares of 115 known pro-Trump fake stories and 7.6 million of 41 known pro-Clinton fake stories."	"one is that the number of training examples in RumourEval (including 5,568 tweets) is relatively limited as compared with PHEME (including 105,354 tweets), which is not enough to train deep neural networks."	neutral
train_17818	"We obtain the best results for Arabic training using labeled data from the Bulgarian treebank, and the best results for Bulgarian training on Portuguese only."	"Our labels seem meaningful, but come from different treebanks, e.g. ’pnct’ from the Danish treebank and ’PUNC’ from the Portuguese one."	neutral
train_156116	"L 1 regularization method (Tsuruoka et al., 2009)  model with the +id feature setting on both the development set and training data set, respectively, and their hyperameters are tuned on the dev-test set."	"experiments on IWSLT Chineseto-english translation tasks show that, with the help of grouping these features, our method can overcome the above pitfalls and thus achieves significant improvements."	neutral
train_69249	An isolator occurs when the only hypernym is in a different semantic category to the synset under investigation.	the present study looks for any of the listed faults or potential faults within WordNet 3.0.	neutral
train_83242	Delete-based approaches remove unimportant words from the source sentence and generate a shorter sentence by stitching the remaining fragments together.	we adopt the dataset provided by Toutanova et al. (2016) for our experiments.	neutral
train_48618	"We also compute HPE-skew, the (positive) skewness of the empirical distribution of HPE on the corpus tokens."	"note that to ensure a false discovery rate of at most = .05, all reported -values have to be corrected using Benjamini and Hochberg (1995)’s procedure: only 푝 ≤ .05·5/28 ≈ 0.009 is significant."	neutral
train_11408	Figure 1 illustrates this using the words pineapple and apple pie.	"instead of word sequences sequences of characters are considered, as proposed by Denoual & Lepage (2005)."	neutral
train_55550	Utilizing a refined definition of substitutable lexical entailment both as an end goal and as an analysis vehicle for distributional similarity.	these findings suggest a direct indication of an improved quality of the bootstrapped feature vectors.	neutral
train_151311	"From this point of view, word error rate (WER), which is the most widely used evaluation measure of ASR accuracy, is not an appropriate evaluation measure when we want to use ASR systems for IR because all words are treated identically in WER."	"for well-defined IRs such as relational database retrieval (E. Levin et al., 2000), significant words (=keywords) are obvious."	neutral
train_126008	"It has been reported that the fast align approach is more than 10 times faster than baseline GIZA++, with comparable results in end-to-end French-, Chinese-, and Arabic-to-English translation experiments."	aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems.	neutral
train_183427	"When co-referent text mentions appear in different languages, these techniques cannot be easily applied."	"we learn the parameters Î» using a quasi-Newton procedure with L 1 (lasso) regularization (Andrew and Gao, 2007)."	neutral
train_125647	The instances for training the remaining six classifiers are generated similarly.	it will be used in combination with the original 31 features.	neutral
train_122003	"Therefore, we can make better corrections using such context information on nodes."	"the CoNLL-2013 shared task is one of the most famous, which focuses on correcting five types of errors that are commonly made by non-native speakers of English, including determiner, preposition, noun number, subject-verb agreement and verb form errors."	neutral
train_78284	"Second, to combine these two parts of signals, there are connections between two bi-directional LSTMs where the zero step of hidden layers on IL side is initialized by the last step of hidden layers on English side."	"we have tag are look-up tables for English/IL word, English/IL character, and English tag respectively."	neutral
train_153310	"Since determining protein interaction partners is crucial to understand both the functional role of individual proteins and the organization of the entire biological process, there is a significant interest in protein-protein interaction (PPI) extraction."	4) Merge any two consecutive NP/VP nodes along the paths into a single one.	neutral
train_52848	"This claim is further supported by the evidence concerning the Repeated Name Penalty (Gordon, Grosz, and Gillion 1993)."	we return to this issue in Section 5.	neutral
train_161396	This makes the prediction of those slots more challenging for the system.	gCAS's act-slot pair performance is far from perfect.	neutral
train_20013	"Contrary to the bipartite graph, one-mode projections are directed as they follow the text order."	table 7 shows the results obtained for the comparisons between the two first categories and the Britannica Student articles.	neutral
train_89103	Results were surprisingly good for target words from the Hong Kong subcorpus.	"it is important to see if the classification could accommodate new words from heterogeneous data sources, and whether simple similarity measures and clustering methods could cope with such variation."	neutral
train_111831	"Moreover, a SuperARV language model was presented (Wang and Harper, 2002), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model."	"through the reranking paradigm, a new best sentence hypothesis is obtained."	neutral
train_35705	We also thank Alexander M. Rush for providing the dataset for comparison and helpful discussions.	"in abstractive sentence summarization, there is no explicit alignment relationship between the input sentence and the summary except for the extracted common words."	neutral
train_154388	This paper proposes a novel idea of combining WordNet and ConceptNet for WSD.	"for ambiguous concepts with multiple senses, there are multiple nodes in the network."	neutral
train_120606	"After checking all combinations, the combination with the highest score is output."	table 7 shows an example of case alternation between the passive and active voices.	neutral
train_167359	"Additionally, this study improves on previous works by using actor model, which enables us to take advantage of all available cores by highly parallelizing the corpus building process."	"it is therefore worth investigating how early deduplication, e.g.  removing copyright notices, influences resulting corpora. "	neutral
train_149716	The proposed method benefits from multiple transferable knowledge and shows competitive performances with the state of the art using limited resources.	"we replace English words with target language words if the translation pairs exist in the bilingual dictionaries (Rolston and Kirchhoff, 2016)."	neutral
train_174188	"The way doctors deliver bad news related to damage associated with care has a significant impact on the therapeutic process: disease evolution, adherence with treatment recommendations, litigation possibilities (Andrade et al., 2010)."	"different gestures of both doctors and patients have been annotated: head movements, posture changes, gaze direction, eyebrow expressions, hand gestures, and smiles."	neutral
train_161170	"It is appealing to find that afte 3 iterations, the distribution of the voting weightsÎ± ij will converge to a sharp distribution and the values will be very close to 0 or 1."	"a natural question was raised, Will carefully designed aggregation operations help the Enc-Dec paradigm to achieve the best performance?"	neutral
train_178386	"We showed that using treelets and a tree-based ordering model results in significantly better translations than a leading phrase-based system (Pharaoh, Koehn 2004), keeping all other models identical."	"alignments are seldom purely one-to-one and monotone in practice; Figure 1b displays common behavior such as one-to-many alignments, inserted words, and non-monotone translation."	neutral
train_83490	"We compare our JTAV framework with: doc2vec (Le and Mikolov, 2014), a paragraph embedding approach;"	"the first is bi-modal retrieval, which utilizes one modality to find similar content in the other modality, such as finding the most relevant texts to a given image (Li et al., 2017)."	neutral
train_2771	The principle of software modularity calls for local optimization.	"since our goal is to evaluate the effect of anaphoricity information on coreference resolution, we make no attempt to modify our system to adhere to the rules specifically designed for ACE."	neutral
train_140654	"As more tasks are added step by step, MTLE produces increasing performance gains for each task and achieves an average improvement of 5.5% when all the four tasks are trained together."	models of these papers ignore essential information of labels and mostly can only address pairwise interactions between two tasks.	neutral
train_87832	"The first two are large-scale generic thesauruses, both constructed using the similarity metric described in (Lin, 1998b), but based on different corpora."	similarity-based techniques do not discard any data.	neutral
train_159101	"Our models do not require any handcrafted iSRL annotations for training, and thus can be applied to all predicates observed in large unannotated data on which they are trained."	"for a good reader, a reasonable interpretation of the second loss should be that it receives the same A0 and A1 as the first instance."	neutral
train_180192	"These filter responses, which can be computed very efficiently, are used as input to a learning algorithm that generates the final detector."	the experiments performed yield interesting results.	neutral
train_30121	Its has two free parameters: k1 to tune termfrequency saturation; and b to calibrate the document-length normalization.	"additionally, we compare BOW-CNN with six well-established IR algorithms available on the Lucene package (Hatcher et al., 2004)."	neutral
train_112866	The generic averaged perceptron training algorithm appears in Figure 3.	"averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008;Clark and Curran, 2007a) as well as on other NLP tasks."	neutral
train_164268	"Combining the output from annotators is required since some NEs have been discovered by both annotation engines, while others come from only one annotator."	one can also observe that only a little more than 50% of all NEs are associated with a KB node.	neutral
train_214465	"From a practical point of view in software application (real scenario) for the algorithms we also do not have the assurance that all documents given as examples of an author, have actually been written by the author in question."	"one of the principal evaluation labs for the dissemination, experimentation and collaboration in the development of methods for the authorship analysis is found in the PAN 1 lab associated to CLEF."	neutral
train_52991	The expanded versions go a long way toward making the individual contributions more convincing and easier to replicate.	"kracht's book gives a uniform introduction, which currently does not exist at this level, to an important area of mathematical linguistics."	neutral
train_131909	This paper presents a deep neural solver to automatically solve math word problems.	"by using single layer LSTMs with 128 nodes and a symmetric window of length 3, our model achieves 99.1% accuracy."	neutral
train_112055	Then the annotation experiment which was used to verify that the form-based representation can be understood and applied by other human annotators is discussed in Section 3.2.	"this is contrasted with a dialog structure recognition process (Alexandersson and Reithinger, 1997;Bangalore et al., 2006;Hardy et al., 2004), where pre-specified dialog structure components are recognized as a dialog progresses."	neutral
train_115351	"With this new rule type, G 2 allows phrase/rule concatenation before reordering with another hierarchical rule."	"alignment models estimated over the parallel text are used to generate these alignments, but these models are then typically used no further in rule extraction."	neutral
train_78283	A major problem of this framework is that it suffers from noises produced by word alignment errors.	but this method will fail if word alignment results are totally wrong.	neutral
train_125644	All other learning parameters are set to their default values.	employing sieves for spatial relation extraction obviates the need for such ad-hoc decisions.	neutral
train_42972	"Instead of conducting fusion on a holistic level, we innovate to leverage a sliding window to explore inter-modality dynamics locally."	"3D-CNN (Ji et al., 2013) is applied for visual feature pre-extraction."	neutral
train_218631	"However, we hypothesize that each relation may contain useful information about the others, and training on only one relation inevitably neglects some relevant information."	"in the multitask setting, relations are presented to the model in the order they are listed in the result tables within each batch."	neutral
train_36067	Prerequisite relations essentially can be considered as the dependency among knowledge concepts.	we then formally define the average video coverage (avc) and the average survival time (ast) of a concept a as follows.	neutral
train_91271	"By automatically annotating unlabeled bitexts with these bilingual models, we can train new monolingual models that do not rely on bilingual data at test time, but still perform substantially better than models trained using only monolingual resources."	"we are training the bilingual view to match the output of these same models, which can be trivially achieved by putting weight on only the monolingual model scores and never recruiting any bilingual features."	neutral
train_42286	"We begin with vanilla SEQ2SEQ models with attention (Bahdanau et al., 2015) and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of Narayan et al. (2017) and approaching their best RDF-aware method."	note that the number of simple sentences to generate is not given as part of the input.	neutral
train_4420	"In our case, the true positive rate is the fraction of boundaries correctly classified, and the false positive rate is the fraction of non-boundary positions incorrectly classified as boundaries."	we can also represent graph nodes with non-overlapping blocks of words of fixed length.	neutral
train_66759	Only those classes were used in the study that had 5 or more members.	we experimented with several similarity metrics and found that on both datasets Jensen-Shannon Divergence yields the best classification results (see Table 1).	neutral
train_23933	"It demonstrates that incorporating more prior knowledge into our algorithm would restrain other useful clues on estimating candidate confidence, and hurt the performance."	"we perform extraction with a global graph co-ranking process, where error propagation can be effectively alleviated."	neutral
train_42975	"Moreover, BC-LSTM is over 6 times faster than TFN in time complexity measured by FLOPs and the number of parameters is over 3 times smaller."	"we incorporate two levels of attention mechanism into ABS-LSTM, i.e., Regional Interdependence Attention and Global Interaction Attention."	neutral
train_172494	"The concept is overlapped with the concept of ellipsis, more specifically contained in the concept of ellipsis."	"most of the time, we don't know what the exact wording of the omission is, but we are aware that it must be a noun and represent a thing."	neutral
train_77765	"Various kernels, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel  and dependency tree kernel , have been proposed to solve the relation classification problem."	"rNN represents recursive neural networks for relation classification, as proposed by Socher et al. (2012)."	neutral
train_119037	Note that datasets with different vocabulary size can lead to different entropy: the entropy of picking a word from the vocabulary uniformly at random would have been different.	recall from Section 4 that our comments dataset contains roughly 237K repliers and 357K original com- menters.	neutral
train_192994	We do not have access to aligned source words for gold constraints.	"since there is no correspondence between constraints and the source words they cover, correct constraint placement is not guaranteed and the corresponding source words may be translated more than once."	neutral
train_108438	"The more often a bilingual phrase pair co-occurs, or the closer a bilingual phrase pair is within a snippet, the more likely they are translations of each other."	it is possible to discover these translation pairs through their surface strings.	neutral
train_6859	We then evaluate the sub-sequence filtering approach to reducing the actual error rate of these models by adding both 3 and 4-grams from the Gigaword Corpus to the baseline.	"a BF represents a set S = {x 1 , x 2 , ..., x n } with n elements drawn from a universe U of size N ."	neutral
train_153646	Ng (2010) presents a comprehensive review of recent approaches to within-document coreference resolution.	"candidate ranking then considers each candidate in greater detail, producing a ranked list."	neutral
train_155480	We experiment with the parallel Chinese-Spanish corpus (United Nations) to explore alternatives of SMT strategies which consist on using a pivot language.	"in this study we use the UN corpus taking advantage of the fact that (as far as we are concerned) it is the biggest parallel corpus freely-available in Chinese-Spanish and it contains the same sentences in six other languages, therefore we can experiment with different pivot languages."	neutral
train_169418	"Potential terms are n-grams, mainly unigrams, occurring in Swedish patent texts."	"n T Os are more equally Figure 1: Decision Tree computed for occurrences of the T C Aspect: each node is an XML tag and each edge exhibits the normalized distance between on occurrence and the closest tag of this type, for each decision (T O or n T O), the proportion of True Positives is given distributed within the document."	neutral
train_169481	"First, the number of additional atoms that need to be added to the network can be large depending on the domain size of the predicate's arguments."	the system configurations underlying the three rows in this table are the same as those in table 2.	neutral
train_30436	"Moreover, dropout is used for regularizing the model (Zaremba et al., 2015)."	"model output is shown for SEQ2SEQ (a, b) and SEQ2TREE (c, d)."	neutral
train_160439	"Annotations with more than 1 token were split into a sequence of tokens (e.g., BAB/BUP to BAB, BUP)."	the audio was manually transcribed and aligned with videos; the gestures were manually annotated and segmented according to video and audio recordings.	neutral
train_148990	"Following the experimental settings and evaluation metrics in Bai and Zhao (2018), we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009), which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015), which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score."	"these models typically use pre-trained semantic embeddings generated from language modeling tasks, like Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)."	neutral
train_129027	"RA-CNN consistently outperforms all of the baseline models, across all five datasets."	"when training new classification systems, it is natural to acquire supervision at both the document and sentence level, with the aim of inducing a better predictive model, potentially with less effort."	neutral
train_152881	"While translating from a language of moderate case marking and morphology (English) to one with relatively richer case marking and morphology (Hindi), we are faced with the problem of extracting information from the source language sentence, transferring the information onto the target side, and translating this information into the appropriate case markers and morphological affixes."	it is imperative to make it possible for the system to learn general rules for morphology and case marking.	neutral
train_52842	"The percentage of RET is even greater than with IF (114 [17.0%], almost twice the percentage of CON [9.4%]) as is that of Rough Shifts (100 [14.9%], almost three times the percentage of Smooth Shifts [5.2%])."	"rank CFs according to grammatical function, linear order, a combination of the two as in Gordon, Grosz, and Gillion (1993), or information status as in Strube and Hahn (1999)."	neutral
train_81274	We perform hashtag recommendation as follows.	not all microblog posts have hashtags created by their authors.	neutral
train_192623	"In total, we obtained nine affective values for 2.2 million words."	"We used the 15 verb classes from GermaNet (Hamp and Feldweg, 1997;Kunze, 2000)."	neutral
train_193668	"The sophistication of the linguistic structure of the questions in the FreebaseQA data set is compared to other similar data sets based on the average length, in number of words, of the questions."	machine learning approaches for NLP are data hungry since they require large amounts of real-world data to train the models for the best possible performance.	neutral
train_180709	"We note, however, that since these are not true phonemes (but rather phonemes copied over from pronunciation dictionaries and word transcripts), we must cautiously interpret these results."	"(1997) had previously used phoneme lattices, although with ad hoc edit costs and without efficient indexing."	neutral
train_40587	"Suggestively, the one task where NLI clearly outperforms NMT is WC."	"In constructing our probing benchmarks, we adopted the following criteria."	neutral
train_174189	This offers an optimal sensorial immersion of the user.	"our objective is not to learn a model (in a machine learning point of view) but to extract automatically and manually information to model the virtual patient's behavior (Porhet et al., 2017) that, then, will be validated through perceptive studies."	neutral
train_12321	We begin using word features and obtain a token-level F1score of .311 for text and .311 for numeric attributes.	"if the labeled subsequences are long and have substructure, e.g., 'San Remo, Italy', our system splits at the separator token, and creates additional seed sets from prefixes and postfixes."	neutral
train_28685	"If we use different composition functions for children with different tags like TG-RNN, the number of tag pairs will amount to as many as k×k, which makes the models infeasible due to too many parameters."	vectors of interior nodes are computed recursively by composition of child nodes' vectors.	neutral
train_106447	"Furthermore, the abbreviation matcher consists of 5 simple match routines and each routine is dedicated to a certain type of abbreviations."	some examples of definitions and their patterns are in Table 2.	neutral
train_49096	The simplicity of these methods allow them to apply to many NLP sequence tasks and various neural model architectures.	whether the word is the Right/Left/None-most dependent of its governor.	neutral
train_17747	We will therefore refer to our model as the variable mixture model (VMM).	"This can be viewed as a classification problem in which the target word Wi corresponds to the class that must be predicted, based on features extracted from the conditioning context, e.g. a word occurring in the context."	neutral
train_154389	"Of course, in real applications, it is impossible to know which kind of noises exist and where they are in advance."	"although hypernymy, hyponymy, and meronymy/holonymy are transitive, and can generate the indirect WordNet resources, the number of meronymy/holonymy is far below than those of the other two in the WordNet."	neutral
train_52846	"Rule 2 (BFP), formulated in terms of single transitions, accounts for larger percentages of the data (single utterances) and was found to be verified both with the vanilla instantiation and with the ""best"" instantiations."	"we also saw that in languages such as Italian, Japanese, and Turkish, the preferred realizations of CBs are morphologically null elements (Kameyama 1986;walker, Iida, Cote 1994;Turan 1998;Di Eugenio 1998)."	neutral
train_66755	"They determine w(f,c) from the distribution of f between c and c , attributing greater weights to those f that correlate with c or c most."	both TS and TSL 1 require computation of similarities between a large set of words and thus incur significant computational costs.	neutral
train_170622	"In this case, the uncertain semantics will be implicitly conveyed by the whole sentence rather than by the explicit cue phrases."	"we found that in Sina microblogs, there were a lot of sentences expressing the uncertainty statement with expected semantics."	neutral
train_37942	"As the final vocabulary V contains all individual characters in the corpus, character-based segmentation is also included in the set of segmentation candidates S(X)."	"while these sequences encode the same input ""Hello world"", NMT handles them as completely different inputs."	neutral
train_220118	"Compared with LDA with full context (FULL) in row 6, performance is slightly improved, perhaps due to the fact that longer contexts induce more accurate topics."	their assumption is that words with similar distributions have similar meanings.	neutral
train_88972	"The resulting algorithm shows clear promise not only for transcription, but also for ranking of transcriptions and structural classification."	the details of their experiment and model differ from ours in a number of respects.	neutral
train_107238	"Our results indicate that humans achieve an average accuracy of about 77.45% to 88.58%, where some domains seem to be easier than others."	the segment granularity is more fine-grained for paragraphs than for topics.	neutral
train_162862	"To make it more clear, we show the inference results for each symptom with and without graph in Figure 5."	"(Yaghoobzadeh and Schütze, 2016) used knowledge base and aggregated corpus-level contextual information to learn an entity's classes."	neutral
train_162967	It verifies that the multiplicative composition used in our approach is able to introduce new information to the base codebook.	"given the over-parameterization of deep neural nets, the effective compression of them has been receiving increasing attention from the research community."	neutral
train_163451	"Massive news articles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011)."	"in this paper, we propose a neural news recommendation approach with multi-head selfattention (NRMS)."	neutral
train_130972	The learning process of REHESSION is summarized as below.	"among four tasks, the relation classification of Wiki-KBP dataset has highest label quality, i.e. conflicting label ratio, but with least number of training instances."	neutral
train_150146	This paper reports a new way of extracting terms that is tuned for a very small corpus.	a corpus of each size was generated by singly adding an article randomly selected from the corpus of each genre.	neutral
train_141815	Layered sequence labeling model will first extract the inner entities (contained by other entities) and feed them into the next layer to extract outer entities.	these methods can be divided into three groups: Finkel and Manning (2009) and Ju et al.	neutral
train_191163	We then put a distribution over the candidate responses conditioned on the summarized dialog history h his (Equation 10).	"in Figure 5, the user already mentioned that he/she wants to find a ""cheap"" restaurant, but the GMN and QRN seem to ""forget"" this information."	neutral
train_218775	"Compared to traditional pair-wise temporal relation representations, temporal dependency trees facilitate efficient annotations, higher inter-annotator agreement, and efficient computations."	our training data consists of two parts.	neutral
train_70768	"In Experiment 4, using 1,350 frequent terms, we examine the effects of seed lexicon size and specificity and we apply a heuristic based on cognates."	"according to our results, translation spotting is more accurate when the seed lexicon contains (5,000) entries from both the medical domain and general language instead of general language words only, but only by a very small margin."	neutral
train_35764	"End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently Bahdanau et al., 2015)."	"we find that while the attention identifies the 10-th source word ""kuadaxiyang"" (cross-atlantic) to be most relevant, the relevance vector of the target word R y 9 finds that multiple source and target words should contribute to the generation of the next target word."	neutral
train_137874	"In this section, we describe our linearization traversal order, which generalizes the DFS traversal applied previously only for trees."	"in Section 4, we adapt the MTL strategy to train a single model for all SDP formalisms."	neutral
train_198955	"The results of the last experiment provided results for ""conceptual clusters""."	"when a cluster surfaces based on conceptual categories but not collocation statistics, inaccurate translations result."	neutral
train_193818	We find that our joint neural approach to SCL improves unsupervised domain adaptation substantially on a standard sentiment classification task.	"the key idea in SCL is that a subset of features, believed to be predictive across domains, are selected as pivot features."	neutral
train_11725	It measures the quality of a summary by counting the unit overlaps between the candidate summary and a set of reference summaries.	"the knowledge on the document side, i.e. the topics embedded in the documents, can help the context understanding and guide the sentence selection in the summarization procedure. "	neutral
train_154396	"Concept airplane has an atLocation relation with the concept airplane hangar, whereas plane has not such relation with airplane hangar."	we do not list the accuracies of combining other indirect resources with the direct ones).	neutral
train_218181	"In practise, the likelihood component is optimized using negative sampling with EM for the latent slots."	"while this step is not strictly required, we found that it leads to generally better results than random initialization given the relatively small size of our predicate-argument training corpus."	neutral
train_65381	"The third dataset ""all"" mixes all the sentences."	we resort to the following approximation method instead.	neutral
train_128796	"The MPQA 2.0 corpus has some labeled opinion expressions, but they are generic subjective expressions as opposed to aspect-sentiment phrases (see Section 1) we find in reviews."	"the thread of research in (Brody and Elhadad, 2010;titov and McDonald, 2008;Zhao et al., 2010;Mei et al., 2007;Jo and Oh, 2011) focus on extracting and grouping aspect and opinion words via generative models but lack the natural aspect opinion correspondence (e.g., in the manner they appear in sentences)."	neutral
train_29449	"Using a few different NLP tasks, we then empirically show that our method can indeed learn a better classifier for the target domain than a few baselines."	"domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain."	neutral
train_161544	"By choosing perceived offensiveness and discrepancies in perception as proxy measures for MAS, we hope to shift studies of MAS from a judgment call on an individual's intent to a description of how it affects its targets, which can be described as an objective measure of human behavior that can be validated through study."	"this moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had Îº val- ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported Î± values for some words below 0.30 at determining meaning."	neutral
train_136590	We create the POM for all annotated verbs using the same procedure as Del Tredici and Bel (2016).	"for the sake of a meaningful evaluation, we ensured that 25% of these metaphorically used tokens were novel metaphors."	neutral
train_61348	"For example, Toprak et al. (2010) reported a kappa of 0.56 for polar fact sentences in customer reviews, and Benamara et al. (2016) obtained 0.48 when annotating implicit opinions in French news reactions."	"despite substantial progress in the field, implicit topic/aspect identification as well as complex explicit aspects (which use, for example, verbal constructions) remain unsolved."	neutral
train_75249	"Since weighted Jaccard coefficient needs non-negative entries and we want the cosine similarity of two documents to range from 0 to 1, we translate the values of the z-score so that they are always non-negative."	"a web page W is initially represented as the sequence of tokens starting in uppercase, in the order as they appear in the web page."	neutral
train_151310	"Here, N is the number of words in the correct transcript, I is the number of incorrectly inserted words, D is the number of deletion errors, and S is the number of substitution errors."	"for these reasons, it is difficult to apply the method to a large-scale speech-based IR system."	neutral
train_88969	"Sequence alignment is a problem that crops up in many forms, both in computational linguistics (CL) and in other endeavors."	"let t be the lowest scoring rhyming word, and let t be the highest-scoring non-rhyming word."	neutral
train_143592	"By transferring latent dialogue knowledge from multiple sources of varying generality, we obtained a model with superior generalization to an underrepresented domain."	it can be seen that few-shot models with LAED representation are the best performing models for this objective.	neutral
train_71692	 Web search engine queries resulting in user clicks on a Wikipedia article are taken as alternative surface forms for the entry.	"attempting to address this point, we see that the F1 values of WikiRes on the MSNBC test set and on the Yahoo!"	neutral
train_143721	We conduct a series of experiments that demonstrate the effects of our framework in both the evaluation of recommendation and dialog generation.	our proposed method performs the best in all evaluations compared with the baselines.	neutral
train_195392	"Therefore, Goyal et al. (2017b) propose a continuous surrogate, J˜, by defining a continuous approximation (soft-k-argmax) of the discrete k-argmax and using this to compute an approximation to a composition of the loss function and the beam search function."	"recently, energy based neural structured prediction models (Amos et al., 2016;Belanger and McCallum, 2016;Belanger et al., 2017) were proposed that define an energy function over candidate structured output space and use gradient based optimization to form predictions making the overall optimization search aware."	neutral
train_29603	"Most Open IE systems employ syntactic information such as parse trees and part of speech (POS) tags, but ignore lexical information."	our modified kernel is based on the SubSet Tree (SST) Kernel proposed by Collins and Duffy (2002).	neutral
train_77448	"To test the first hypothesis, we investigate the role of parser accuracy in QE."	"in a similar vein, Gamon et al. (2005) use POS tag trigrams, CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or disfluent."	neutral
train_215633	"In order to apply knowledge distillation on a current training model, we need to obtain soft target probabilities as qk in Eq. (5) for all classes, but they are not available explicitly."	"cBOW predicts a word given its neighbor words, and Skip-gram predicts the neighbor words given a word."	neutral
train_183428	"A cross-language matching error resulted in the linking of ""White House"", and the reduced granularity of the contexts precluded further disambiguation."	the evaluation is dominated by the penalty for spurious system coreference chains.	neutral
train_163975	The entire Odin framework is available as part of the Processors NLP library.	"from the languages that support syntax, Stanford's Tregex matches patterns over constituency trees (Levy and Andrew, 2006)."	neutral
train_85736	"The script is used to write other languages like Amharic, Tigrigna, Argoba, etc."	there are some prepositions written as a separate word.	neutral
train_92608	This paper explores document-level SMT from the tense perspective.	"the vertical axis indicates different tense: 1 to ""past"", 2 to ""present"", 3 to ""future"" and 4 to ""UNK""."	neutral
train_22565	"In addition, our model accounts for a larger portion of the corpus with fewer rules: the top 50, 100, and 200 most common elementary trees in our model's lexicon account for a greater portion of the corpus than the corresponding sets in the TSG."	representing the argument/modifier distinction can help the learner find useful argument structures which generalize robustly.	neutral
train_46121	"On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015;Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a;Currey et al., 2017)."	"to this end, we compare DALI and Bt on adapting from subtitles to medical domains, where the two largest corpus in subtitles and medical domains have 13.9 and 1.3 million sentences."	neutral
train_215137	"They utilise the LCD algorithm that tests variables for dependence, independence, and conditional independence to restrict the possible causal relations (Cooper, 1997)."	"we search for causal relations between events without considering the object, between events given the object, and between events and states."	neutral
train_130005	This demonstrates the robustness of the character-level model to unseen entities.	we find that a two-layer LSTM boosts joint accuracy by over 6%.	neutral
train_65382	We construct a belief network that represents the constraints existing among R's and E's.	we derive weak classifiers by treating these two learning tasks as shallow text processing problems.	neutral
train_55862	"The 25 unique beginners have been shaded, and the two graphics show how the hierarchy evolved between the two WordNet versions used in this study."	the last row gives the total score of the best systems on the three tasks.	neutral
train_134090	"We noticed that Uzbek and Uyghur are very similar, sharing a sizable amount of vocabulary, and several morphological rules."	"accordingly, the drawback to this approach is the difficulty of finding any parallel data, let alone millions of sentences."	neutral
train_11447	The Bengali blog data have been collected from the web blog archive (www.amarblog.com) containing 1300 sentences on 14 different topics and their corresponding user comments have been retrieved.	it signifies that the best possible reference range for valence and other emotion tags have been selected.	neutral
train_52844	"Our second major finding is that parameters do matter: that is, it is possible to set the parameters in such a way as to make all three claims verified in a statistical sense."	"we also found a trade-off between Rule 1, on the one hand, and Constraint 1 and Rule 2, on the other: Setting the parameters to minimize the violations of local coherence leads to increased violations of salience, and vice versa."	neutral
train_180710	"For example, if we have never seen the word houses in language model training, but have examples of house, we still can expect houses are to be more probable than houses fly."	a lattice is a directed acyclic graph that is used to compactly represent the search space for a speech recognition system.	neutral
train_193820	"Our results also show that while existing pivot selection methods perform well, they are below an oracle-provided ceiling for many source-target pairs for the sentiment classification task we examine."	"a neural version of SCL still obtains near state-of-the-art performance (Ziser and Reichart, 2017)."	neutral
train_85999	"Motivated by the fact that later corrections are not as effective, the authors argued that the first post seen by a user is influential for their future opinion and thus it is important to show users rumours only once they are confirmed to be true."	note that this occurs in only two and one cases for baselines B1 and B2 respectively.	neutral
train_101132	"For instance, given the question “What are invertebrates?”, the sentence “At least 99% of all animal species are invertebrates, comprising . . . ” was labeled “-1” , while “Invertebrates are animals without backbones.” was labeled “+1”."	"and (d) PAS adds further information as the best model is PSK+STK+SRK, which improves BOW from 24.2 to 39.1, i.e. 61%."	neutral
train_55545	"Because the lexical entailment relation is directional, each candidate pair was duplicated to create two directional pairs, yielding a test set of 3,200 pairs."	table 9 Bootstrapped weighting: top 10 common features for country-state and country-party along with their corresponding ranks in the two (sorted) feature vectors.	neutral
train_172417	"Everyday language is built up of prefabricated parts and templates that form a speaker's individual discourse experience (Hopper, 1998;MacWhinney, 2001;Bybee and McClelland, 2005)."	"the Constructicon lists constructions, such as the Way_manner construction (e.g., She whistled her way down the lane), and lists the roles associated with the construction and the Construction Evoking Element (CEE) (e.g., one's way in this context)."	neutral
train_123175	"Although they make use of both WordNet glosses and relations, and gloss information is utilized for a classifier, this classifier is generated only for weighting edges between sense nodes and classification nodes, not for classifying all senses."	"it has a positive effect on the object, a crime, since performing a crime brings it into existence."	neutral
train_13564	The implementation was the same as in Sim. 1.	"we present a new rational model of eye movement control in reading, the central assumption of which is that eye movement decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence."	neutral
train_5489	"It can be seen that in topic cluster-1, which is about expense reimbursement and related stuff, most of the queries can be answered quickly in standard ways."	we extract all n-grams which occur more frequently than a threshold and do not contain any stopword.	neutral
train_5490	Based on the topic level information we can check whether the agent identified the issues and offered the known solutions on a given topic.	the documents contain well formed sentences which allow for parsers to be used.	neutral
train_115895	"In standard Dutch, the noun 'slachtoffer' goes with the 'het' definite article which is marked in its lexical entry."	"it should be noted that since our approach cannot differentiate between incomplete and incorrect entries, no entry in the lexicon is modified."	neutral
train_188340	"At the same time, we want to give special thanks to the anonymous reviewers for their insightful comments as well as suggestions."	all those works for document representation paid little attention to the variability of intra-topic documents.	neutral
train_42287	"We split each prediction to Table 2: BLEU scores, simple sentences per complex sentence (#S/C) and tokens per simple sentence (#T/S), as computed over the test set.
"	it is not required to generalize to unseen relations.	neutral
train_131848	"Kushman et al. (2014) aligns nouns to variable slots {x1 , x2 , ...} which leads to a huge hypothesis space and does not perform as well as the number slot alignment only method proposed later by (Zhou et al., 2015)."	first we parse the equation to a hierarchical tree.	neutral
train_168177	"Wikipedia was used because it provides a large and wide-coverage source of text, completely independent from the datasets used and from WordNet."	"for each word in the corpus that is in one or more WordNet synsets, if there is a previously trained model, probabilities P (w|s) are predicted from the distribution of LDA."	neutral
train_86004	"Next, we will investigate further the integration of temporal information for the rumour detection task."	this restricts the idea of stance-based rumour verification to only data where human stance labels are available.	neutral
train_4040	"In these cases, some of the reference translations also use ""impact""."	"the third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification (Carreras et al., 2002) ."	neutral
train_93768	We use recall to measure the number of top-K sorted word pairs that are found in both the rankings.	we focus on solving this central problem in the context of NLP applications.	neutral
train_174391	"We also make the most of the richness of the encoding, while implementing some graphical choices to make their interpretation as clear as possible for end users."	it is assumed that from the point of view of users a graph is a more suitable tool to visualize and navigate through the terminological structure of a domain especially since some of its features can be exploited by designers to highlight different properties of relations and the nodes that these relations link.	neutral
train_153309	"Finally, since the dependency type between ""PROT1"" and ""Association"" is prep_between, the preposition word ""between"" and its constituent ancestors are added into the SCP as rendered by the dotted lines."	this paper proposes a principled way to automatically generate constituent structure representation for tree kernel-based protein-protein interaction (PPI) extraction.	neutral
train_26608	This is a classical K-Nearest-Neighbor classifier baseline.	"to select the classification threshold for ith relation type, we use the value which can achieve the best F-measure on training dataset (with an additional restriction that precision should > 10%)."	neutral
train_48008	"Dataset TriviaQA (Joshi et al., 2017) is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web."	"this is a seemingly ill-posed problem since for most of the time, the scheduling should be performed without knowing which question will arrive next."	neutral
train_203818	"The process of building the corpus includes query generation, document sampling via google API and document classifier."	this study implemented an automated system that builds a language specific lexicon by using an open corpus or the World Wide Web.	neutral
train_120354	This gives us per ranking as many sets as there are letters that change.	"note that in EGY, every verb manifests a stem change between the perfective and imperfective forms, while in GER many verbs in fact do have the same stem for all inflected forms."	neutral
train_129666	"While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes."	"we also experiment with constraining the BSO model during training, as described in Section 4.2, by defining the succ function to only allow successor sequences containing un-used words in the source sentence."	neutral
train_62320	The category at the root node of this fragment is E(c).	"the rule is also thought to be specialized for that token, meaning that the rule contributes to the derivation by introducing some structure representing the syntactic frame and valencies of the token itself."	neutral
train_192709	We find that MERGE is mostly responsible for combining spans of words to form a named entity in English parsing.	evaluation on negation detection.	neutral
train_47606	"To capture the semantic shifts across time domains, our intuition is to model the dynamic process."	the first T - 1 time domains are used for training; the last time domain is split into two equal-sized sets for development and testing.	neutral
train_153258	"We note that, in the case of the former experiments, the size of the unlabeled examples is increasing in the direction 98a to 91a."	"if we use unlabeled data within the epoch of the test set, we hardly see a degradation trend as the time gap between the epochs of seeds and test set is increased."	neutral
train_118148	"The tag set is simply {B, I, O} which are signs of begin, inside, outside of a vertex."	"through data analysis, we observe that 60.5% of sentences in our corpus do not follow the assumption used by them."	neutral
train_123168	"So as not to limit the inferences that may be drawn, our annotations include events that are +effect or -effect either the agent or object."	"the other set is used as a seed set by the graph model, and as a training set by the gloss classifer."	neutral
train_73916	"Contrarily to Banerjee, however, we do not consider the sum of squared sub-string overlaps, but merely a bag-of-words overlap that allows us to generate a dictionary from WordNet, where each word contained in any of the word sense definitions is indexed by a unique integer and where each resulting definition is sorted."	"when an ant arrives on a nest of another term (that corresponds to a sense thereof), it can either continue its exploration or depending on the score between this nest and its mother nest, decide to build a bridge between them and to follow it home."	neutral
train_105351	The objective is to optimise ROUGE-1 by maximising the number of word tokens paired up between system and reference summaries.	"2 We use both the original 665 bytes summary budget as well as the 100 word summary budget used by (Hong et al., 2014)."	neutral
train_885	"It is difficult to compare our result to other results, since most Shallow-parsers pursue different tasks, and use different evaluation metrics."	the program takes as input a file containing rules.	neutral
train_115482	Words that have non-existent counterparts can be aligned to the NULL word.	"first, we deal with different languages."	neutral
train_170365	"Since there was no appropriate data available to create an automatic PICO approach or sentiment classifier, we created it ourselves with the help of expert and nonexpert annotators."	"an Intervention that did not perform better than its Comparison, should be classified as negative."	neutral
train_115348	"Common practice (Koehn et al., 2003) takes a set of word alignment links L and defines the alignment constraints C A so that there is a consistency between the links in the  (f j2 j1 ,ei2 i1 ) phrase pair. "	"bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (blunsom et al., 2008;blunsom et al., 2009)."	neutral
train_12968	"For example, considering Example (5), Figure 2 illustrates the representation of the structured feature for this relation instance."	and thus an SVM classifier can be learned and then used for recognition.	neutral
train_27579	In most situations the date is only 1-2 years apart.	"our solution does not depend on a specific implementation, only that we are able to accurately identify abbreviation expansions."	neutral
train_140984	"In the discussion of their results, Wu et al. (2016) note that their raters ""[did] not necessarily fully understand each randomly sampled sentence sufficiently"" because it was provided with no context."	"when evaluating entire documents, raters show a statistically significant preference for HUMAN (x= 104, n= 178, p<.05)."	neutral
train_110484	"In Section 3, we describe the self-trained sentence classifier, which requires only a few seed patterns and relevant and irrelevant documents for training."	sometimes a general semantic class can be partitioned into subclasses that are associated with different roles.	neutral
train_120742	"Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965;Lederer and Hill, 2001;Kim et al., 2013)."	these studies all suggest that brand and its associations play important roles in the customers' perceptions and decisions.	neutral
train_206376	"In an attempt to answer this question, this paper examines the three approaches to the causative alternation by utilizing the possibility of having idiomatic interpretations as a probe into the syntactic structure."	"it predicts that, if an idiomatic interpretation is possible with a causative, then it should be possible with its inchoative counterpart, as shown in Table 2 below."	neutral
train_156259	"The overall best performing features were WD, QD, MP and OIT, which is in line with the findings in the correlation study in Section 5.2."	many interactions happen outside the context of a pre-defined static power structure or hierarchy.	neutral
train_201736	"The finding that the SEP is not distinguished from the SEB regarding TTR or D, indicates that prime ministers refer to a similar diversity of political content and in a similarly balanced way in the two types of addresses."	"the results indicate that the SEP has a significantly larger N than the SEB, and the two types of addresses are not distinguished by D."	neutral
train_75248	"Capitalized n-grams usually are Named Entities (organizations and company names, locations or other person names related with the individual) or information not detected by some NE recognizers as for example, the title of books, films, TV shows, and so on."	"the combination of z-score and Jaccard gets the best BR results, but the BP results are the lowest."	neutral
train_26862	"For example, identifying the gender of a person is important for generating a good description."	table 3 shows the results of the model transfer experiment.	neutral
train_135180	"The ""common space"" variant performs the worst by a large margin, confirming our hypothesis that discrepancy between the two embedding spaces harms the model's ability to generalize."	"in this paper, we closely follow the architecture proposed by Lample et al."	neutral
train_53741	(3) The cloud was made of dust.	"the ISS system can disambiguate any pair of concepts, provided they are in WordNet or can be classified by NERD."	neutral
train_74198	"In order to avoid noise in the data, we extracted only those instances where the IC verb was the main verb of an Arg1 which contained only a single sentence."	"the most relevant experiment specific to the tendency towards causal inference is again the one by Murray (1997) in which subjects were asked to continue individual sentences that ended with either a period or a connective of the aforementioned types (additive, causal or adversative)."	neutral
train_117730	"Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e.WordNet, or a machine readable thesaurus)."	"dinu and Lapata's model, which performs similarity calculations in the latent space, is not able to provide accurate word vectors, and thus perform worse at the paraphrase induction task."	neutral
train_120611	"2) In some passive sentences, there are two surface ni-cases as in Example (11)."	"since the annotation quality was not very high, we manually checked all tags and modified inappropriate ones."	neutral
train_206371	Section 4 briefly discusses ditransitive causatives and their related forms.	"since the pattern observed in these examples cannot be predicted, as shown in Table 1, it is clear that the causativization approach should be excluded even in cases where it might appear to be appropriate."	neutral
train_217592	"It combines layout, discourse and terminological analyses to bridge the gap between the document layout and lexical resources."	"The vector-based strategies present interesting precisions, which seems to confirm a correlation between the lexical cohesion of terms and their likelihood of being involved in a relation."	neutral
train_21637	"These include making sure that both the root of the clause and its subject appear in the extracted pattern, and that conjunctions between entities should not be dropped (Figure 1, 3)."	"in order to advance towards this goal, this paper explores the following questions:  What is a good way of using syntactic patterns to represent events for generating headlines?"	neutral
train_107561	One problem with evaluating a spell checker designed to correct search queries is that evaluation data is hard to get.	we present an approach that uses an iterative transformation of the input query strings into other strings that correspond to more and more likely queries according to statistics extracted from internet search query logs.	neutral
train_27280	OntoNotes provides annotation of constituency trees only.	our analyses indicate that it may be possible to achieve small but significant increases in accuracy of dependency parsing through ensemble methods.	neutral
train_152050	There are few studies on data selection for translation model training.	the range of improvement is not stable because the MERT algorithm uses random numbers while searching for the optimum weights.	neutral
train_206248	"The resulting structure can be used in two contexts: on the one hand, they can be used as prepositional complements in locative adjuncts, as illustrated in (3b)."	"Valence Principle: in a headed phrase, for each valence feature F, the F value of the mother is determined as follows: -If the valence list of F consists of synsem objects, its value corresponds to the head daughter's F value minus the SYNSEM values of its sisters."	neutral
train_45189	Artificial Titles enable the decoder to be trained on these tokens.	"the mixed performance of training on t art indicates the artificial title quality is lower for StackEx, (d) vs. (b)."	neutral
train_110485	"For example, one role in the terrorism domain is physical target, which refers to physical objects that are the target of an attack."	this results in an asymmetry in the training set.	neutral
train_74555	"Finally, we end with the conclusion and perspectives."	"for example, in the matching result M3 of fIGURE 4(b), [Pred] and [A1] are separated by a gap word ""å‡ç¨Ž""."	neutral
train_44455	"As the core of Chinese IME, P2C conversion has been formulized into a seq2seq model as machine translation between pinyin and character sequences, there are still a few differences between P2C converting and standard machine translation."	"in practice, we need to control the size of the vocabulary for acceptable decoding speed."	neutral
train_46118	We also show the percentage of source words or subwords in the training corpus of five domains being covered by the WMT14 corpus.	"to remedy this problem, we propose an unsupervised adaptation method which finetunes a pre-trained out-of-domain NMt model using a pseudo-in-domain corpus."	neutral
train_115140	This section focuses on an orthogonal direction: improving literal strategies with learning.	"we train the speaker, S:LEARNED, (similarly, listener, L:LEARNED) on training examples which comprise the utterances produced by the human annotators (see Section 6.1 for details on how this data was collected)."	neutral
train_155016	"Generally speaking, the articles in news domain usually contain descriptions of date and events accompanying the publication sequences."	The authors would like to thank the anonymous reviewers for their comments on this paper.	neutral
train_46123	"Instead of naively mixing out-of-domain and in-domain data, Britz et al. (2017) circumvent the domain shift problem by jointly learning domain discrimination and the translation."	"we scan the source and reference of the test set to count the number of valid hits C, then scan the output file to get the count C t in the same way."	neutral
train_202558	"the K means, Expectation-Maximization, spectral clustering, Information Bottleneck, Probabilistic Latent Semantic Analysis, cost-based pairwise clustering (Brew and Schulte im Walde, 2002;Schulte im Walde, 2006;Korhonen et al., 2008;Sun and Korhonen, 2009;Vlachos et al., 2009)."	"after providing a short introduction to the basic principles of manual verb classification, this paper reviews recent research in automatic classification -particularly focussing on work conducted in English -and discusses then the various current challenges that need to be met for substantial further advances."	neutral
train_83491	"For acoustic content, DCRNN outperforms MFCC and convnet because it can learn acoustic information both locally and globally."	"t att is an attention vector generated from T small with an average pooling layer, and works on every word vector of T large through the element-wise product."	neutral
train_215093	"In common sense, syntactic structure may keep a crucial part for human being to understand the meaning of a given text."	it also may help to identify the semantic equivalence between two given texts.	reasoning
train_168450	"In addition, a better understanding of such procedures has also been recognised as a bottleneck for machine translation (Helmreich et al., 2004)."	"modelling how humans translate may result not only in a theory of translation better capable of describing, explaining, and predicting translational phenomena but also in a better understanding of what can be done to improve machine translation."	reasoning
train_62301	"It is not hard to see that, as long as we work under the restriction to polynomial space, every move to the left in the standard model can be simulated by a (polynomial) number of moves to the right in the circular tape model."	even ATMs with polynomially bounded circular tape precisely characterize EXPTIME.	reasoning
train_116166	However search queries consist of keywords -relatively discrete and regardless of order.	"except some proper nouns such as person's name, a lot of bigrams in the query log are formed in an adhoc way."	reasoning
train_145232	"Yet, the good squared and good annotations identified by our method show comparable quality to our previous in-house annotation results which were performed with the required expertise and with the training for linguistic annotation (Yang et al., 2019)."	"we believe that our results show a great potential for the crowdsourced annotation, especially when it is followed by a working quality control method."	reasoning
train_154377	"Firstly, Threshold filter cannot effectively remove the second kind of noise terms, because it may filter out too much right terms from the WSP of a wrong sense, while remain terms belonging to the second kind of noise."	there are not enough right terms to relieve the second kind of noise terms for the wrong sense.	reasoning
train_154372	"However, it is inevitable that there are some noisy terms in WSPs, which will dramatically decrease the performance of disambiguating ConceptNet."	we need to pay efforts to reduce such noises.	reasoning
train_137868	"Intuitively, since different graph traversals affect the sequence of encoded nodes during training, the network will inevitably have to learn different weights and attention when presented with different orderings."	"some traversal orderings may be easier to learn than others, leading to better (hopefully more semantic) abstractions."	reasoning
train_118138	"From individual opinion representation, each subgraph of G which takes an opinion expression as root is connected and acyclic."	"the connectedness is guaranteed for opinion expressions are connected in opinion thread; the acyclic is guaranteed by the fact that if a modifier is shared by different opinion expressions, the inedges from them always keep (directed) acyclic."	reasoning
train_12901	"However, word alignment algorithms have suboptimal accuracy, so the results would have to be checked manually."	"we abandoned this idea, and instead we simply placed the different TimeML markup in the correct positions manually."	reasoning
train_44985	We also want to know the effect of the denoising data augmentation.	"we remove the denoising data augmentation from our model, and compare with the full model."	reasoning
train_214338	The taggers use different tagsets.	we will map these tagsets to a unified tagset consisting of main POS tags.	reasoning
train_109060	"In comparative evaluations, we ultimately want to determine if one technique is ""better"" than another."	the system rankings produced by a particular scoring method are often more important than the actual scores themselves.	reasoning
train_57995	"The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively."	we see that our second hypothesis-that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena-is borne out.	reasoning
train_215979	Having data skewness can have a direct impact on the results of a machine learning model and gaining further insights can contribute to obtaining higher results.	the impact of skewness on our dataset had been researched.	reasoning
train_112041	"A dialog in this domain usually has a single goal, to create an air-travel itinerary which may include hotel and car reservations."	the entire dialog corresponds to one task.	reasoning
train_154380	"Although hypernymy, hyponymy, and meronymy/holonymy are transitive, and can generate the indirect WordNet resources, the number of meronymy/holonymy is far below than those of the other two in the WordNet."	we ignore the indirect WordNet resources derived from meronymy/holonymy.	reasoning
train_131907	The output of our seq2seq model is a single equation containing one unknown variable.	our approach is only applicable to the problems whose solution involves one linear equation of one unknown variable.	reasoning
train_89090	"The automatic classification results were evaluated against human judgement, and the performance was encouraging, with accuracy reaching over 85% in some cases."	"while human judgement is not straightforward and it is difficult to create a PanChinese lexicon manually, it is observed that combining simple clustering methods with the appropriate data sources appears to be a promising approach toward its automatic construction."	reasoning
train_53723	"Our classification rule learning approach is based on the WordNet semantic classes of the two concepts that represent the part and the whole, respectively."	"if the WSD system fails to annotate the concepts with the correct senses, the ISS system can generate wrong semantic classes, which leads to wrong conclusions."	reasoning
train_38432	"Moreover, the recent research by Moosavi and Strube (2017) found that the extensive use of lexical and surface features biases entity coreference resolvers towards seen mentions and do not generalize to unseen domains, and the finding can perfectly apply to event coreference resolution."	"we propose to improve event coreference resolution by modeling correlations between event corefer-ences and the overall topic structures of a document, which is more likely to yield robust and generalizable event coreference resolvers."	reasoning
train_121992	"Similar to the previous work (Xing et al., 2013), we find that adding more prepositions will not improve the performance in our experiments."	"we only consider a fixed set of prepositions: {in, for, to, of, on}."	reasoning
train_164993	"Like Thai, Japanese and Chinese text, Vietnamese text is also a text without any explicit separator between words."	"identifying the word boundaries is a challenging basic issue to above oriental languages for natural language processing (Doan, 2008)."	reasoning
train_149144	"However, 75.0% of the aggregate history has pronouns and the percentage of mixed category pronouns increase to 27.8% of our data."	pronoun disambiguation potentially becomes a problem for a quarter of the original data.	reasoning
train_152288	"Given a subset of features, Maxent classified 74.5% of the documents correctly compared to 82% for AdaBoost."	adaboost was chosen as the classifier for further experiments.	reasoning
train_126002	The seg rev process is applied recursively but not deeply (only twice in our experiments) for each source sentence in the training data.	the seg rev process is lightweight and shallow.	reasoning
train_150757	"Since the CoNLL dataset is aimed at holistic SRL across all argument types, it incorporates a much larger set of verbs and tagging scenarios; as a result, the semantic role labelling of PPs is far more heterogeneous and realistic than is the case in the treebank."	we conclude that the results of our treebank preposition SRD system are not very meaningful in terms of predicting the success of the method at identifying and semantically labelling PPs in open text.	reasoning
train_115890	"As we saw, even if the grammar is able to produce a full-span analysis for a given sentence, this analysis could still not be the correct one."	it is possible that a word could have a problematic lexical entry even if it only occurs in sentences which are assigned a full-span parse.	reasoning
train_199304	These constraints form interaction among constraints expressed in the lexical entries.	"there is no need for assuming tentative labels A, B, C, and D of subordinate clauses."	reasoning
train_31954	All experiments still must have Prune Grammar and Search Bindings turned on in order to terminate.	we believe these optimizations would result in a constant factor speedup.	reasoning
train_176601	"Furthermore, it is debatable whether there exists a limited and fixed set of so-called primitives."	"we adopt 2,233 primitives from HowNet and extend 2698 basic concepts which make a deeper hierarchical structure and more precise semantic branching."	reasoning
train_32105	We use a random walk algorithm to sample paths such that the expected times a path is sampled equals its weight.	"the sampled path lengths range from 1 to 19, average 2.1, with an exponential tail."	reasoning
train_123505	"Following the previous work of Pitler and Nenkova (2008), we also assume that each readability factor affects readability independently. "	readability is calculated as a weighted linear sum of all readability factors.	reasoning
train_112353	One perspective is that our semiring fundamentally finds expectations.	we must be finding âˆ‡Z by formulating it as a certain expectation r.	reasoning
train_27586	"Since the output of the hidden units is a re-encoding of the information in the visible layer, we obtain a deeper representation and a more precise expression of information in the input reviews."	this approach enables the model to learn multi-faceted information with a simple yet expressive structure.	reasoning
train_218169	"However, building these frame lexicons is very expensive and time consuming."	it remains difficult to port applications from resource-rich languages or domains to data impoverished ones.	reasoning
train_114009	Its size is exponential in the length of the sentence and the number of hypotheses involved in combination.	pruning techniques are necessary to reduce the search space.	reasoning
train_210390	"Furthermore, distinct from prior studies, all of the subjects in the current project were L3 learners who had learned English prior to Cantonese at the time of task."	"the phonological similarity among Cantonese, English and Urdu typologies could also explain the accent results."	reasoning
train_50231	"Evaluation metrics such as topic coherence, perplexity, and log-likelihood measure how well topics model data, but are not sufficient to measure whether user feedback is incorporated as expected."	we propose new control metrics to measure how well models reflect users' refinement intentions.	reasoning
train_120590	"For example, the instances of the ga-case of the case frame "" -5 (be pushed down-5)"" and the wo-case of the case frame "" -4 (push down-4),"" which are considered to be aligned and represent patient, are similar."	we exploit semantic similarity sim SEM between the instances of the corresponding cases.	reasoning
train_164996	"Under the supervised condition, the labeled training corpus, with the same distribution to the unlabeled testing corpus, can help to predict dictionary performance."	we propose a square overlap ratio (SOR) measure to predict the performance of dictionary.	reasoning
train_2455	"Because the important unit of the work in the applications of text chunking is a phrase, F-score is far more important than accuracy."	we have much room to improve in F-score.	reasoning
train_35755	"As a result, the lack of interpretability makes it very difficult to understand translation process and debug NMT systems."	it is important to develop new methods for visualizing and understanding NMT.	reasoning
train_74542	We believe that they complement each other because they perform translation from different angles.	"to capture this complementation, for the PAS span in the decoding hypergraph, we can use both our PAS-based translation method and the traditional translation method."	reasoning
train_145229	"For the rerequested annotations, we dismissed the first annotations and took only the re-annotated results for our experiment."	"counting only the approved (and not rejected) annotations, 218 workers (with the lifetime approval rate > 95%) participated in our annotation, over about two weeks including the time for the re-request."	reasoning
train_85970	The general assumption is that true and false rumours would have different patterns in the stance distribution over time.	"after training models for true and false rumours, we can build a binary veracity classifier by comparing sequence occurrence probabilities for the two cases."	reasoning
train_131904	"That is, most equations and structured answers extracted by our method are correct, and many problems are dropped from the dataset."	"we get dataset Math23k which contains 23,161 problems labeled with structured equations and answers."	reasoning
train_204937	"Also, since there were no translations for the n-grams such as "" "" [(1) chiu masao, (2) chiu paul, (3) paul chiu] and "" "" [(1) zaiseibuchou, (2)(3) finance minister], the words in the ""nonmatching list"" could not be moved to the ""matching list""."	the parallel translation expression was judged as incorrect and removed from the final extraction results.	reasoning
train_128583	"Bowman et al. (2015), on the contrary, improve a natural language inference task from an accuracy of 71.3% to 80.8% by initializing parameters with an additional dataset of 550,000 samples."	more systematic studies are needed to shed light on transferring neural networks in the field of NLP.	reasoning
train_61327	"Instances that are not inscribed are considered to be invoked (also sometimes called evoked), in which ""an evaluative response is projected by reference to events or states which are conventionally prized"" (Hunston and Thompson 2000b, page 142)."	a bright kid or a vicious kid are inscribed.	reasoning
train_53730	"If a certain class of negative or positive examples is not seen in the training data (and therefore it is not captured by the classification rules), the system cannot classify its instances."	"the larger and more diverse the training data, the better the classification rules."	reasoning
train_170612	"As a result, the quality of information in social media has become a primary concern for many social media oriented tasks, such as rumor detection (Qazvinian et al., 2011) and credibility evaluations (Jaworski et al., 2014)."	uncertainty identification is significant for users to synthesize information to derive reliable interpretation.	reasoning
train_28798	"Similarly, some prominent tokens (e.g., ""Olympics"") are irrelevant and have no predicates anchored to them."	"instead of anchoring each predicate in the logical form to tokens in the utterance via lexical rules, we propose parsing more freely."	reasoning
train_883	"Contrary to the French Treebank, the Penn Treebank contains non-surfastic constructions such as empty nodes, and constituents that are not triggered by a lexical items."	"before evaluating our new shallow-parser, we automatically removed from the test sentences all opening brackets that were not immediately followed by a lexical item, with their corresponding closing brackets, as well as all the constituents which contained an empty element."	reasoning
train_166127	" As the length of both alignments is the same, only substitutions and matches are possible, and there are no deletions or insertions. "	the Levenshtein distance is equal to the number of mismatches (substitutions) between these two sequences.	reasoning
train_2748	"In particular, to our knowledge, there have been no attempts to (1) globally optimize an anaphoricity determination procedure for coreference performance and (2) incorporate anaphoricity into coreference systems as a feature."	"as part of our investigation, we propose a new corpus-based method for achieving global optimization and experiment with representing anaphoricity as a feature in the coreference system."	reasoning
train_123153	Mappings between FrameNet and WordNet are not perfect.	we opted to manually annotate the senses of the words in the word-level lexicon.	reasoning
train_105498	We propose a simple method that extracts audio samples from movies using textual sentiment analysis.	"it is possible to automatically construct a larger dataset of audio samples with positive, negative emotional and neutral speech."	reasoning
train_120580	"Specifically, since a normalized-case structure represents the same meaning in the same representation, normalized-case analysis is useful for recognizing textual entailment and information retrieval."	we need a system that first analyzes surface cases and then alternates the surface cases with normalized-cases.	reasoning
train_215980	Table 4 demonstrates that skewness in the dataset affects the performance of the classifier.	we explored several approaches for optimizing the results while working with skewed dataset.	reasoning
train_202910	"This is because under PL, patterns, lexical or superlexical, are just indices for instances, and they do not (need to) have meanings of their own."	"it can be argued that they appear to have meanings of their own just because they serve as ""keys"" to full instances that bear meanings."	reasoning
train_52800	"Competing versions of the central definitions and claims of the theory have also been proposed: For example, different definitions of backward-looking center (CB) can be found in Grosz, Joshi, and Weinstein (1983, 1995) and Gordon, Grosz, and Gillion (1993)."	"a researcher wishing to test the predictions of centering, or to use it for practical applications, is confronted with a large number of possible instantiations of the theory."	reasoning
train_195026	The Genia event extraction shared task provides the annotations of all entity mentions.	"for each trigger, we use all the entity mentions that occur in the same sentence as its candidate arguments, and then assign an argument role or None."	reasoning
train_42009	"At each time step, we maintain the top b parsing states, pruning off the rest."	a candidate parse that made it to the end of decoding had to survive within the top b at every step.	reasoning
train_111818	"Moreover, a SuperARV language model was presented (Wang and Harper, 2002), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model."	"these knowledge was integrated in the representation level, and then the joint probabilities of words and corresponding SuperARVs were estimated."	reasoning
train_202906	"The rank of patterns is defined as follows: In a PL constructed for an array with k-segments, the number of constants in a pattern corresponds to its rank."	the null pattern at the top is always at rank 0.	reasoning
train_36014	"By utilizing the transductive learning framework, we boost the F-measure by 1.7% and 2.1%, respectively."	our method is effective to predict hypernyms of Chinese entities.	reasoning
train_129011	"Providing such rationales during manual classification is a natural interaction for annotators, and requires little additional effort (Settles, 2011; McDonnell et al., 2016)."	"when training new classification systems, it is natural to acquire supervision at both the document and sentence level, with the aim of inducing a better predictive model, potentially with less effort."	reasoning
train_71712	"When we apply phrase substitution on a non-terminal node, then any simplification operation (including dropping, reordering and substitution) cannot happen on its descendants any more because when a node has been replaced then its descendants are no longer existing."	for each non-terminal node we must decide whether a substitution should take place at this node or at its descendants.	reasoning
train_128790	"We optimize the collocations of relevant aspect words and phrases in the GPU framework in two ways: Word to phrase: Intuitively, if an aspect word is assigned to a topic then that topic should represent that aspect and to all other phrases in that aspect's phrase set (i.e., phrases containing that aspect) should belong to the same topic."	when an aspect word is assigned to a topic then each phrase in its aspect set is promoted with a small count in that topic.	reasoning
train_133447	"That is, these annotations cannot be generated without background knowledge or added context."	in this preliminary work we focus on predicting CI lyric annotations.	reasoning
train_143585	"Although in our few-shot approach we use full in-domain dialogues, we end up having significantly less in-domain training data, with the crucial difference that none of those has to be annotated for our approach."	the method we introduced attains state-of-the-art in both accuracy and data-efficiency.	reasoning
train_124782	"As introduced in Section 3.1, a span-based target annotation of an opinion in MPQA 2.0 captures the most important target this opinion is expressed toward."	the head of the target span can be considered to be the most important eTarget of an opinion.	reasoning
train_168929	The theory of priming applies to any perceptual entities regardless of modality.	"the use of lexical and visual features as priming cues is cognitively valid (Stenberg et al., 1995)."	reasoning
train_153393	"The clues exploited in this paper are based on the fact that the direction of eye gaze directly reflects the focus of attention (Richardson et al.,
2007; Just and Carpenter, 1976) , i.e. when one utters a referring expression, he potentially focuses on the object involved by fixating his eyes on it."	we use the eye fixations as clues for identifying the pieces focused on using the following criteria: the nearest piece to the eye fixation point is more likely a target of focus over all other pieces.	reasoning
train_171142	The alignment tools we used do not guarantee full alignment coverage for each word in a sentence.	some target or frame elements are not projected from English to Chinese if no alignment information is provided.	reasoning
train_176614	"From the word similarity point of view, the degree of similarity for å¢¨é¡""sunglasses"" and å°ˆåˆ©""patent"" should not be high no matter which ontology is applied, for the former is a concrete object but the latter is an abstract one."	their distance in an ontology is also far from each other.	reasoning
train_166126	"For a given phrase alignment, each pair of aligned phoneme symbols is converted into a single token by joining the symbols with symbol ':' as a delimiter."	the alignment of two sequences is converted into a sequence of these tokens.	reasoning
train_39127	"Because of the discrete choice of neutral words, the loss is no longer differentiable over the neutralization module."	we formulate it as a reinforcement learning problem and use policy gradient to train the neutralization module.	reasoning
train_173829	Texts are short and include many references to concepts that have a specific terminology.	sA methods must be adapted to the syntactic and semantic features of financial microblogs.	reasoning
train_214337	"Handling Slang We handle these cases by mapping slangs to their IV equivalents, but slang is an open class and it is difficult to detect all slangs in tweets domain."	we select the most frequent twenty slang words from 17k types in our corpus (10 million tokens) and map them to their IV equivalents.	reasoning
train_19337	"However, when tagset BMES is used, the learned constraints don't always make reliable predictions, and the overall precision is not high enough to constrain a probabilistic model."	we will only use the deterministic constraints that predict IB tags in following CWS experiments.	reasoning
train_154877	"Furthermore, damage to communication equipment, power outages, and inundation of the air waves prevented the use of mobile phones, which are generally the most important tool for communication during a disaster."	Twitter or social network services (SNS) such as mixi3 played an important role for propagating safety information among people. 	reasoning
train_160162	"Although the English unsupervised approach assumes that there are explicit word segmentations, conventional analyzers often fail to segment non-standard words in Japanese."	"to extract variants in an unsupervised fashion, we have to introduce an idea to generate correct word segmentation of variant words."	reasoning
train_73906	"Indeed, the closer a node is to a given nest, the more ants from that nest will have passed through and deposited odour components."	the odour of that node will reflect its nest neighbourhood and allow ants to find their way by computing the score between their odour (that of their mother nest) and the surrounding nodes and by choosing to go on the node yielding the highest score.	reasoning
train_39123	The proposed method requires the two modules to have initial learning ability.	"we propose a novel pre-training method, which uses a self-attention based sentiment classifier (SASC)."	reasoning
train_7683	"The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large."	we explore efficient algorithms for pruning this space that lead to empirically effective results.	reasoning
train_74534	"They not only achieved the start-of-the-art monolingual SRL performance to date, but acquired the mapping between bilingual arguments."	we follow their work to achieve bilingual SRL results for our training set.	reasoning
train_86799	"Unlike the voting algorithms, the classifiers do not require a uniform input."	we have tested if their performance can be improved by supplying them with information about the input of the first classification stage.	reasoning
train_90653	"When no constraint available, however, all word pairs in the an input sequence must be considered, leading to very poor efficiency in computation for no gain in effectiveness."	the training sample needs to be pruned properly.	reasoning
train_272	"The problem is not the ambiguity per se, but that the regular form TAG parser, unlike a standard TAG parser, does not always distinguish these multiple derivations, because root and foot adjunction are both performed by the same rule analogous to our Pop-push."	"for a given application of this rule, it is not possible to say which tree is adjoining into which without examining the rest of the derivation."	reasoning
train_112704	"For example, we linked (MEDICAL SPECIALTIES -gynaecology.n) to the Gynaecology Wikipage."	"we could include the adjective gynaecologic, pointing to the Gynaecology page, into the MEDICAL SPECIALTIES frame for sentences like ""Fellowship training in a gynaecologic subspeciality can range from one to four years""."	reasoning
train_36056	"If the number of course concepts is n, the number of all possible pairs to be checked could reach n Ã— (n âˆ’ 1)/2, which requires arduous human labeling work."	"for each dataset, we randomly select 25 percent of all possible pairs for evaluation."	reasoning
train_196758	"However, this approach suffered from the issue of noisy labels."	"some subsequent studies (Riedel et al., 2010;Hoffmann et al., 2011;Surdeanu et al., 2012) considered distant supervision relation extraction as a multi-instance learning problem, which extracted relation from a bag of sentences instead of a single sentence."	reasoning
train_73904	"Depending on the lexical information present and the structure of the graph, ants will favour following bridges between more closely related senses."	"the more closely related the senses of the nests are, the more a bridge between them will contribute to their mutual reinforcement and to the sharing of resources between them (thus forming meta-nests); while the bridges between more distant senses will tend to fade away."	reasoning
train_123209	"The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages, which can be adopted as an important source of external similarity."	the translation similarity of page pairs will influence each other.	reasoning
train_21888	"Context Feature It is expected that words acquiring new senses will tend to neighbor different sets of words (e.g. different arguments, prepositions, parts of speech, etc.)."	we define an additional type level feature to be the ratio of the number of new domain n-grams (up to length three) that contain word w and which do not appear in the old domain to the total number of new domain n-grams containing w. 	reasoning
train_160755	"We find that taking seven top-ranking subtopics (chains) can lead to a good coverage of the given topic, and selecting four sentences for each subtopic to form a paragraph will make the overall length just around 1,000."	we limit the numbers for subtopic and sentence selection as such.	reasoning
train_67548	"Even when this is not the case, it is possible that simply recognizing the name of the correct theorem or definition out of a menu does not imply that the student is fully knowledgeable about the actual content of the theorem involved."	it is plausible that asking the students to express the content of these theorems and definitions in their own words as a form of selfexplanation could lead to a deeper level of their understanding.	reasoning
train_98114	"At the time of analysis (June 2018), over 25% of the YouCook2 videos had been removed from YouTube, and therefore we do not consider them."	all our experiments operate on a subset of the YouCook2 data.	reasoning
train_44443	"Note that as we have discussed above, segmentation over pinyin sequence is also necessary to alleviate the ambiguity of pinyin-to-character mapping."	the task here for IME online working actually requires an online word segmentation algorithm if we want to keep the aim of open vocabulary learning.	reasoning
train_220113	"As in Â§6.1, for each instance in the original dataset, we find the most similar sentence in ukWac for each instance using word embeddings and add it into the dataset."	"the number of instances is doubled, and we use the enriched dataset for our sense-topic model."	reasoning
train_112046	Predicted segments provide more context to the clustering algorithm that induces the HMM states.	a more robust state representation is obtained.	reasoning
train_149465	"After mapping the Freebase types of Donald Trump to the target tag set, this sample will be weakly annotated as /person/politician, /person/tv personality, and /person/business, which is exactly the same as the type information (the ""Types From KB"" in Figure 1) obtained through EL."	"during training, when the EL system links the mention to the correct entity, the model only needs to output the types in the KB type representation."	reasoning
train_178383	"While we found our treelet system more resistant to degradation at smaller phrase sizes than the phrase-based system, it nevertheless suffered significantly at very small phrase sizes."	"it is also subject to practical problems of size, and again these problems are exacerbated since there are potentially an exponential number of treelets."	reasoning
train_40359	"In many sequence models where the vocabulary size is large, exact inference by finding the true shortest path in the graph discussed in Section 3.2 is intractable."	"approximate inference techniques such as beam search are often used, or the size of the search space is reduced, for example, by using a Markov assumption."	reasoning
train_214984	"When going to lower levels, the number of specific bursts decreases and approaches the number of bursts shared."	similar observations as the ones drawn from the counts of bursts (Figure 3) can be made.	reasoning
train_123145	"One of our goals is to investigate whether the +/-effect property tends to be shared among semantically-related senses, and another is to use a method that applies to all word senses, not just to the senses of words in a given word-level lexicon."	"we build a graph-based model in which each node is a WordNet sense, and edges represent semantic WordNet relations between senses."	reasoning
train_88776	"For both types of error corpus, it is not enough to collect a large set of sentences which are likely to contain an error -it is also necessary to examine each sentence in order to determine whether an error has actually occurred, and, if it has, to note the nature of the error."	"like the creation of a treebank, the creation of a corpus of ungrammatical sentences requires time and linguistic knowledge, and is by no means a trivial task."	reasoning
train_194720	Those extracted-and-edited sentences serve as pivotal points in the target language space to guide the unsupervised learning.	the learned target language distribution could be closer to the real one.	reasoning
train_10975	"This relatively small improvement is mainly due to the selection of the whole sentence alignment: for many sentences the best alignment still contains alignment errors, some of which could be fixed by other aligners."	it is desirable to combine alignment links from different alignments.	reasoning
train_135867	"As in conventional CNNs, the computation of the convolution operation with RNFs can be easily parallelized."	RNF-based CNN models can be 3-8x faster than their rNN counterparts.	reasoning
train_118142	"We observe 60.5% of sentences and 32.1% of opinion expressions contain other modifiers besides ""target""."	only mining the relations between opinion expressions and evaluation target is actually at risk of inaccurate and incomplete results.	reasoning
train_36970	"In this work, we make a naive assumption that two neighboring sentences share a ""common root""."	"a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the ""common root"", as shown in Figure 2."	reasoning
train_53733	"However, York#1 is defined in WordNet as the House of York, the English royal house that reigned from 1461 to 1485."	"the ISS system will consider York#1 a group instead of an entity, yielding an erroneous result."	reasoning
train_204638	The accuracy of the human annotation might be primed by what the POS tagger had tagged.	the human verifications were not treated as the “gold standard” but an inter-annotation agreement (IAA) score that was derived from the annotators‟ identification of the mis-segmented and mis-tagged tokens3. 	reasoning
train_53726	"For example, the concepts import and export are not listed in WordNet as denoting the act of importing/exporting commodities from a foreign country."	relations such as import of sweater and export of milk are mis-classified.	reasoning
train_69694	"The proposed approach still performs much better than the reference algorithms when using a general purpose stop word list, which means that the proposed approach can improve performance well even as a completely unsupervised approach without any training."	the results demonstrate that the proposed approach can be applied to different domains easily even without he proposed approach is  This work was done while the first author was working at the Hong Kong Polytechnic University supported by CERG Grant B-Q941 and Central Research Grant: G-U297.	reasoning
train_29917	"The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language."	our approach is able to capture context-dependent semantic similarities of translation pairs.	reasoning
train_193663	Our baseline QA results have also shown that FreebaseQA is a much more difficult KBQA task than either WebQSP or SimpleQuestions.	freebaseQA may be an invaluable asset to the investigation of more advanced machine learning methods for factoid KBQA problems.	reasoning
train_145228	"In other words, they were asked to read a sentence, to annotate the sentence, then to read the next sentence, and so on."	"before proceeding with the annotation, we built an annotation tool in order to provide an environment that helps unidirectional reading."	reasoning
train_215101	"Finally, this combination does not make any improvement over the baseline."	this combination of syntactic approaches cannot solve the STS task.	reasoning
train_204025	"Nevertheless, there will be differences with respect to morphosyntactic categories that do not exist in all the languages."	"we expect all the five languages to have nouns and verbs, for example, but not four case distinctions."	reasoning
train_38046	"Narayan and Gardent (2014) argued that syntactic structures do not always capture the semantic arguments of a frame, which may result in wrong splitting boundaries."	"they proposed a supervised system (HYBRID) that uses semantic structures (Discourse Semantic Representations, (Kamp, 1981)) for sentence splitting and deletion."	reasoning
train_42178	"It seems that the numbers of XX-tags in output and tokens in input should keep consistent for better performance since Seq2seq models look to somehow learn such relationship, and used it during the decoding."	using subword information as features is one promising approach for leveraging subword information into constituency parsing.	reasoning
train_200288	"Currently, one of the major focuses of language instruction is to enhance learners' ability to communicate, that is, to enhance their oral communication skills."	language assessment should emphasize the competent use of language in spoken communication.	reasoning
train_200294	"In addition, the -desu and -masu forms are taught by many of the textbooks for beginning learners of Japanese such as Genki published by The Japan Times and Japanese for Busy People I published by the Association of Japanese-Language Teaching."	sJT test items employ the -desu and -masu forms more than any other forms of honorifics.	reasoning
train_64524	"In S2, the parse tree correctly records that node [13] (""John F. Kennedy"") is not the object of the killing."	"despite its being closer to ""killed"", the candidate in S2 receives no extra credit from semantic relation matching."	reasoning
train_210575	"At the same time, the amount of patterns need to not be too large that might overwhelm a user or subsequent language assessment systems."	our goal is to return a reasonable-sized set of patterns and cover typical usage of a given word.	reasoning
train_27559	"This both allows us to identify variants of expansions (e.g., different pluralization, spelling, hyphenation, etc.) as well as finding more plausible expansions (those that are repeated multiple times in a corpus)."	each expansion/abbreviation pair has an associated count which can be used to threshold and filter for increased quality.	reasoning
train_2875	"If this rule is applied exhaustively to an MRS M, we obtain an MRS M without EPconjunctions."	it should be intuitively clear that the configurations of M and M correspond; the configurations of M also correspond to the minimal solved forms of the translation of M .	reasoning
train_192620	"For particle verbs not covered by GermaNet, we used the existing verbs as a seed set and applied a nearest-prototype (centroid) classifier to all other BVs and PVs, with a centroid for each of the 15 classes."	we were able to assign class labels to all verbs in our dataset.	reasoning
train_160307	"Once the lower bound of dimensions is reached, the algorithm gets enough degrees of freedom."	"from this point onwards, even if we increase dimensions, there will not be any statistically significant difference in the standard deviation."	reasoning
train_219420	"Specifically, given a collection of documents retrieved for a target language, the task is to identify the documents that contain text in English in addition to the target language."	"we re-train each system for each target language, using only training data for English and the target language."	reasoning
train_168453	"By separating the notion of 1-gram from that of word spelling, we were able to consider any sequence of morphemes a separate grammatical word according to our needs."	we could treat both the 1-gram Papierballs 'paper ball' and the 2-gram Balls Papier 'ball of paper' as a sequence of two grammatical words.	reasoning
train_55528	"It should be noted at this point that these observations regarding feature weight behavior are based on subjective intuition of how characteristic features are for a word meaning, which is quite difficult to assess systematically."	we next propose a quantitative measure for analyzing the quality of feature vector weights.	reasoning
train_124749	The core idea behind BRAE is that a phrase and its correct translation should share the same semantic meaning.	they can supervise each other to learn their semantic phrase embeddings.	reasoning
train_95021	Each NP that is extracted represents one training example.	if an NP is incorrect then we label it to one of the label from Table 2.	reasoning
train_164994	"In raw Vietnamese texts, space symbol can be treated as an overload symbol, which is a connector within a word or is a separator between words."	the Vietnamese word segmentation (VWS) problem can be defined as a binary categorization task for each space symbol.	reasoning
train_75502	"Although it is straightforward to recast skill inference as a standard text classification problem, i.e., predicting the skills with the profile text alone, personal profiles usually are poorly organized, even with critical information missing."	"predicting the skills with the profile text alone, personal profiles usually are poorly organized, even with critical information missing."	reasoning
train_134388	Our analysis of the document corpus revealed that most text edits to sentences do not significantly change the structure of the sentence.	a simple heuristic based approach can be used to identify edit types.	reasoning
train_167357	"After creating the N-gram model, duplicated N-grams have been removed."	data set size has been reduced by approximately 99.4%.	reasoning
train_176609	"However, 'teacher', also denotes a kind of occupation and should be regarded as an 'occupation value' as well."	we mark the semantic function of ' teacher' as {OccupationValue|è·æ¥­å€¼}to include both meaning facets.	reasoning
train_55501	"A feature represents another word (or term) w with which w co-occurs, and possibly specifies also the syntactic relationship between the two words, as in Grefenstette (1994), Lin (1998), and Weeds and Weir (2005)."	"a word (or term) w is represented by a feature vector, where each entry in the vector corresponds to a feature f ."	reasoning
train_42959	"In addition, given that the local interactions, which do not contain equally valuable information, are used as input into ABS-LSTM across time steps, it is understandable that the produced states do not contribute equally to recognizing emotion."	"we incorporate two levels of attention mechanism into ABS-LSTM, i.e., Regional Interdependence Attention and Global Interaction Attention."	reasoning
train_164304	"ParaLink is particularly efficient for this type of data since it combines the power of disambiguation PPRSim model with ability to efficiently cluster misspelled and corrupted names, that are typical for forum posts."	it achieves a better performance on a dataset with more informal documents.	reasoning
train_218508	"Our model performs best and achieves a significant boost in MR and Hits@10 compared to the baselines, while IKRL slightly outperforms TransE in terms of MR only."	the results confirm the robustness of our method for large-scale datasets.	reasoning
train_88778	"For each sentence in the original tagged corpus, an attempt is made to automatically produce four ungrammatical sentences, one for each of the four error types."	"the output of the error creation procedure is, in fact, four error corpora."	reasoning
train_6428	A second objective was to extend the information that could be learned from the process beyond hyponyms of a given word.	the approach was extended to finding lexical patterns that could produce synonyms and other standard lexical relations.	reasoning
train_23907	They extracted opinion targets/words in advanced through simple phrase detection.	the extraction performance is far from expectation.	reasoning
train_202907	Lexical patterns are always more remote from the fully lexicalized instances than the superlexical ones and are allowed only indirect contributions to the overall interpretation of a given sentence.	"""superlexical"" semantics supersedes the ""lexical"" semantics."	reasoning
train_133445	"Compared to many traditional NLP systems, which are trained on newswire or similar text, an automated system capable of explaining abstract language, or finding alternative text expressions for slang (and other unknown terms) would exhibit a deeper understanding of the nuances of language."	research in this area may open the door to a variety of interesting use cases.	reasoning
train_33676	We apply threshold algorithm to obtain the top short texts based on each query word.	the top-1 result comes from these two ranked lists based on threshold algorithm.	reasoning
train_146656	"To isolate and evaluate the impact of the above (combinatorial) factors, a large experimental study was necessary."	"we conducted over 2.3K experiments on 8 popular, large, datasets of sizes ranging from 120K-3.6M."	reasoning
train_45504	"But as the dependency tree edges have directions, a word plays a different role regarding it is the head or the dependent in an edge."	"instead of using one vector representation, we employ two vectors to distinguish the two roles (Dozat and Manning, 2017)."	reasoning
train_65775	"Unfortunately, an analysis of the parse-based heuristic on our training data (the data set will be described in Section 4), uncovered numerous, rather than just a few, sources of error."	"rather than trying to handcraft a more complex collection of heuristics, we chose to adopt a supervised machine learning approach that relies on features identified in this analysis."	reasoning
train_107127	"Only a limited set of characters are used as family names, while the first name can be any character(s)."	the family name is a very important and useful feature in identifying an NE in the person category.	reasoning
train_150993	"With the development of Web and the open of accessible electronic documents, digital library, and scientific articles, these resources will become more and more abundant."	"method 3 is a feasible way to solve the terminology translation acquisition, which is also validated by the following experiments."	reasoning
train_150759	The unsatisfactory results of our CoNLL preposition SRL system show that the relatively simplistic feature sets used in our research are far from sufficient.	"we will direct our future work towards using additional NLP tools, information repositories and feature engineering to improve all three stages of preposition semantic role labelling."	reasoning
train_136582	"And indeed, if a difference of concreteness between components of an expression hints at a metaphor, as is proposed by the conceptual metaphor theory, then it is plausible that it does not hint at novelty of the metaphoric expression at the same time."	we investigate a feature with more semantic capacity in the next subsection.	reasoning
train_164483	"The system workflow is based on representation models applied to all resources, which represent objects of linguistic processing, namely Knowledge Bases (KBs), Web pages and full texts."	"we develop an architecture, which takes advantage from the semantic information stored in Linguistic Resources (LRs) and is based on the integration of NooJ."	reasoning
train_83854	"In fakeness discrimination, we treated all classes the same."	we would like to incorporate class differences by enforcing larger margin between certain classes.	reasoning
train_98170	"Put differently, the oracle policy selects the set of unlabeled examples that maximizes the target metric of our model on a set sampled from the same distribution as the test set."	" the oracle policy enjoys extremely favorable conditions compared to a trained policy, and we expect it to provide an upper bound on the performance of πθ."	reasoning
train_169414	We observed that a combination of our three methods of annotation that roughly favours T O annotations will pull down precision very close to the worst precision of the three methods and will provide a very low improvement of recall.	association rules could probably suggest a better combination of these three methods.	reasoning
train_217537	Only 109 of the 1453 senses identified in Portuguese did not have an equivalent verb sense in English identified in Propbank.	"as the frame files of such 109 verbs could not obtain the fields brought from Propbank automatically, they required manual edition."	reasoning
train_203814	Space is the common separator of words.	space marks are used as the explicit delimiters or token separator.	reasoning
train_70238	"Random stem generation Keeping the suffix fixed and randomly selecting a stem ties the generated form to the syntactic context, but changes the semantics."	"these generated errors are firstly semantic errors (#1b), featuring stems inappropriate for the context, in addition to having some other morphological error."	reasoning
train_187465	"For the POS induction task, we specifically need embeddings that capture syntactic similarities."	"we experiment with two types of embeddings that are known for such properties: Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word."	reasoning
train_40582	"Suggestively, the one task where NLI clearly outperforms NMT is WC."	NLI training is better at preserving shallower word features that might be more useful in downstream tasks (cf. Figure 2 and discussion there).	reasoning
train_17745	It scales almost as well to large datasets as standard N-Gram models: training requires only two passes over the data as opposed to a single pass required by N-Gram models.	"the experiments provide empirical evidence that the VMM is based on a reasonable set of modeling assumptions, which translate into an accurate and scalable model."	reasoning
train_86260	We can also find that LSTM-CSS and LSTM-CSS-E nearly perform equally.	we believe that attention mechanism can make the LSTM translation network focus on relevant content and alleviate the influence of the places where local constraints are input.	reasoning
train_165200	"Users may further find more alternatives to complete the task by directly employing patterns such as ""how to"" + task description."	potential how-knowledge is implicitly embedded in search query logs.	reasoning
train_218071	"In the BAKING A CAKE scenario, for example, there is little to no difference between mentions of making the dough and preparing ingredients."	these two events are often confused: Approximately 50% of the instances labeled as PREPARE INGREDIENTS are actually instances of MAKE DOUGH.	reasoning
train_167801	"Since our extraction method is deterministic, all lemmas with the same signature will be parsed in the same way."	it is sufficient for a human to verify and correct a single lemma with a particular signature to determine that all lemmas with that same signature are correctly extracted.	reasoning
train_75808	"While most of these are wide-coverage grammars, and are being actually used for morphological disambiguation in Apertium, they are also too big and complex to be easily used for the early stages of parser development."	"we have written a small Hungarian CG, aimed to fully disambiguate a short Hungarian story, which was used as the development corpus."	reasoning
train_160648	We also show that combining multiple related language pivot models can rival a direct translation model.	the use of subwords as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus.	reasoning
train_154355	"In addition, ConceptNet defines nearly thirty kinds of semantic relations, such as CapableOf (agent's ability), SubeventOf (event hierarchy), MotivationOf (affect), DesireOf (want to), and so on, most of which are not included in WordNet."	"if extending WordNet with the large amounts of semantic relations contained in ConceptNet, it is desirable to improve the performances of WordNet-based WSD methods."	reasoning
train_218645	"The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015), named entity recognition (Kuru et al., 2016), and time normalization (Laparra et al., 2018a)."	"there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from."	reasoning
train_131435	"In addition, since ADADELTA computes the moving average of updates, and ADAM needs to compute the bias-corrected gradient estimate, they require more intricate implementations."	these two methods are not as popular as ADAGRAD for beginners.	reasoning
train_69200	"However, detailed research (Zhou et al., 2005) shows that it's difficult to extract new effective features to further improve the extraction accuracy."	"researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g."	reasoning
train_210383	"Moreover, learning variables also affect the accent of new and similar vowels in Hong Kong Cantonese produced by non-native speakers."	this study explores the factors that influence the production of the new and similar vowels through having native speakers listen to the Pakistani children speak Cantonese and by the correlation of learning factors and accent ratings.	reasoning
train_194721	A proper initialization can also help align the semantic spaces and extract potential parallel pairs within them.	"following the previous methods, we use the inferred bilingual dictionary as described in Conneau et al. (2018) for unrelated language pairs and the shared BPE in Lample et al. (2018b) as initialization for related ones."	reasoning
train_11169	"Commonly adopted metrics for ranking, such as mean average precision (Buckley and Voorhees, 2000) and Normalized Discounted Cumulative Gain (JÃ¤rvelin and KekÃ¤lÃ¤inen, 2000), is designed for data sets with human relevance judgment, which is not available to us."	"we use the Kendall's tau coefficient (Kendall, 1938; Joachims, 2002) to measure the degree of correlation between two rankings."	reasoning
train_28517	"Due to having fine-grained frame semantic role labeled for each sentence, we think the filler of DNI maybe also instantiates the FE of other annotated frames in the context."	"we collect the overt FE content set Ï• instantiated in the discourse, and this set forms the overall set of candidates for DNI linking."	reasoning
train_123156	"LGC fits in a univariate regularization framework, where the output matrix is treated as the only variable in optimization, and the optimal solutions can be easily obtained by solving a linear system."	we adopt the LGC method in this paper.	reasoning
train_130971	"Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier."	its discriminative module might suffer from the overwhelming size of text features.	reasoning
train_36057	"Instead of a simple descriptive feature, each of our proposed feature determines whether a concept pair has prerequisite relation from a specific aspect; its function is similar to an independent weak classifier."	"rather than using a linear combination of features for classification (e.g., SVM and LR), a boosting model (e.g., Random Forest) is more suitable for this task."	reasoning
train_197200	"In other words, if for each document, RNN finds one good way of ordering relevant labels (such as hierarchically) and allocates most of the probability mass to the sequence in that order, the model still assigns low probabilities to the ground truth label sets and will be penalized heavily."	the model has little freedom in discovering and concentrating on some natural label order.	reasoning
train_169405	Some occurrences are terminological and other are not.	a global decision at the corpus level about the terminological status of all occurrences of a lexical unit would then be erroneous.	reasoning
train_123152	"Sometimes, events have positive or negative effects on agents or other entities as well."	"in this paper, we consider a sense to be +effect (-effect) if it has +effect (-effect) on an entity, which may be the agent, the theme, or some other entity."	reasoning
train_174703	"The phenomenon of attribution has not typically been studied directly, but instead as part of other phenomena such as opinion analysis, discourse analysis, or the analysis of dialogue in narrative."	there are many existing resources that contain annotations relevant to attribution but which fail to fully capture all attributive phenomena or to adequately label all parts of attributions.	reasoning
train_219792	We want to keep words referring to actual objects in the image.	"we use V (x j ), a visual similarity score, as our confidence of an object corresponding to word x j ."	reasoning
train_215342	Twitter is an increasingly popular social media platform among the Arabic speaking people who tend to frequently use their Arabic dialects when sharing their stories and opinions.	we focused on Twitter to collect the data for our corpus.	reasoning
train_6097	All of the component distributions of our lexical and distortion models are multinomials.	"upon assuming these expectations as values for the hidden alignment vectors, we maximize likelihood of the training data simply by computing relative frequencies for each component multinomial."	reasoning
train_185167	"However, larger data sets come with increased demands on computational resources; moreover, they tend to include redundant information as their size increases."	"the performance gain curves of large-scale systems with respect to the amount of training data often show ""diminishing returns"": new data is often less valuable (in terms of performance gain) when added to a larger pre-existing data set than when added to a smaller pre-existing set (e.g., (Moore, 2003))."	reasoning
train_88781	"The standard deviation of results across crossvalidation runs is below 0.006 on all measures, except for Method 4."	we only report average percentages.	reasoning
train_165811	"Since we want to test the performance of our system specifically for low-resource languages, no suitable evaluation data could be obtained from any standard tasks."	"we constructed a small evaluation corpus containing 20 ambiguous lemmas selected from a set of high-and mediumfrequency words in Norwegian Nynorsk, in the style of the English lexical substitution task (McCarthy and Navigli, 2009)."	reasoning
train_47597	"Popular word representations for classification train word embeddings using the context of each word (a window around the word, e.g., in skip-gram or continuous bag of words methods) (Mikolov et al., 2013; Bojanowski et al., 2016)."	"we seek to understand how semantic contexts of words shift across time, in addition to the words themselves."	reasoning
train_184654	"The numbers above are interesting because they provide intrinsic evaluation of the concept induction procedure, but they do not tell us much about their usability."	"and in order to better assess the value of the discovered concepts, we decided to carry out two extrinsic evaluations using an information extraction task."	reasoning
train_38577	"Such systems rely on end-to-end training on large amounts of data, making no prior assumptions about linguistic structure and focusing on stastically frequent patterns."	they somehow step away from computational linguistics as they learn implicit linguistic information automatically without aiming at explaining or even exhibiting classic linguistic structures underlying the decision.	reasoning
train_75504	"For example, it is very likely that C++, C, and Python programming languages may co-occur in the one's profile, i.e., if a person has skill C++, it is highly possible that he would have the skills such as C or Python."	it is useful to integrate skill connection information when inferring personal skills.	reasoning
train_116461	The quality of this approximation is dependent on how accurately the N-best lists represent the search space of the system.	"the hypothesis list is iteratively grown: decoding with an initial parameter vector seeds the N-best lists; next, parameter estimation and N-best list gathering alternate until the search space is deemed representative."	reasoning
train_73731	"It is worth noting that the results of the proposed hybrid system is very close to the results of the rule-based component when it comes to the numerical and temporal expressions, and the two approaches achieve the same results in recognizing NEs of the 3rd group."	the hybrid approach proves its suitability for the recognition of the three groups of NEs.	reasoning
train_198939	"However, this approach is not applicable to words such as `break' in Japanese, 'hit' in Cantonese, or 'wash' in Vietnamese, where there is no decomposable morphology."	"for languages with little morphology, a different approach is necessary."	reasoning
train_121527	"These results echo the category-level findings, that word-level brain activity is also organised in a similar way across lobes."	this diminishes our chances of uncovering neat interactions between models and brain areas (where for instance the Window2 model correlates with the frontal lobe and Object model matches the occipital lobe).	reasoning
train_55498	"Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus."	"we may infer that visit-country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992;Dagan, Marcus, and Markovitch 1995;Karov and Edelman 1996;Ng and Lee 1996;Ng 1997;Dagan, Lee, and Pereira 1999;Lee 1999;Weeds and Weir 2005)."	reasoning
train_164303	Adding paraphrase clustering (step II and III) further improves the B 3 +F score to achieve 79.7% and 80.5% correspondingly.	we show that paraphrase similarity can be efficiently incorporated into the entity linking pipeline and improve the performance.	reasoning
train_199723	It is clear that the order of words in the reduced sentence is different from the input sentence.	with our configuration the larger tree t can procedure a smaller tree s with a changeable of the word order by using a sequence of operators.	reasoning
train_131434	"This cycle repeats, until some learning rates decrease to zero and learning effectively stops."	the algorithm will never break the tie or infer good topics.	reasoning
train_21904	"More simply put, we count how many of the phonetic prefixes of the words in the sentence are repeated, and then we normalize this value by the total number of phonemes in s. The rhyme feature works exactly in the same way, with the only difference that we invert the phonetic representation of each word before adding it to the TRIE."	we give higher scores to sentences in which several words share the same phonetic ending.	reasoning
train_136583	"Generally, very novel metaphors also display a high metaphoricity."	"we expect that low-POM verbs (i.e., verbs that occur similarly often in many different contexts) exhibit a low novelty score on average and low variance, while high-POM verbs should show a higher average novelty score."	reasoning
train_95722	Each type has a characteristic mapping between semantic roles and opinion holders and targets.	the problem of opinion role induction is reduced to automatically categorizing opinion verbs.	reasoning
train_156698	"Undoubtedly, manual compilation is the best way to create such an emotion lexicon but is much expensive in terms of time and human effort."	"the objective of the present paper is to develop a method for automatically creating such a list of words from the glosses of a dictionary, as well as from a thesaurus and a corpus."	reasoning
train_77443	The difference between source and target in the models that use constituency trees is especially substantial and statistically significant.	it is apparent that the suspected lower quality of constituency parse trees of MT output is not the reason for the lower QE performance.	reasoning
train_96493	"Intuitively, we know the 4-th and 5-th sentence can give strong supporting ideas to illustrate why the author concludes the story with the last paragraph."	it proves that our attention mechanism on sentences captures the key sentences to represent essays indeed.	reasoning
train_182393	"While HTER has been shown to correlate quite well with human judgment of MT quality, it is quite challenging to obtain HTER scores for MT output, since this would require hiring and training human subjects to perform the editing task."	other metrics such as BLEU or TER are used as proxies for HTER.	reasoning
train_67336	"Identifying sets of objects originally followed the incremental algorithm (Dale and Reiter 1995), as in (Bateman 1999), (Stone 2000) and (Krahmer et al. 2003), with limited coverage, since only few attributes typically apply to all intended referents and to none of the potential distractors."	"van Deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations."	reasoning
train_197171	"(2011) notes that on sentence compression, longer sentences are perceived by human annotators to preserve more meaning than shorter sentences, controlling for quality."	the drop in human-judged adequacy may be related to our sentences' relatively short lengths.	reasoning
train_99333	"This compares favorably to the best accuracy reported in (Stamatatos et al., 2000) of 72% on group A and 70% on group B."	"our accuracy improvement is 2% on group A and 18% on group B, which is surprising given the relative simplicity of our method."	reasoning
train_19369	"We note that these smoothing parameters are tailored to short sentences: in a binary tree, the number of constituents grows linearly with sentence length, whereas the number of distituents grows quadratically."	the ratio of constituents to distituents is not constant across sentence lengths.	reasoning
train_47630	"Moreover, a Bayes test could directly compute the probabilities of the hypotheses, which help users to make a more reasonable decision."	"a Bayes test is increasingly preferred and recommended recently as an advanced tool to analyze the experimental results (Benavoli et al., 2016)."	reasoning
train_122586	"Since we are exploiting automatically computed (and thus potentially noisy) stance information, we hypothesize that the effectiveness of such information would depend in part on the way it is exploited in RC systems."	"we introduce a set of stance-supported models for RC, starting with simple pipeline models and then moving on to joint models with increasing sophistication."	reasoning
train_10113	"While such supervised approaches have yielded accurate parsers (Charniak, 2001), the syntactic annotation of corpora such as the Penn Treebank is extremely costly, and consequently there are few treebanks of comparable size."	there has been recent interest in unsupervised parsing.	reasoning
train_99882	"The results of the study confirmed the predicted effect and, moreover, provide a classification of clarification question types."	the particular kinds of analysis found to initiate insertion sequences in HHC situations are clearly active in HCI clarification questions as well.	reasoning
train_170184	"Team members in highly-demanding operational tasks do not often notice triggers that cause them to be emotionally and mentally stressed (Murphy, 1996; Stein, 2001)."	they might communicate with their teammates and express their emotions in ways that may not be noticed in observable audio-visual cues.	reasoning
train_108431	"We also compare our system with LiveTrans, which only searched within English web pages, thus with limited search space and more noises (incorrect English candidates)."	it was more difficult to select the correct translation.	reasoning
train_28516	"However, this is difficult for automatic detector, which inevitably introduces some false detected NIs."	we conduct a second-level identifying.	reasoning
train_78949	"But as we know, the words used in poems, especially poems written in ancient time, are different from modern languages."	"the existing methods may fail to generate meaningful poems if a user wants to write a poem for a modern term (e.g., Barack Obama)."	reasoning
train_6090	"Using the joint training technique of Liang et al. (2006) to initialize the model parameters, we achieve an AER superior to the GIZA++ implementation of IBM model 4 (Och and Ney, 2003) and a reduction of 56.3% in aligned interior nodes, a measure of agreement between alignments and parses."	"our alignments yield more rules, which better match those we would extract had we used manual alignments."	reasoning
train_6092	"However, no technique has yet been shown to robustly extract smaller component rules from a large transducer rule."	"for the purpose of maximizing the coverage of the extracted translation model, we prefer to extract many small, minimal rules and generate larger rules via composition."	reasoning
train_172152	"To solve this problem, the recording of the user screen is manually segmented into one or several actions thanks to the log file."	the measured duration accurately correspond to the actual actions.	reasoning
train_98528	"These applications represent different domains (e.g. restaurants, flight booking services, smart homes, etc.) and they are created by system admins."	"they contain a user-defined label for every utterance, which serves as our ground truth."	reasoning
train_195379	"However, when considering high-capacity neural parametrizations that condition on the whole input sequence, both model classes are theoretically equivalent in terms of the distributions they are capable of representing."	the practical advantage of global normalization in the context of modern neural methods remains unclear.	reasoning
train_214985	"This case study leads to several important obser-vations: Frontiers can be identified when there are few bursts across a position and many before/after that position; words that are bursty at one level in the topic hierarchy (i.e., specific at this level) can become general for lower levels in the hierarchy; when going to lower levels in the topic hierarchy, the number of bursts decreases; there are segments with no bursty words."	"burst analysis is relevant in the context of hierarchical topic segmentation, but an appropriate way to exploit it has to be proposed; we address this open issue in the following section."	reasoning
train_206356	"Moreover, while structural complexity per se is independent of the issue of the derivational relationship between the causative and inchoative alternants, it has been also standard to assume that causatives are virtually derived from their inchoative counterparts via operations like Predicate Raising in Generative Semantics or Head Movement in the GB/MP framework."	"the causativization approach, which views causatives as based on inchoatives, has been influential across a variety of theoretical perspectives."	reasoning
train_155476	We also implemented it but the results were similar that those using MBR decoding in each system and keeping the 1best translation.	"we maintained MBR decoding for the rest of the experiments, which is also easier to work with."	reasoning
train_130969	"Universal Schemas (Riedel et al., 2013;Verga et al., 2015;Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it's designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus."	"it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type."	reasoning
train_123211	"Some existing parallel corpora are subject to subscription or license fee and thus not freely available, while others are domain-specific."	a lot of previous research has focused on automatically mining parallel corpora from the web.	reasoning
train_217589	Text layout supports a large part of semantics and participates to the coherence of the text; it thus contributes to the elaboration of the discourse.	"we adapted the discourse analysis to treat the layout, according to the following principles: -a DU corresponds to a visual unit (a bloc); -two units sharing the same role (title, paragraph, etc.) and the same typographic and dispositional markers are linked with a multinuclear relation; otherwise, they are linked with a nuclear-satellite relation."	reasoning
train_62404	"The phenomenon we discuss in this article has been a subject of interest for linguists, philosophers, and computational linguists for decades."	"it has been addressed in various contexts from a variety of perspectives, as discussed in the following sections."	reasoning
train_145602	"However, the distributions of the normalized attention weights converge to a few words for the shorter text generation task; thus the normalized attention weights does not satisfy the assumption."	"our proposed HCL with non-normalized attention weights can accurately compute this inconsistency, contrary to the HCL with normalized attention weights."	reasoning
train_138692	"Previous work on code-switched POS tagging has achieved accuracies as high as 90.20% (Bhat et al., 2018), but because those kinds of accuracies require supervised, in-domain training data, they are not directly comparable with the scenario that we are concerned with."	we compare against a natural baseline for our task: use a monolingual tagger and treat everything as if it were a single language.	reasoning
train_136574	We use a best-worst scaling factor of 1.5 and four items per tuple.	each metaphor appears in six different best-worst scaling comparisons.	reasoning
train_149705	Language variations exist among source and target languages and training on monolingual corpus might limit learning the variations.	"we translate the source language, English, to the target language for the training corpus to reconstruct source language sentences and learn source and target languages jointly."	reasoning
train_58184	"The book is unusually clear and honest in highlighting limitations in the current understanding of crucially relevant concepts, including existing formalization techniques."	"the book is not only a valuable summary of the currently available tools for interpreting motion, but also a useful starting point for further research that aims to fill various gaps identified by Mani and Pustejovsky's exploration of the field."	reasoning
train_160436	"Speech and gesture information within the same semantic category can then be fused to form a complete multimodal meaning, where previous methods on representing multimodal semantic (Bergmann and Kopp, 2008;Bergmann et al., 2013a;Lascarides and Stone, 2009; Giorgolo, 2010) can be applied."	this enables HCIs to construct and represent multimodal semantics of natural communications involving iconic gestures.	reasoning
train_108964	We see that the taggers trained on data selected by MWC outperform those trained on randomly selected data.	"in the main experiments, we always use MWC to select the set of manually tagged data for training T anno ."	reasoning
train_160317	"Then, for each 10 sentence pairs, the 5th and 10th pairs are used for the development and test sets respectively, and 8 remaining pairs are used for the training set."	"each of the development and test sets has 2,915 pairs, and the training set has 23,320 pairs."	reasoning
train_74537	"If we ignore it and its target counterpart, the remaining PAS is still reasonable."	we extend the PAS transformation rules based on this insight.	reasoning
train_117281	"Unfortunately, the text entry rates provided by AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009)."	researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques.	reasoning
