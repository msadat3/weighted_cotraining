id	sentence1	sentence2	label
train_543	"Specifically, we do not make use of the possibilities offered by the interleaving of the RE and LC, as the examples we cover are too simple."	"this setup enables RE, in principle, to make use of information about precisely how a previous reference to an entity has been realised."	contrasting
train_2447	"Since the position of a word plays an important role as a syntactic constraint in English, the methods are successful even with local information."	"these methods are not appropriate for chunking Korean and Japanese, because such languages have a characteristic of partially free wordorder."	contrasting
train_2758	"For training coreference classifiers and locally-optimized anaphoricity models, we use both RIPPER and MaxEnt as the underlying learning algorithms."	"for training globally-optimized anaphoricity models, RIPPER is always used in conjunction with Method 1 and Max-Ent with Method 2, as described in Section 2.2."	contrasting
train_3573	Table 4 revealed that our simple method of filtering caused a fatal bias in training data when a preliminary distribution was used only for filtering.	the model combined with a preliminary model achieved sufficient accuracy.	contrasting
train_3665	"Clearly, when the system returns at most one or two attachments, the recall on the polysemous nodes is lower than on the Full set."	it is interesting to note that recall on the polysemous nodes equals the recall on the Full set after K=3.	contrasting
train_3790	"For these purposes, and because of its higher content validity, IPSyn scores often tells us more than MLU scores."	the MLU holds the advantage of being far easier to compute.	contrasting
train_5331	The solution for a maximization problem can be found using an exhaustive search method.	the complexity is very high in practice for a large number of pairs to be processed.	contrasting
train_5484	"Most speech recognition systems perform well when trained for a particular accent (Lawson et al., 2003)."	"with call centers now being located in different parts of the world, the requirement of handling different accents by the same speech recognition system further increases word error rates."	contrasting
train_6391	"In this point, one can say that the grammar-driven tree kernel is a specialization of the PT kernel."	"the important difference between them is that the PT kernel is not grammar-driven, thus many nonlinguistically motivated structures are matched in the PT kernel."	contrasting
train_6431	"In some recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought."	"this work does not focus on differentiating among different relations, so that the generated relations might conflate a number of distinct ones."	contrasting
train_7691	We have shown that VB is both practical and effective for use in MT models.	"our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession."	contrasting
train_8998	"Parsing schemata are not suitable for directly describing deterministic parsers, since they work at a high abstraction level where a set of operations are defined without imposing order constraints on them."	"many deterministic parsers can be viewed as particular optimisations of more general, nondeterministic algorithms."	contrasting
train_10116	"Additionally prototype-driven grammar induction needs to be used in conjunction with other unsupervised methods (distributional similarity and CCM (Klein and Manning, 2004)) to attain reasonable accuracy, and is only evaluated on length 10 or less sentences with no lexical information."	"gE uses only the provided constraints and unparsed sentences, and is used to train a feature-rich discriminative model."	contrasting
train_10979	"This is similar to the ""loose phrases"" described in (Ayan and Dorr, 2006a), which increased the number of correct phrase translations and improved the translation quality."	removing incorrect content word links produced cleaner phrase translation tables.	contrasting
train_11171	"For example, in query pair (Toyota Camry, ), 9/13 English pages are anchored by the URLs containing keywords ""toyota"" and/or ""camry"", and 3/5 constraint documents' URLs also contain them."	the URLs of returned Chinese pages are less regular in general.	contrasting
train_11401	"The correlation coefficient for the NIST score is still slightly negative, indicating that trying to take a word sequence's information content into account is hopeless at the sentence level."	"the correlation coefficient for the BLEU score almost doubles from 0.078 to 0.133, which, however, is still unsatisfactory."	contrasting
train_12314	"Typically, a set of seeds consists precisely of the labeled subsequences."	"if the labeled subsequences are long and have substructure, e.g., 'San Remo, Italy', our system splits at the separator token, and creates additional seed sets from prefixes and postfixes."	contrasting
train_12311	Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web.	the primary approach (supervised learning of relation-specific extractors) requires manually-labeled training data for each relation and doesn't scale to the thousands of relations encoded in Web text.	contrasting
train_13363	"An alternative approach might be to simply treat messages as unnormalized probability distributions, and to minimize the KL divergence between some approximating messageÎ¼(w) and the true message Âµ(w)."	messages are not always probability distributions and -because the number of possible strings is in principle infinitethey need not sum to a finite number.	contrasting
train_13641	"The most well known international evaluation on the factoid QA task is the Text REtrieval Conference (TREC) 1 , and the annotated questions and answers released by TREC have become important resources for the researchers."	"when facing a non-factoid question such as why, how, or what about, however, almost no automatic QA systems work very well."	contrasting
train_15309	"In the training data, many of the sentences are questions directed to a second person, you."	"chinese questions do not invert and the subject remains in the canonical first position, thus the transition from the start of sentence to you is highly weighted."	contrasting
train_18093	"Like us, Ozbal and colleagues use both a textual model and a visual model (as well as Google adjective-noun cooccurrence counts) to find the typical color of an object."	"their visual model works by analyzing pictures associated with an object, and determining the color of the object directly by image analysis."	contrasting
train_18434	Previous works have showed that supervised learning methods are superior for this task.	the performance of supervised methods highly relies on manually labeled training data.	contrasting
train_19130	our ranking reordering model indeed significantly reduces the crossing-link numbers over the original sentence pairs.	"the performance of the ranking reorder model still fall far short of oracle, which is the lowest crossing-link number of all possible permutations allowed by the parse tree."	contrasting
train_19336	"As shown in Table 8, when tagset IB is used for character tagging, high precision predictions can be made by the deterministic constraints that are learned with respect to this tagset."	"when tagset BMES is used, the learned constraints don't always make reliable predictions, and the overall precision is not high enough to constrain a probabilistic model."	contrasting
train_21229	"As the Figures show, in both tasks, SHEM achieved high performances with 11 emotions."	bHEM achieved high performances with six emotions.	contrasting
train_21886	"If it were the case that only one sense existed for all word tokens of a particular type within a single domain, we would expect our word type features to be able to spot new senses without the help of the word token features."	"in fact, even within a single domain, we find that often a word type is used with several senses, suggesting that word token features may also be useful."	contrasting
train_21893	"We observe that words that are less frequent in both the old and the new domains are more likely to have a new sense than more frequent words, which causes the CONSTANT base line to perform reasonably well."	it is more difficult for our models to make good predictions for less frequent words.	contrasting
train_21889	"The idea is that, if a word is used in one of the known senses then its context must have been seen previously and hence we hope that the PSD classifier outputs a spiky distribution."	if the word takes a new sense then hopefully it is used in an unseen context resulting in the PSD classifier outputting an uniform distribution.	contrasting
train_22467	"When available, using in-project training data proves significantly more successful than using out-of-project data."	"we find that when using out-of-project data, a dataset based on more words than code performs consistently better."	contrasting
train_23906	"In addition, (Hai et al., 2012) extracted opinion targets/words in a bootstrapping process, which had an error propagation problem."	"we perform extraction with a global graph co-ranking process, where error propagation can be effectively alleviated."	contrasting
train_23942	Most previous work using PR mainly experiments with featurelabel constraints.	we explore a rich set of linguistically-motivated constraints which cannot be naturally formulated in the feature-label form.	contrasting
train_26101	"Not having coreference resolution leads to vague questions, some of which can be filtered as discussed previously."	further work on filters is needed to avoid questions such as: Source sentence: Air cools when it comes into contact with a cold surface or when it rises.	contrasting
train_26420	"As presented in subsection 3.2, the method combines translation model and language model to rank the sentence pairs in the general-domain corpus."	it does not evaluate the inverse translation probability of sentence pair and the probability of target language sentence.	contrasting
train_26417	"Related work in literature has proven that the expanded corpora can substantially improve the performance of ma-chine translation (Duh et al., 2010; Haddow and Koehn, 2012)."	the methods are still far from satisfactory for real application for the following reasons: ï‚Ÿ There isn't ready-made domain-specific parallel bitext.	contrasting
train_26479	"In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a good label for this topic."	"previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic (Mei et al., 2007)."	contrasting
train_28855	We borrow the feature templates from Sagae and Lavie (2006).	"we found the full feature templates make training and decoding of the structured perceptron much slower, and instead developed simplified templates by removing some, e.g., that access to the child information on the second top node on the stack."	contrasting
train_28860	"This is particularly true with DP; it sometimes outperforms Z&C, probably because our simple features facilitate state merging of DP, which expands search space."	our main result that the system with optimal search gets a much higher score (90.7 F1) than beambased systems with a larger beam size (90.2 F1) indicates that ordinary beam-based systems suffer from severe search errors even with the help of DP.	contrasting
train_28852	"In practice, this global model is much stronger than the local MaxEnt model."	"training this model without any approximation is hard, and the common practice is to rely on well-known heuristics such as an early update with beam search (Collins and Roark, 2004)."	contrasting
train_29598	"Most Open IE systems employ syntactic information such as parse trees and part of speech (POS) tags, but ignore lexical information."	previous work suggests that Open IE would benefit from lexical information because the same syntactic structure may correspond to different relations.	contrasting
train_29599	"The results in the first two rows indi-
cate that adding unsmoothed lexical information
to the method of Xu et al. (2013) is not helpful, which we attribute to data sparsity."	smoothed word representations do improve F-measure.	contrasting
train_29940	"Although the 1,000-best oracle remains at the same level over the iterations, the 1,000best average score 5 increases by 2 BLEU at the last iteration over the first 1,000-best hypotheses produced by Moses, pointing out a strong improvement of the average quality of the 1,000-best hypotheses."	"except for the IN configuration on medical En→Fr, multi-pass Moses does not bring improvements by itself over the Rerank baseline."	contrasting
train_32111	Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way.	distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data.	contrasting
train_33181	"Our stancetaking pathbased features that we identified as intuitively having a connection to the Disagree Strongly class together cover only 51% of Disagree Strongly instances, meaning that it is in principle impossible for our system to identify the remaining 49%."	"our decision to incorporate only features that are expected to have fairly high precision for some class was intentional, as the lesson we learned from the Faulkner-based system is that it is difficult to learn a good classifier for stance classification using a large number of weakly or non-predictive features."	contrasting
train_33699	Such clustering is usually performed through manually defined semantic and syntactic features defined over argument instances.	the representation based on these features are usually sparse and difficult to generalize.	contrasting
train_35770	Manual annotations are often inconsistent and annotation errors can thus be identified by looking at the variance in the data.	"to this, we focus on detecting errors in automatically labelled data."	contrasting
train_36048	"In teaching videos of ""back propagation"", the concept ""gradient descent"" is frequently mentioned when illustrating the optimization detail of back propagation."	"however, ""back propagation"" is unlikely to be mentioned when teaching ""gradient descent""."	contrasting
train_36260	"However, the questions in the dataset are derived from search logs and the answers are crowdsourced."	trivia enthusiasts provided both questions and answers for our dataset.	contrasting
train_36959	Seo et al. (2015) took SAT geometry questions as their benchmark.	the nature of SAT geometry questions restricts the resulting formula's complexity.	contrasting
train_37934	"In order to increase the robustness, they inject noise to input sentences by randomly changing the internal representation of sentences."	"these previous approaches often depend on heuristics to generate synthetic noises, which do not always reflect the real noises on training and inference."	contrasting
train_38048	"In the case of SEMoses, meaning preservation is improved when manual UCCA annotation is used."	"simplicity degrades, possibly due to the larger number of Scenes marked by the human annotator (TUPA tends to under-predict Scenes)."	contrasting
train_38335	"To solve the problems, one needs to know how many numbers to be summed up (e.g. “2 numbers/3 numbers”), and the relation between variables (""the first/second number"")."	an equation system does not encode these information explicitly.	contrasting
train_38336	They belong to the same type of problems asking about the summation of consecutive integers.	their meaning representations are very different in the Dolphin language and in equations.	contrasting
train_38438	Event mentions that occur in the first few paragraphs are more likely to initiate an event chain.	event mentions in later parts of a document may be coreferential with a previously seen event mention but are extremely unlikely to begin a new coreference chain.	contrasting
train_38427	"This is especially true for nominals and pronouns, two common types of entity mentions, where the nearest preceding mention that is also compatible in basic properties (e.g., gender, person and number) is likely to co-refer with the current mention."	coreferential event mentions are rarely from the same sentence ( 10%) and are often sentences apart.	contrasting
train_42156	"The proposed embedding mechanism is related to cross domain embeddings (Bollegala et al., 2015(Bollegala et al., , 2017 and domain-specific embeddings (Xu et al., 2018a,b)."	we require the domain of the domain embeddings must exactly match the domain of the aspect extraction task.	contrasting
train_42284	"We believe this split strikes a good balance between challenge and feasibility: to succeed, a model needs to learn to identify relations in the complex sentence, link them to their arguments, and produce a rephrasing of them."	it is not required to generalize to unseen relations.	contrasting
train_44981	A possible solution is to apply beam search to enlarge the searching space at the first stage.	"in our preliminary experiments, when the beam size is small, the diversity of predicted key facts is low, and also does not help to improve the accuracy."	contrasting
train_45187	"At the same time, the labeled data domain is switched to the source domain, so that both the embedding and decoder domains are abruptly changed."	in ASADA the embedding is gradually adapted from the target domain to jointly embed the source and target (F1).	contrasting
train_45186	"Use of the second text discourages this so that both the encoder and decoder are trained on text from the target domain (enabling use of an expanded, joint vocabulary trained on both source and target) to learn its style and vocabulary."	"the artificial titles will generally be different from the real titles, which may lead to lower summarization performance."	contrasting
train_46112	"Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017)."	"while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspecific words, which we argue is one of the major difficulties caused by domain shift."	contrasting
train_46165	"In the ideal case where the input and output sentences have equal length, the translation will finish k steps after the source sentence finishes, i.e., the tail length is also k. This is consistent with human interpreters who start and stop a few seconds after the speaker starts and stops."	input and output sentences generally have different lengths.	contrasting
train_47682	"Based on the gold parse tree (Figure 1), ""an extensive presence"" is the maximum span of the first coreferring mention in Example 1."	"the corresponding maximum boundary for this same mention is ""an extensive presence, of course in this country"" based on the system parse tree (Figure 2)."	contrasting
train_48599	"As Figure 2 shows, the number of merges that maximizes log-likelihood of our dev set differs from language to language."	"as we will see in Figure 3, tuning this parameter does not substantially influence our results."	contrasting
train_48606	"Speakers prefer shorter dependencies in both production and processing, and average dependency lengths tend to be much shorter than would be expected from randomly-generated parses (Futrell et al., 2015;Liu et al., 2017)."	"there is substantial variability between languages, and it has been proposed, for example, that head-final languages and case-marking languages tend to have longer dependencies on average."	contrasting
train_49680	Deep active learning (DAL) also outperforms SVM and yields comparable performance to DTAL.	the standard deviations in performance of DAL are substantially higher than those of DTAL (e.g.	contrasting
train_53255	"In sum, the statistical results on nodes with multiple parents suggest that they are a frequent phenomenon and that they are not limited to certain kinds of coherence relations."	"as with crossed dependencies, removing certain kinds of coherence relations (elaboration and similarity) can reduce the mean in-degree of nodes and the proportion of nodes with in-degree greater than one."	contrasting
train_53921	"Often the pairs are restricted to noun-modifier pairs, but there are many interesting relations, such as antonymy, that do not occur in noun-modifier pairs."	noun-modifier pairs are interesting due to their high frequency in English.	contrasting
train_55531	In recent work on distributional similarity (Curran 2004; Weeds and Weir 2005) a variety of alternative weighting functions were compared.	"the quality of these weighting functions was evaluated only through their impact on the performance of a particular word similarity measure, as we did in Section 5."	contrasting
train_57993	"Looking just at nominals, we see in the gold corpus that 62% of the dependents in a modification relation have no inherent rationality (this is the case notably for adjectives), whereas this number for idafa is only 18%."	"the dependent of an idafa is irrational 66% of the time, whereas for modification that number is only 16%."	contrasting
train_61332	Ruppenhofer and Rehbein (2012) argue that a frame-based representation of evaluative language is suitable for capturing multi-word evaluative expressions and idioms such as give away the store and sentiment composition.	"apart from using semantic frames for identifying the topics (or targets) of sentiment (Kim and Hovy 2006) and deriving an intensity-based sentiment lexicon (Raksha et al.  2015), little work has been done to show the real effectiveness of this deep representation in practical sentiment analysis systems."	contrasting
train_61362	"For example, earthquake and election are considered bursty."	"non-bursty words are those that appear more consistently throughout documents discussing different topics-use and they, for example."	contrasting
train_62419	"Here, the distance between the anaphor and the antecedent is small: The antecedent of this fact occurs in the preceding clause."	"in Example (31), the antecedent of this question occurs four sentences away from the anaphor sentence."	contrasting
train_66133	"Broad coverage unification-based deep parsers, however, unavoidably have problems meeting the very high accuracy and efficiency requirements needed for real-time dialog."	"parsers based on lexicalized probabilistic context free grammars such those of Collins (1999) and Charniak (1997), which we call shallow parsers 1 , are robust and efficient, but the structural representations obtained with such parsers are insufficient as input for intelligent reasoning."	contrasting
train_66744	"The former attribute greatest weights to very rare context words, some of which seem rather informative ( knock_by, climb_of, see_into), some also appear to be occasional collocates (remand_to, recover_in ) or parsing mistakes (entrust_to, force_of)."	the latter encourage frequent context words.	contrasting
train_69693	"Thus, new terms are actually ranked higher than other terms in TV_ConSem which explains its higher ability to identify new terms in the low range of N TCList ."	its performance drops in the high range of N TCList because the influence of context words diminishes in terms of percentage in the domain lexicon to distinguish terms from non-terms.	contrasting
train_71688	Milne and Witten's work is most related to what we propose here in that we also employ features similar to their relatedness and commonness features.	"we add to this a much richer set of features which are extracted from Web-scale data sources beyond Wikipedia, and we develop a machine learning approach to automatically blend our features using completely automatically generated training data."	contrasting
train_73243	"Under the framework of extractive summarization, it is important to acquire the relationship between sentences and aspects for sentence selection."	"in most existing HDP models, the sentence level is disregarded and we cannot directly get the aspect distribution of sentences."	contrasting
train_73393	These results demonstrate that our training method can bias the joint model towards the desired task.	"as we try different losses, tagging accuracy rarely changes."	contrasting
train_73394	This may due to the fact that our joint decoder is deterministic thus suffers more from error propagation comparing with beam search based or dynamic programming based decoders.	our joint method can also be enhanced with beam search and we leave it to future work.	contrasting
train_74184	Continuous discourse relations are claimed to be easier to process and more expected than other types.	relations that are discontinuous (for example adversatives) would be less expected and more difficult to process.	contrasting
train_74536	"Virtually, in order to project the translation candidates of source elements to target-side-like PAS, we require that a source argument only aligns to a target argument."	the result of bilingual SRL usually does not satisfy this requirement.	contrasting
train_75816	 It can be seen that hierarchical rule testing indeed improves performance: even a single level of merging results in 30- 42% speedup. 	"it is also immediately evident that aside from special cases, the disadvantages overweight the benefits: memory usage and binary size grow exponentially, affecting compilation and grammar loading time as well, and very soon we run into the limits of physical memory."	contrasting
train_76604	The highest score from the top six relations is achieved by taking words exclusively from the second-order secondary object (OBJ2) relation.	relatively few word types are included in the clusters.	contrasting
train_77757	"Various kernels, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel, and dependency tree kernel , have been proposed to solve the relation classification problem."	the methods mentioned above suffer from a lack of sufficient labeled data for training.	contrasting
train_78647	"Both classifiers predict an iambic pentameter most frequently, but, in the case of the Linear Support Vector Machine there are approximately 50 analyses that only appear once (with slight differences between them)."	the number of unique analyses in the CRF results is around 30.	contrasting
train_78645	The program can analyze poems and check if the predominant stress pattern is iambic or anapestic.	"if the input poem's meter is not one of those two, the system forces each line into one of them."	contrasting
train_82667	"Entity linking (EL), mapping entity mentions in texts to a given knowledge base (KB), serves as a fundamental role in many fields, such as question answering (Zhang et al., 2016), semantic search (Blanco et al., 2015), and information extraction (Ji et al., 2015;Ji et al., 2016)."	this task is non-trivial because entity mentions are usually ambiguous.	contrasting
train_82669	"The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks (Pershina et al., 2015), graph pruning (Hoffart et al., 2011), ranking SVMs (Ratinov et al., 2011), or loopy belief propagation (LBP) (Globerson et al., 2016;Ganea and Hofmann, 2017)."	these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation).	contrasting
train_83227	These abstractive models (which will be termed generate-based models hereafter) have the ability to reorder words or rephrase.	none of these models consider explicit word deletion.	contrasting
train_83226	"Due to the difficulty of abstractive sentence compression, there was only a limited number of work on the task (Cohn and Lapata, 2008;Cohn and Lapata, 2013;Galanis and Androutsopoulos, 2011;Coster and Kauchak, 2011a)."	"with the recent success of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable."	contrasting
train_83488	"The reason is that each representation of the individual modality encodes specific knowledge and is complementary, an aspect that can be explored to facilitate understanding on the entire meaning of the content."	this task could be extremely challenging because we need to explore single-modal information deliberately and jointly learn the intrinsic correlation among various modalities.	contrasting
train_85718	"The rule based machine translation demands various kinds of linguistic resources such as morphological analyzer and synthesizer, syntactic parsers, semantic analyzers and so on."	corpus based approaches (as the name implies) require parallel and monolingual corpora.	contrasting
train_85815	"The discrepancy could be due to the fact that the script that the authors shared with us was optimized for PTB, and some of the settings might need to be changed for Text8."	"since we have not found such differences reported in the paper, we used the same setting for both datasets."	contrasting
train_85995	This is a known problem that there is a performance drop when the system moves to new unseen data.	the performance gap can be closed with extending training data (obtained either manually or using some distance learning) and focusing more on domain independent features.	contrasting
train_86256	"(Kaufman et al., 2017) transferred the semantics of the selected reference clips to test clips, which keeps consistent and maintains temporal coherence."	the existing methods fail to take advantage of the local constraints which can extract compressed features for generating complementary syntactic elements.	contrasting
train_86446	c4 Proponents of the death penalty count on its deterring effect as well as and the ultimate elimination of any potential threat.	p5 despite the death penalty there are significantly more homicides in the US than in Germany.	contrasting
train_86442	"To illustrate the means of persuasion, an emphasis on pathos could, for example, lead to a preference for gradually increasing the strength of emotional appeal throughout the text."	"for a logos-oriented strategy, it may be important that the sequence of units coheres locally and globally (which for a pathos-oriented argument may be less relevant, or even detrimental)."	contrasting
train_87829	Our results show that the similarity-based smoothing of frequency estimates significantly improves an already respectable probabilistic PP attachment model.	our hypothesis that a task-specific thesaurus would outperform a generic thesaurus was not borne out by our experiments.	contrasting
train_88967	"Thus, Klementiev and Roth, in common with the two MT approaches described above, carefully control the features used by the perceptron."	"to these approaches, our algorithm discovers latent alignments, essentially selecting those features necessary for good performance on the task at hand."	contrasting
train_89906	"Out of the models they describe, the HMM models are the most expressive models that can compute posterior probabilities using the forward-backward algorithm."	"unlike sequence alignments, there are no ordering constraints in word alignments, and the alignments are many-to-many as opposed to one-to-one."	contrasting
train_93749	Such vast corpora have led to leaps in the performance of many language-based tasks: the concept is that simple models trained on big data can outperform more complex models with fewer examples.	"this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted?"	contrasting
train_94498	"For example, lexical features play an important role in non-pronoun coreference resolution, but are less important for pronoun anaphora resolution."	gender features are not as important in non-pronoun coreference resolution.	contrasting
train_95243	To resolve them to actual dates/time is a non-trivial task.	"the heuristic of employing the document's publication date as the reference works very well in practice e.g. for a document published on 2011- 07-05, SUTime resolves “last Thursday” to 2011- 06-30."	contrasting
train_95732	5 The table also shows that WordNet may not be appropriate for our present verb categorization task.	"it may be suitable for other subtasks in sentiment analysis, particularly polarity classification."	contrasting
train_96372	"Global models can tap into highly discriminative semantic signals (e.g. coreference and entity relatedness) that are unavailable to local methods, and have significantly outperformed the local approach on standard datasets (Guo and Barbosa, 2014;Pershina et al., 2015;Globerson et al., 2016)."	"global approaches are difficult to apply in domains where only short and noisy text is available, as often occurs in social media, questions and answers, and other short web documents."	contrasting
train_96490	"Empirical results show that with only character embedding features, the performance of our model outperforms CNN-CNN-MoT, and is close to LSTM-MoT."	"there is still a big gap between character embedding and word embedding models, which could come from the fact that we use pretrained word embeddings, which helps improve the performance."	contrasting
train_98520	"Existing language modeling (LM) based approaches such as Para2Vec (Le and Mikolov, 2014) rely on deep neural networks to generate vector representations for the paragraph-level."	"these approaches are normally trained with an abundance of utterances to achieve the required performance, which is usually not present in the conversational text domain (Boyanov et al., 2017)."	contrasting
train_99342	"(Apt et al., 1994) have used word-based language modeling techniques for both English and German."	their techniques do not apply to Asian languages where word segmentation remains a significant problem.	contrasting
train_100789	The beam search decoding algorithm is unchanged from traditional phrase-based and factored decoding.	the creation of translation options is extended to include the use of factored templates.	contrasting
train_100792	"The factored template models were retrained with increased maximum phrase length but this made no difference or negatively impacted translation performance, Figure 1."	"using larger phrase lengths over 5 words does not increase translation performance, as had been expected."	contrasting
train_102503	"miniature camera or miniature cameras does not occur in the training data, and so there is no appropriate phrase pair in any system (baseline, inflection, or inflection&compound-splitting)."	our system with compound splitting has learned from split composita that English miniature can be translated as German Miniatur- and gets the correct output.	contrasting
train_102955	"Prop-Bank (Palmer et al., 2005) is the corpus of reference for verb-argument relations."	relations between a verb and its syntactic arguments are only a fraction of the relations present in texts.	contrasting
train_102958	"In (8), both 'create' and 'buy' are done due to the 'country's lack of natural resources'."	"in (9), the analysts 'forecasting' and the company 'saying' do not have as their cause 'planned price cuts'."	contrasting
train_103161	"We train a classification model for each category of entity pair, as suggested in several previous works (Mani et al., 2006;Chambers, 2013)."	"because there are very few examples of timex-timex pairs in the training corpus, it is not possible to train the classification model for these particular pairs."	contrasting
train_103589	"For example, in Figure 1 banana and redberry are not directly connected but they can be reached via pear or raspberry."	"by considering mediate relationships it becomes more difficult to determine the most appropriate category for each food item since most food items are connected to food items of different categories (in Figure 1, there are not only edges between banana and other types of fruits but there is also some edge to some sweet, i.e. chocolate)."	contrasting
train_105343	"There have been recent advances in supervised summarisation mainly with respect to supervised learning using neural networks (for example (Rush et al., 2015;Chopra et al., 2016))."	"due to data size requirements, these systems are constrained to title generation systems and therefore not in the scope of this work."	contrasting
train_105348	"Evaluation against multiple, rather than single reference summaries is generally recognised as leading to fairer, better quality, evaluation: different human summaries appear to be good even though they do not have identical content (Nenkova and Passonneau, 2004)."	"averaging ROUGE scores across multiple summaries, as is standard practice, makes a perfect 100% score unattainable, even for abstractive systems."	contrasting
train_105459	"As discussed in Section 1, there have been studies to reduce the time complexity of spell correction by various methods."	the recent work of de Amorim and Zampieri (2013) is closest to our work in terms of the goal of the study.	contrasting
train_106441	The problem of abbreviation processing has attracted relatively little attention in NLP field.	technical documents use a lot of abbreviations to represent domainspecific knowledge.	contrasting
train_107123	"Many of these algorithms are, in principle, language-independent."	"when applying these algorithms to languages such as Chinese and Japanese, we must deal with certain language-specific issues: for example, should we build a character-based model or a word-based model?"	contrasting
train_107128	"Our HMM classifier for English uses a set of word-features to indicate whether a word contains all capitalized letters, only digits, or capitalized letters and period, as described in (Bikel et al., 1999)."	chinese does not have capitalization.	contrasting
train_107125	"On one hand, the word-based model is attractive since it allows the system to inspect a larger window of text, which may lead to more informative decisions."	a word segmenter is not error-prone and these errors may propagate and result in errors in NE recognition.	contrasting
train_107235	Note that the distance in words from the previous paragraph boundary (Dist w ) is a good indicator for a paragraph break in the English news domain.	this feature is less useful for the other two languages.	contrasting
train_115136	"As α increases, the utterances increase in complexity, as does the success rate."	"when α approaches 1, the utterances are too complex and the success rate decreases."	contrasting
train_115887	"It is important to note that this paper should be viewed as a case study where we illustrate the results of the application of what we believe to be a good algorithm for dealing with incomplete or incorrect lexical entries-namely, the combination of error mining and LA."	our method is general enough to be applied to other large-scale grammars and languages.	contrasting
train_115876	"If there is no analysis spanning the whole sentence, the parser finds all parses for each substring and returns what it considers to be the best sequence of non-overlapping parses."	"in the context of this experiment, a sentence will be considered successfully parsed only if it receives a full-span analysis."	contrasting
train_116164	"For these unfamiliar topics, users possibly search the web ""after"" they read the news articles and express their diverse opinions in the blog."	"on the topics like ""insurance rate"" or ""consolidation loans,"" Blog is similar to the queries while News is not."	contrasting
train_116165	News or blog articles consist of completed sentences and paragraphs which would contain plenty of meaningful bigrams.	search queries consist of keywords - relatively discrete and regardless of order.	contrasting
train_120599	"Since this case was aligned to no-case of the case frame "" -2 (hit-2),"" the input ga-case was alternated with no-case."	"the cases of the other arguments "" (bat-de)"" and "" (head-wo)"" were output as they were in the passive sentence."	contrasting
train_120737	"Aligned with previous study in marketing science (Moon and Quelch, 2006), an informative set of features related to Starbucks store decorations showed up in our model: ""store"", ""restroom"", ""public"", ""bathroom"", and ""spacious""."	these features stopped to show up on the list of Dunkin' Donuts.	contrasting
train_121104	"The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999;Fung and Cheung, 2004;Gaussier et al., 2004) as dimensions of the space."	"there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons!"	contrasting
train_122599	"Specifically, in 51–54% of the erroneous cases, a reason sentence is misclassified as NONE."	23–30% of the cases are concerned with assigning a reason label to a NONE sentence.	contrasting
train_122760	"Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data."	we directly incorporate the likelihood of partial annotation into the objective function.	contrasting
train_122756	Liu and Zhang (2012)  study domain adaptation using an unsupervised self-training method.	"to their work, we make use of not only unlabeled data, but also leverage any free annotation to achieve better results for domain adaptation."	contrasting
train_123143	Deng and Wiebe (2014) show that such inferences may be exploited to significantly improve explicit sentiment analysis systems.	"to achieve its results, the system developed by Deng and Wiebe (2014) requires that all instances of +/-effect events in the corpus be manually provided as input."	contrasting
train_124587	The corresponding grammars for recognizing and producing graphs are more flexible and powerful than tree grammars.	"because of their high complexity, graph grammars have not been widely used in NLP."	contrasting
train_124588	"Recently, along with progress on graph-based meaning representation, hyperedge replacement grammars (HRG) (Drewes et al., 1997) have been revisited, explored and used for semantic-based machine translation (Jones et al., 2012)."	"the translation process is rather complex and the resources it relies on, namely abstract meaning corpora, are limited as well."	contrasting
train_124747	"The ever increasing user generated content has always been motivation for sentiment analysis research, but majority of work has been done for English Language."	"in recent years, there has been emergence of increasing amount of text in Hindi on electronic sources but NLP Frameworks to process this data is sadly miniscule."	contrasting
train_124759	"In contrast, conventional two-stage approach Pang and Lee (2004), which first generate candidate subjective sentences using min-cut and then selects top subjective sentences within budget to generate a summary, have less computational complexity than joint models."	two-stage approaches are suboptimal for text summarization.	contrasting
train_124760	"For example, when we select subjective sentences first, the sentiment as well information content may become redundant for a particular aspect."	"when we extract sentences first, an important subjective sentence may fail to be selected, simply because it is long."	contrasting
train_124758	Joint models of relevance and subjectivity have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes.	"conventional two-stage approach Pang and Lee (2004), which first generate candidate subjective sentences using min-cut and then selects top subjective sentences within budget to generate a summary, have less computational complexity than joint models."	contrasting
train_124773	"While the targets in aspect-based sentiment analysis are often entity targets, they are mainly product aspects, which are a predefined set."	3 the target in the entity/event-level task may be any noun or verb.	contrasting
train_124774	"Previously, we also propose a set of sentiment inference rules and develop a rule-based system to infer sentiments ."	the rule-based system requires all information regarding explicit sentiments and +/-effect events to be provided as oracle information by manual annotations.	contrasting
train_125402	"A fundamental issue in opinion mining is to search a corpus for opinion units, each of which typically comprises the evaluation by an author for a target object from an aspect, such as ""This hotel is in a good location""."	"few attempts have been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as ""for traveling with small kids""."	contrasting
train_126809	"Vector space models of language like the ones presented in (Collobert et al., 2011b;Mikolov et al., 2013;Pennington et al., 2014) create good representations for the individual words of a language."	"the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences."	contrasting
train_127177	"Initially, manual evaluation was carried out, where human judges were tasked to assess the quality of automatically generated summaries."	"in an effort to make evaluation more scaleable, the automatic ROUGE 1 measure (Lin, 2004b) was introduced in DUC-2004."	contrasting
train_128528	"However, in that work, categories and their instances were known a-priori."	"in case of SICTF, both categories and relations are to be induced."	contrasting
train_128783	Most existing works focus on either generic subjective expression or aspect expression extraction.	"in opinion mining, it is often desirable to mine the aspect specific opinion expressions (or aspectsentiment phrases) containing both the aspect and the opinion."	contrasting
train_129997	"Both word-level and character-level models perform comparably well when predicting the predicate, reaching an accuracy of around 80% (Table  3)."	"the word-level model has considerable difficulty generalizing to unseen entities, and is only able to predict 45% of the entities accurately from the mixed set."	contrasting
train_130966	"From the comparison, it shows that NL strategy yields better performance than TD strategy, since the true labels inferred by Investment are actually wrong for many instances."	"as discussed in Sec. 4.4, our method introduces context-awareness to true label discovery, while the inferred true label guides the relation extractor achieving the best performance."	contrasting
train_131426	"Thus, it may seem reasonable to apply ADA-GRAD to optimize online topic models."	aDaGRaD is not suitable for online topic models (Section 1).	contrasting
train_131902	"An exception is the Dolphin18K dataset (Huang et al., 2016) which contains 18,000+ problems."	this dataset has not been made publicly available so far.	contrasting
train_135523	Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax.	"their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention."	contrasting
train_135831	Greydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher.	this approach needs supervision compared to our approach which uses a pre-trained neural LM.	contrasting
train_135827	"Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010)."	using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.	contrasting
train_135939	"Video content on social media platforms constitutes a major part of the communication between people, as it allows everyone to share their stories."	"if someone is unable to consume video, either due to a disability or network bandwidth, this severely limits their participation and communication."	contrasting
train_136566	"While the basic senses of tight-e.g., being physically close together or firmly attached-conflict with the more abstract budget, the metaphoric use as meaning limited can be readily understood."	the use of penumbra in (2) is more creative and novel.	contrasting
train_136578	"Another artifact of using Wikipedia as the background corpus can be seen on the left: try is only annotated as metaphoric in infinitive-compounds that are decidedly conventionalized (e.g., ""trying to look""), yet it appears comparatively seldom in Wikipedia."	"we chose to use Wikipedia instead of the BNC in order to have an out-ofdomain comparison with a more contemporary, larger background corpus."	contrasting
train_136571	"On the one hand, this is a sensible approach because generally the context of a word determines its metaphoricity (and indeed, its novelty in case of metaphoric use)."	such annotations lack the flexibility and ease of use of tokenbased annotations for which the context is not defined a priori.	contrasting
train_137866	"The DFS approach is an applicable linearization of trees since a recursive traversal, which starts at the root and explores all outgoing edges, is guaranteed to visit all of the graph's nodes."	"dFS linearization is not directly applicable to SdP, as its graphs often consist of several non-connected components."	contrasting
train_140650	"Zero Update achieves competitive performances in Case 1 (90.9 for IMDB) and Case 2 (86.7 for Kitchen), as tasks from these two cases all belong to sentiment datasets of different cardinalities or domains that contain rich semantic correlations with each other."	"the result for IMDB in Case 3 is only 74.2, as sentiment shares less relevance with topic and question type, thus leading to poor transferring performances."	contrasting
train_140978	"Graham et al. (2017) compare crowdsourced to ""expert"" ratings on machine translations from WMT 2012, concluding that, with proper quality control, ""machine translation systems can indeed be evaluated by the crowd alone."""	"it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation."	contrasting
train_141137	They empirically compare their efficiency on two translation tasks and find that some strategies in wide use are not necessarily optimal for accuracy and convergence-wise.	"to the work described here however, one of the sorting strategies produced best results, though no comparison was made with the original ordering of examples."	contrasting
train_141163	Neural models have shown several state-ofthe-art performances on Semantic Role Labeling (SRL).	the neural models require an immense amount of semantic-role corpora and are thus not well suited for lowresource languages or domains.	contrasting
train_143717	"From the figure, it can be found that while there is no mentioned item in the dialog, the baseline and the one only with knowledge perform the worst."	the two models with dialog incorporation perform significantly better.	contrasting
train_145226	They also showed that baseline classifiers could achieve moderate performance on the task.	we focus on local acceptability of arguments with restricted options for reason selection.	contrasting
train_146121	We find that combining the sparse global gradient with the dense local gradient improves convergence.	adding local information means that nodes' parameters will diverge over time.	contrasting
train_146151	We observe no significant trend of favoring one branching direction over the other.	"after training with the language modeling objective, PaLM-U shows a clear right-skewness more than it should: it produces much more right-branching structures than the gold annotation."	contrasting
train_148653	"In their work (K Sarma et al., 2018), the authors demonstrate the effectiveness of KCCA based embeddings via classification experiments."	"in order to verify that the word level adaptation performed by the KCCA step can capture relevant domain semantics, we perform the following experiment."	contrasting
train_149403	The UMT framework makes it possible to apply neural models to tasks where limited human labeled data is available.	"in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same."	contrasting
train_149711	"The previous cross-lingual settings were only for European languages, which share similar alphabets."	many languages use non-Latin orthography.	contrasting
train_150141	Many methods of term extraction have been discussed in terms of their accuracy on huge corpora.	"when we try to apply various methods that derive from frequency to a small corpus, we may not be able to achieve sufficient accuracy because of the shortage of statistical information on frequency."	contrasting
train_150994	All possible forms of terminology translations can be comprehensively mined after character-based string frequency estimation.	there are many irrelevant items and redundancy noises formed in the process of mining.	contrasting
train_151306	"Generally, trigram model is used as acoustic model in order to improve the recognition accuracy."	"monophone model is used in this paper, since the proposed estimation method needs recognition error (and IRDR)."	contrasting
train_152044	A parallel corpus that has similar statistical characteristics to the target domain should yield a more efficient translation model.	domainmismatched training data might reduce the translation model's performance.	contrasting
train_153644	6 Additional Experiments We believe that the entities in a document that cooccur with a target entity are important clues for disambiguating entities.	"while we had ready access to named entity recognizers in English, we did not have NER capability in all of the languages of our collection."	contrasting
train_154351	"The widely used knowledge resource in such methods is WordNet (Fellbaum, 1998)."	"wordNet-based wSD methods usually achieved lower performance compared to supervised methods, mainly due the fact that the lexical and semantic knowledge contained in wordNet is not sufficient for wSD."	contrasting
train_156701	"Takamura et al., (2007) used the Potts model for extracting semantic orientation of phrases (pair of adjective and a noun): positive, negative or neutral."	"to that, we have used the Potts model for identifying the emotional class (es) of a word."	contrasting
train_156733	"When the number of instances is large enough, the statistical model will effectively incorporate these entity type constraints as long as entity types are extracted as features."	"in active learning, even with suitable training examples, we will select and present to the user some instances violating these constraints."	contrasting
train_156855	Table 4 shows the Chinese SRL results after adding the N-best dependency parsing related features.	"it is not surprising that SRL can get better performance when N > 1, because the larger N, a more accurate dependency parsing results can be likely obtained."	contrasting
train_159242	"For instance, since concrete concepts are perception revealing, they would benefit from a strong emphasis on the perception embedding."	emotion-revealing word groups such as abstract concepts would be opposite.	contrasting
train_159245	"In the SimLex-999 dataset which focuses on the word similarity, the cognition (lexical relation) and the sentiment modules turned out to be important."	"in the WordSim-353 dataset which focuses on the word relatedness, both linear context and syntactic context are turned out to be critical."	contrasting
train_159954	"After predicting scores for every concept and selecting the highest scoring subgraph with the ILP, we use this subgraph as the summary concept map."	this graph might contain multiple edges between certain concepts.	contrasting
train_160161	The basic framework of Japanese normalization is quite similar to that of English normalization.	"the problem is more complicated in Japanese normalization because Japanese words are not segmented using explicit delimiters, so we have to estimate word segmentation simultaneously in the decoding step."	contrasting
train_160166	"For example, Sasaki et al. (2013) proposed a character-level sequential labeling method for normalization."	it handles only one-to-one character transformations and does not take the word-level context into account.	contrasting
train_160169	"In a previous study (Han and Baldwin, 2011), the nodes were assumed to be single words."	our method assigns a node to not only single words but also short phrases (See. 3.1).	contrasting
train_163974	"On the other hand, the surface-based grammar misses the ubiquitination event involving ""TRAF6"" (and the negative regulation of this ubiquitination), because the last two tokens of the sentence are not explicitly handled by the rules."	syntax-based grammars assume that a syntactic parser is available and produces robust output.	contrasting
train_164351	"In Hiragana TIMES, NE abstraction methods were effective."	"in BTEC, NE abstraction methods were not effective because most of the sentences are typical dialogues that are easy to translate with the baseline, e.g., ""I must arrive in Tokyo by tomorrow morning ."""	contrasting
train_165199	How-to websites like wikiHow 1 and eHow 2 help how to do things in daily life.	the quantity of wikiHow and the quality of eHow cannot compete with those of wikipedia.	contrasting
train_166232	"At the end of the validation process, a total of 2,290 personalities constitute our speaker dictionary (Table 1 shows the detail)."	"it has to be kept in mind that, despite our efforts to reduce it, such dictionaries are by nature greatly imbalanced."	contrasting
train_168928	"The proposed method was evaluated on a semantic similarity task (WS353 dataset), where the multimodal model was found to yield higher performance compared to the textual one."	the best performance was moderate (0.32 correlation coefficient).	contrasting
train_169493	"Then, they asked a single member in their research group to rate the credibility of those 1000 articles on a 5 Likert scale."	"this corpus is in English, and was rated by a single person only, which makes it inapplicable in our case."	contrasting
train_169497	"For example, judging the credibility of a tweet using only its text is not enough; instead one may additionally rely on the author background, expertise and external web references."	a blog post might contain enough cues in its text to assess its credibility.	contrasting
train_172146	"The annotation consists in enriching the documents with summaries, keywords or participant names in order to satisfy the complex queries elaborated by INA customers or researchers within media databases."	"due to the increasing number of documents and the limited number of annotators, many documents remain undocumented or only partly documented."	contrasting
train_174056	"The formant tracker PRAAT showed systematic errors with male speakers, while SNACK and ASSP performed better on male than on female speakers."	when comparing the average RMSE error value for female speakers in PRAAT (194Hz) with male speakers in SNACK/ASSP (234/177Hz) we can see that the performance was in the same range (underlined values in Table 2).	contrasting
train_183174	A common way to obtain such a measure is to use an n-gram language model trained on a large background corpus.	"language models tend to assign smaller probabilities to longer sentences; therefore they favor short sentences, but not necessarily the most appropriate compressions."	contrasting
train_183422	"To our knowledge, Baron and Freedman (2008) reported the only previous results on the ACE2008 data set."	"they only gave gold results for English, and clustered the entire evaluation corpus (test+development)."	contrasting
train_184256	One can imagine automatically mining image/caption data (like that in Figure 1) to train object recognition systems.	"in order to do so reliably, one must know whether the ""car"" actually appears or not."	contrasting
train_184651	"Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications."	"it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). "	contrasting
train_185171	The reason f f ac outperforms f SC is that f SC primarily controls for over-coverage of any element not in the subset via the Î± saturation hyper-parameter.	it does not ensure that every non-selected element has good representation in the subset.	contrasting
train_188332	"Meanwhile, some improved linear analysis methods were proposed for encoding documents with a reliable similarity information (Yih et al., 2011; Chang et al., 2013)."	all those works for document representation paid little attention to the variability of intra-topic documents.	contrasting
train_189189	"Cosine similarity is unable to differentiate between these two cases, assigning a high score to both these pairs, causing both of them to be labeled positive."	balAPinc with sparse representations teases them apart by giving a high score to the first pair and a low score to the second.	contrasting
train_191959	A solution to this task will represent a substantial step towards automatic warrant reconstruction.	we present experiments with several neural attention and language models which reveal that current approaches based on the words and phrases in arguments and warrants do not suffice to solve the task.	contrasting
train_192986	A naive approach to decoding with constraints would be to use a large beam size and select from the set of complete hypotheses the best that satisfies all constraints.	this is infeasible in practice because it would require searching a potentially very large space to ensure that even hypotheses with low model score due to the inclusion of a constraint would be part of the set of outputs.	contrasting
train_194737	"Recently, Lample et al. (2018a); Artetxe et al. (2018); Lample et al.  (2018b) make encouraging progress on unsupervised NMT structure mainly based on initialization, denoising language modeling, and back-translation."	all these unsupervised models are based on the back-translation learning framework to generate pseudo language pairs for training.	contrasting
train_195659	"As mentioned in Section 2, BSO relies on raw
score function to eliminate label bias effects."	without using locally-normalized score does not mean that we should stop using the probabilistic value function.	contrasting
train_196281	"The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules."	"language is continuously evolving, and there are exceptions to most grammar rules we know."	contrasting
train_197665	"Indeed, for dependency parsing ensemble learning has shown to improve accuracy (Surdeanu and Manning, 2010;Kuncoro et al., 2016; Che et al., 2018)."	ensembles are computationally demanding due to the large number of participating models.	contrasting
train_197667	For all languages in the multi-task setting the gain from including +SGLD in +ADV is statistically significant.	the gain decays as the amount of training data increases: from 3.8% on Irish (ga) to 1.1% on Persian (fa).	contrasting
train_198544	An embedded question is not a query directed toward the addressee.	"in the case of questions, as shown in (8), a sense of query is directed to the addressee:"	contrasting
train_199298	Yoshimoto et al. (2000) proposed a formal treatment of Minami's hierarchy under the framework of HPSG and Discourse Representation Theory.	"their analysis focused only on the interpretation of temporal relation between the events described in subordinate and matrix clauses, and it simply reformulates Minami's hierarchy as 'hierarchy feature' which can take value A, B, C, and D. Minami's (and Yoshimoto's) work seems to capture a surface ranking for such combination with using somewhat tentative levels, and there is no answer to the question: Which linguistic information is the origin of HCF (and his level of hierarchy)?"	contrasting
train_199299	"When the headed structure is complementhead structure or pseudo-lexical-rule structure, the semantic head is identical to the syntactic one."	the semantic head is the modifier when the headed structure is modifier-head structure.	contrasting
train_199301	ADJCNT feature in nagara specifies that the adjacent VP does not have tense.	"level B clauses must contain tense markers, so the ADJCNT feature in node is specified as such."	contrasting
train_202549	"Verb classifications have been used to support a number of practical tasks and applications, such as parsing, information extraction, question-answering, and machine translation."	large-scale exploitation of verb classes in real-world or domain-sensitive tasks has not been possible because existing manually built classifications are incomprehensive.	contrasting
train_203066	"The admission of scramblability shown above seems to be problematic for the surfacecompositional analysis of epistemic verb construction in Section 4 since that postulates ""control""."	the argument is not decisive.	contrasting
train_203653	"In terms of computational implementation, there still are more issues for our analysis to be resolved."	we can observe that the grammar implemented in the LKB system appears to be feasible enough to extend to more complex data in a process of building a comprehensive KRG.	contrasting
train_203708	Syntactic class membership is a way of labeling syntactic roles in a PS-tree because a PS-tree does not and cannot specify the types of syntactic links existing between two items in a natural and explicit way.	"to PS-tree, class membership is not specified in a D-tree."	contrasting
train_203707	A D-tree contains only terminal nodes; no abstract representation of syntactic groupings is used.	"a PS-tree contains both terminal and non-terminal nodes; most nodes are, however, non-terminals."	contrasting
train_203706	"A D-tree shows a relational characteristic of the syntactic representation in the form of hierarchical links between items, i.e., which items are related to which other items and in which way."	a phrase-structure grammar uses a phrase-structure tree (PS-tree) to describe the groupings of words into the so-called constituents at different levels of sentence construction.	contrasting
train_204024	Most of the dialects in the corpus have been transcribed with one transcription only; that of the standard orthography of that country.	"the dialects of Norway and of the two areas of Ã„lvdalen and Gotland (considered to have dialects very far from Standard Swedish) have also been transcribed with a script that is phonetic-like (the Norwegian version is compliant with the transcription used in Papazian and Helleland (2005), the Ã–vdalian one is the one recommended by the Ã–vdalian Language Council, while the Gotland one has been used by the Swedia 2000 project)."	contrasting
train_205668	"Although both datasets contain multiple participants, they differ in the nature of their content; thus, we found problems applying the DA set from Forsyth (2007) directly to the library chat forums."	we observed that there is overlap between the two sets of dialogue acts	contrasting
train_206365	"If this head is assumed to be present, we will need to consider the following structure as a possible MIC, in addition to those in (18) (and 8 25, then the presence of an agent is necessarily implied."	"if VOICE is not part of its MIC, then the presence of an agent is not required."	contrasting
train_206589	"Three topic features made from each topic model are added to the normal features, and the extended feature are used in learning for WSD."	"regarding the topic features made from the source domain, this topic features have the weight because this topic features reduce the accuracy of WSD."	contrasting
train_208504	"These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size."	"in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs."	contrasting
train_209513	Learners of Japanese language from kanji background countries benefit substantially from kanji characters commonly used in both Japanese and Chinese when they read Japanese sentences or documents.	it is more challenging for them to read and learn various Japanese functional expressions with varied meanings and usages.	contrasting
train_211053	"The overall results demonstrated that it did not, which may appear to challenge the analysis of Ascrambling as movement to SpTP, the high subject position."	"if we adopt the distinction between low and high subject positions and assume that HA and PC are low subject properties, the apparent challenge can be dismissed."	contrasting
train_211803	"For metaphors, the GV based WSD method is better than the one using baseline WSD, within the 1% statistical error threshold (pvalue 0.01)."	for expanded senses we cannot support within 5% statistical error that GV based WSD performs better than baseline (p-value 0.20).	contrasting
train_212689	"For verbs taking abstract arguments without definitive visual features, the classifier can often learn to disregard the visual data."	"for verbs taking physical arguments (such as food, animals, or people), the classifier can make accurate predictions using the nouns' visual properties."	contrasting
train_214334	"This can be inferred from high accuracy of stateof-the-art POS tagging not only for English, but also most other languages such as Arabic, which reaches 97% for Arabic and English being at 97.32% (Gadde et al., 2011)."	"the performance of standard POS taggers for English is severely degraded on Tweets due to their noisiness and sparseness (Ritter et al., 2011)."	contrasting
train_215100	"Among the three methods, SG seems to integrate well with the bag-of-word approach in which its combinations outperform the baseline on three datasets MSRvid, OnWN, and SMTnews."	none of these settings equals to the baseline in overall result.	contrasting
train_215135	Another characteristic of textual instructions is that they often omit the direct object.	"as the results showed, the usage of objects reduces the generation of false positives."	contrasting
train_215345	"Therefore, finding words that end with the Arabic letter Taa' Marbootah such as (Feeling cold), (Feeling sleepy), or (Feeling hungry) in the tweets of a user indicates that the writer is a female."	"if the writer is a male, the adjec-tives used in his tweets would not have that letter."	contrasting
train_215971	"Especially, sentiment analysis has received a considerable attention from both industry and research community."	only a few research examples exist for Azerbaijani language.	contrasting
train_216327	Mueller and Thyagarajan (2016) report that their architecture does not perform well with active-passive equivalence.	"as shown in Table 4, our architecture performs slightly better than the LSTM based architecture."	contrasting
train_216326	"The Siamese neural network with GRU was tested with a cyclical learning rate (Smith, 2015), which has the advantage of forcing the model to find another local minimum if the current minimum is not robust and makes the model generalize better to unseen data. "	neither cyclical learning rate nor reducing learning rate on plateau increased the performance further.	contrasting
train_217826	"Sentiment analysis tasks are formulated as determining whether a piece of text is positive, negative, or neutral, or determining from text the speaker’s opinion and the target of the opinion (the entity towards which opinion is expressed)."	"in stance detection, systems are to determine favorability towards a given (prechosen) target of interest."	contrasting
train_218069	"There are only 3 EDs describing the event, each of which uses the event verb ""choose""."	"we find that ""choose"" is used in less than 10% of the event mentions in InScript."	contrasting
train_218588	(2017) introduced a different set of parameters for each edge-type in their LSTM-based approach for relation extraction.	"to these works, our mTreeLSTM model incorporates relation information via a multiplicative mechanism, which we have shown is more effective and uses less parameters."	contrasting
train_218584	"On one hand, the AMR tree structure can be used directly with the TreeLSTM architecture described in Section 2, in which only node information is utilized to encode sentences into certain fixed-length embedding vectors."	"since AMR provides rich information about semantic relations between nodes, the mTreeLSTM architecture is more applicable due to its capability of modeling edges in the tree."	contrasting
train_219497	"This could not be applied to the other two disfluency detectors, so we cannot test those differences for significance."	"we note that the 20 samples for our disfluency detector ranged in accuracy from 83.3-84.6, so we doubt that 0.2% mean improvement over the Qian and Liu (2013) result is meaningful."	contrasting
train_220098	"Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster."	"recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness."	contrasting
train_220103	"Without further information, the texts might discuss any of a number of possible topics."	"if the sense of cold is that of cold ischemia, then the most probable topics would be those related to organ transplantation."	contrasting
train_220114	Another approach is inspired by the observation that each local context token is treated equally in terms of its contribution to the sense.	our intuition is that certain tokens are more indicative than others.	contrasting
train_220938	"If (1) is satisfied, that is, there are multiple KBs and enough entries in them that can be aligned, then the proposed method can be applied."	"the performance of this method highly depends on (2), the quality of features and how well the indirect supervision examples approximate the text at prediction time."	contrasting
train_221875	"For example, given the Sentence #2006, the bottom-up and the in-order parsers give both correct results."	"the top-down parser makes an incorrect decision to generate an S, leading to subsequent incorrect decisions on VP to complete S. "	contrasting
train_221871	"This is likely because the bottomup parser takes advantages of rich local features from partially-built trees, which are useful for parsing short sentences."	these local structures are can be insufficient for parsing long sentences due to error propagation.	contrasting
train_221868	"In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003)."	"such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages."	contrasting
train_223494	"For instance, the model generates tokens approximately following the dependency parse when we used the SYN order for the machine translation task."	"the model trained using the RF order learns to first produce verbs and nouns first, before filling up the sequence with remaining functional words."	contrasting
train_223498	Our model was trained with either pre-defined orders or searched adaptive orders.	"to conventional neural autoregressive models that often generate from left to right, our model can flexibly generate a sequence following an arbitrary order."	contrasting
train_266	"If two tree sets adjoin into the same tree, the two site-segments must be simultaneously removable."	"the two sitesegments must be disjoint, or one must contain the other."	entailment
train_428	"Essentially, the probability of a given segment set permuting in different string contexts diminishes as the number of co-occurring segments decreases."	"for a given string pair, the greater the segment overlap between them (relative to the overall string lengths), the lower the probability that those segments are going to occur in different orderings."	entailment
train_2460	"Since the memory-based learning is an efficient method to handle exceptional cases of the rules, it supports the rules by making decisions only for the exceptions of the rules."	the memory-based learning enhances the rules by efficiently handling the exceptional cases of the rules.	entailment
train_2767	"In this section, we examine which anaphoricity features are important in order to gain linguistic insights into the problem."	we measure the informativeness of a feature by computing its information gain (see p.22 of Quinlan (1993) for details) on our three data sets for training anaphoricity classifiers.	entailment
train_2755	"Based on this definition of conservativeness, we can construct an anaphoricity model parameterized by cr."	the parametric model maps a given value of cr to the anaphoricity classifier trained with this cost ratio.	entailment
train_3390	"Then kmeans clustering was conducted on latent semantic space transformed from context vector matrix, using normalized Euclidean distance."	context vectors were reduced to 100 dimensions using SVD.	entailment
train_4408	We aim to jointly maximize the intra-segmental similarity and minimize the similarity between different segments.	we want to find the segmentation with a maximally homogeneous set of segments that are also maxi-mally different from each other.	entailment
train_6387	Many kernels have been proposed and applied to the NLP study.	Haussler (1999) proposed the well-known convolution kernels for a discrete structure.	entailment
train_9110	"When the transition from previous word to current word is lowprobability, from the parser's perspective, the surprisal is high and the psycholinguistic claim is that behavioral measures should register increased cognitive difficulty."	rare parser actions are cognitively costly.	entailment
train_9325	The basic premise of the heuristic is that the text currently being read provides an approximation of the current user interest.	"as a user reads a sentence, it potentially represents a fine-grained information need."	entailment
train_10124	"The first set of constraints specify the most frequent head tag, attachment direction, and distance combinations for each child tag."	"we select oracle constraints of the type (parent-CPOS,child-CPOS,direction,distance)."	entailment
train_10980	It furthermore led to better translation scores for Chinese and Arabic documents with different genres.	it improved the translation scores of the tail documents by 0.5-1.4 points measured by the combined metric of (TER-BLEU)/2.	entailment
train_12953	"While the feature based approach may not be able to fully utilize the syntactic information in a parse tree, an alternative to the feature-based methods, tree kernel methods (Haussler, 1999) have been proposed to implicitly explore features in a high dimensional space by employing a kernel function to calculate the similarity between two objects directly."	"the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004)."	entailment
train_13550	"In this paper, we present another rational model of eye movement control in reading that, like Reichle and Laurent, makes predictions for fixation durations and locations, but which focuses instead on the dynamics of word identification at the core of the task of reading."	our model identifies the words in a sentence by performing Bayesian inference combining noisy input from a realistic visual model with a language model that takes context into account.	entailment
train_13562	"In Simulation 2, we perform a more direct test of the idea that making regressions is a rational response to the problem of confidence falling about previous regions using optimization techniques."	"we search for optimal policy parameter values (α,β) for three different measures of performance, each representing a different tradeoff between the importance of accuracy and speed."	entailment
train_13559	"We implemented our model with wFSAs using the OpenFST library (Allauzen, Riley, Schalkwyk, Skut, & Mohri, 2007)."	"we constructed the model's initial belief state (i.e., the distribution over sentences given by its language model) by directly translating the bigram model into a wFSA in the log semiring."	entailment
train_13551	"The most recent version, E-Z Reader 10 (Reichle, Warren, & McConnell, 2009), has a new component that can produce longer between-word regressions."	"the model includes a flag for postlexical integration failure, that -when triggered -will instruct the model to produce a between-word regression to the site of the failure."	entailment
train_14385	We have carried out experimentation ranging τw between 0.0 and 1.0 and found that this value can be used to solve the above mentioned undesired effect for the MEAN CM.	"varying the value of τw we can stretch the interval in which the transition between the fully automatic SMT system and the conventional IMT system is produced, allowing us to obtain smother transitions."	entailment
train_14383	We propose an alternative scenario where not all the source sentences are interactively translated by the user.	"only those source sentences whose initial fully automatic translation are incorrect, according to some quality criterion, are interactively translated."	entailment
train_15034	"Although the metric shows an improved correlation with human judgment of translation quality (Callison-Burch et al., 2007;GimÃ©nez and MÃ rquez, 2007;Callison-Burch et al., 2008;GimÃ©nez and MÃ rquez, 2008), it is not commonly used in large-scale MT evaluation campaigns, perhaps due to its high time cost and/or the difficulty of interpreting its score because of its highly complex combination of many heterogenous types of features."	note that the feature based representations of semantic roles used in these aggregate metrics do not actually capture the structural predicate-argument relations.	entailment
train_15201	Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances.	we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.	entailment
train_17742	"The third feature set (long-range, LR) is an extension of SR which also includes longer-distance features."	this feature set additionally includes all unigram bag features up to a distance d = 9.	entailment
train_21636	"Finally, in order to understand whether the noisy-OR Bayesian network is useful for generalizing across patterns into latent events, we added a baseline that extracts all patterns from the test collection following the same COLLECTIONTOPAT-TERNS algorithm (including the application of the linguistically motivated heuristics), and then produces a headline straightaway from the most frequent pattern extracted."	"the only difference with respect to HEADY is that in this case no generalization through the Noisy-OR network is carried out, and that headlines are generated from patterns directly observed in the test news collections."	entailment
train_21633	"Our final goal in this research is to build a headline generation system that, given a news collection, is able to describe it with the most compact, objective and informative headline."	"we want the system to be able to:  Generate headlines in an open-domain, unsupervised way, so that it does not need to rely on training data which is expensive to produce."	entailment
train_21819	"Unlike previous work (Knight and Marcu, 2000;Galley and McKeown, 2007), we do not produce a new parse tree,  but focus on learning to identify the proper set of constituents to be removed."	"when a node is dropped from the tree, all words it subsumes will be deleted from the sentence."	entailment
train_21817	Then we conduct simple query expansion based on the title of the topic and cross-document coreference resolution.	we first add the words from the topic title to the query.	entailment
train_23160	"We infer the function label of each node in the PS trees based on the morphological features, syntactic environment, and dash-feature (if exist), using deterministic grammar rules (Glinert, 1989)."	"we compare each edge with a set of templates, and, once finding a template that fits the morphological and syntactic profile of an edge, we assign functions to all daughters."	entailment
train_23903	"First, we operate over a heterogeneous graph to model semantic relations and opinion relations into a unified model."	"our heterogeneous graph is composed of three subgraphs which model different relation types and candidates, as shown in Figure 1."	entailment
train_23943	We develop a rich set of context-aware posterior constraints for sentence-level sentiment analysis by exploiting lexical and discourse knowledge.	 we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations that indicate sentiment coherence or sentiment changes both within and across sentences.	entailment
train_23951	This confirms that encoding lexical and discourse knowledge as posterior constraints allows the featurebased model to gain additional learning power for sentence-level sentiment prediction.	incorporating discourse constraints leads to consistent improvements to our model.	entailment
train_26482	We compare different summarisation algorithms based on their ability to provide a good label to a given topic.	we investigate the use of lexical features by comparing three different wellknown multi-document summarisation algorithms against the top-n topic terms baseline.	entailment
train_26595	"To model the semantic consistency, this paper proposes a local subspace based method."	"given sufficient training instances, our method first models each relation type as a linear subspace spanned by its training instances."	entailment
train_26602	"In this paper, we select informative training instances by seeking a most compact subset of instances which can span the whole subspace of a relation type."	all instances of ith type can be represented as a linear combination of these selected instances.	entailment
train_26594	"In this paper, we propose a new distant supervision method, called Semantic Consistency, which can identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region."	"we propose a semantic consistency model, which first models the local subspace around an instance as a sparse linear combination of training instances, then estimate the semantic consistency by exploiting the characteristics of the local subspace."	entailment
train_27565	"To test this systematically, we calculated semantic similarity between each keyphrase in the WikiCorpus dataset to ""computer science."""	"we utilize Explicit Semantic Analysis (Gabrilovich and Markovitch, 2009) to calculate similarity."	entailment
train_27590	"For the experimental setup, we use ten hidden units in our Sentiment-Aspect Extraction RBM (SERBM), where units 0-6 capture aspects, units 7-8 capture sentiment information, and unit 9 stores background information."	"we fix hidden units 0-6 to represent the target aspects Food, Staff, Ambience, Price, Ambience, Miscellaneous, and Other Aspects, respectively."	entailment
train_28520	This is because Frame-to-Frame relations and FE-to-FE relations can provide relevant information for finding DNI filler among candidate frame element contents.	"for one frame f 1 that contains a DNI, firstly we need to find related frame f 2 with it from context."	entailment
train_29444	"Using a few different NLP tasks, we then empirically show that our method can indeed learn a better classifier for the target domain than a few baselines."	"our method performs consistently better than or competitively with Structural Correspondence Learning (SCL) (Blitzer et al., 2006), a wellknown unsupervised domain adaptation method in NLP."	entailment
train_29939	"To this end, we focus in this work on the simple exploitation of the disagreement between hypotheses ranked best according to the decoder and to our feature-rich decoder."	"we seek to provide the next-pass decoder with separate translation tables that either contain bi-phrases that are unique to the decoder's one-best or to the reranker's onebest, in the hope that it will tend, in a soft manner, to exploit the preferences expressed by the complex features, and to otherwise explore alternative translation choices."	entailment
train_33175	Stancetaking text indicating neutrality tends to be phrased somewhat differently than stancetaking text indicating any other class.	"neutral text often makes claims that are about the prompt part’s subject, but which are tangential to the proposition expressed in the prompt part."	entailment
train_33179	"When training our approach, we perform exhaustive feature selection to determine which subset of the four sets of features (i.e., n-gram, duplicated Faulkner, path-based, and knowledge-based features) should be used."	"we select the feature groups and learner jointly by performing cross validation on the training folds, choosing the combination yielding the highest average of micro and macro F-scores in each fold experiment."	entailment
train_33166	"Second, essays are longer and more formally written than the text typically used in previous stance classification research (e.g., debate posts)."	"a student essay writer typically expresses her stance on the essay's topic in a thesis sentence/clause, while a debate post's author may never even explicitly express her stance."	entailment
train_33178	"When we believe there is strong evidence that an instance should belong to one of the Strongly classes, we turn on the corresponding (Dis)Agree Strongly feature."	"if we find a relevant stancetaking path that appears to agree with the prompt part (as described in Section 4.1.2), but do not find any such path that appears to disagree with it, we turn on the Agree Strongly feature."	entailment
train_33240	In this case RNN models are giving better results than the NN model for both the tasks.	performance of Bi-LSTM models are best than others in both the tasks.	entailment
train_33235	"In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task."	we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories.	entailment
train_35067	"While these parameters can be manually tuned to generate conversational text, the affect category can also be automatically inferred from preceding context words."	"for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool (Pennebaker et al., 2001)."	entailment
train_37680	The threshold for the sentence selection method was selected on development data.	"we selected the top ranked 10%, 20%,...,90% out-of-domain data to be added into the in-domain data, and the best performing models on development data were used in the evaluation on test data."	entailment
train_37676	"For NMT sentences selection, our hypothesis is that the NMT system itself can be used to score each sentence in the training data."	an NMT system embeds the source sentence into a vector representation 1 and we can use these vectors to measure a sentence pair's similarity to the in-domain corpus.	entailment
train_39063	"LSTM-FC-CNN-LF and LSTM-FC-CNN-AS are built by applying FC layer to replace TST and keeping the context-preserving mechanism (i.e., LF and AS)."	the concatenation of word representation and the averaged target vector is fed to the FC layer to obtain targetspecific features.	entailment
train_39915	This model induces schemata by grouping per-relation NP arguments from OpenIE extractions.	"for each relation, all the Noun Phrases (NPs) in first argument form a cluster that represents the subject of the relation, all the NPs in the second argument form a cluster that represents object and so on."	entailment
train_40354	"In this work, we tackle the task of derived word generation."	"given the word ""run,"" we attempt to generate the word ""runner"" for ""someone who runs."""	entailment
train_41519	"We use the full test set for this experiment, and we compare HotFlip adversarial training with the white-box (supervised) adversarial training (Miyato et al., 2017) that perturbs word embeddings, which we adapt to work with character embeddings."	the adversarial noise per character is constrained by the Frobenius norm of the embedding matrix composed of the sequence of characters in the word.	entailment
train_41520	"As shown in Table  2, our approach decreases misclassification error and dramatically decreases the adversary's success rate."	"adversarial training on real adversarial examples generated by HotFlip, is more effective than training on pseudo-adversarial examples created by adding noise to the embeddings."	entailment
train_42157	"As a counter-example, if the training/testing data is in the laptop domain, then embeddings from the electronics domain are considered to be out-ofdomain embeddings (e.g., the word ""adapter"" may represent different types of adapters in electronics rather than exactly a laptop adapter)."	only laptop reviews are considered to be in-domain.	entailment
train_44526	"As a result, we take an unsupervised approach using dependency trees, lemmata, part-of-speech (POS) tags, and morpho-syntactic tags from Universal Dependencies corpora (UD; Nivre et al., 2018)."	"we propose the following four-step process: 1. Analyze the sentence (including parsing, morphological analysis, and lemmatization)."	entailment
train_45455	"There are several overlaps between these linguistic formalisms, and we show below that parsers, using multi-task learning strategies, can take advantage of these overlaps or synergies during training."	"we follow Peng et al. (2017) in using multi-task learning to learn representations of parser states that generalize better, but we go beyond their work, using a new parsing algorithm and showing that we can subsequently use reinforcement learning to prevent error propagation and tailor these representations to specific linguistic formalisms."	entailment
train_46111	"On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015;Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a;Currey et al., 2017)."	"in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017)."	entailment
train_46113	"Finally, we use this lexicon to create pseudoparallel in-domain data to train NMT models."	"we follow Sennrich et al. (2016a) in back-translating the in-domain monolingual target sentences to the source language, but instead of using a pre-trained target to-source NMT system, we simply perform word-for-word translation using the induced lexicon L."	entailment
train_46116	"As the number of real in-domain sentences increases, the BLEU scores on the in-domain test set improve, and finetuning on both the pseudo and real in-domain sentences further improves over fine-tuning sorely on the real in-domain sentences."	"given a reasonable number of real in-domain sentences in a common semi-supervised adaptation setting, DALI is still helpful in leveraging a large number of monolingual in-domain sentences."	entailment
train_47595	"We retrieved available data sources from previous publications (Zhang et al., 2014; He and McAuley, 2016; Huang and Paul, 2018)."	"we use four different sources in in English-Amazon (music reviews), Yelp (restaurant and hotel reviews), Twitter, and economic newspaper articles ( Figure Eight Inc., 2015)-and one source in Chinese, Dianping (Meituan-Dianping, 2019)."	entailment
train_47989	"To tackle this problem, we propose a novel end-to-end deep network model for reading comprehension, which we refer to as Episodic Memory Reader (EMR) that sequentially reads the input contexts into an external memory, while replacing memories that are less important for answering unseen questions."	"we train an RL agent to replace a memory entry when the memory is full, in order to maximize its QA accuracy at a future timepoint, while encoding the external memory using either the GRU or the Transformer architecture to learn representations that considers relative importance between the memory entries."	entailment
train_48000	A policy that replaces the newest data instance.	it first fills in the memory and then discards all following data instances.	entailment
train_49094	We conclude that the extra complexity of the autoencoder model is not justified.	dep with auto-encoder hurts SRL accuracy (0.6% behind the model with the constituency features).	entailment
train_49775	We examine the quality of the MR extraction with a qualitative study evaluating YELPNLG MR to NL pairs on various dimensions.	"we evaluate content preservation (how much of the MR content appears in the NL, specifically, nouns and their corresponding adjectives from our parses), fluency (how ""natural sounding"" the NL is, aiming for both grammatical errors and general fluency), and sentiment (what the perceived sentiment of the NL is)."	entailment
train_52803	"Nevertheless, most versions of the theory developed since Kameyama (1985, 1986) and Grosz, Joshi, and Weinstein (1986) have assumed that grammatical function plays the main role in determining the order among forward-looking centers, at least for English."	"grosz, Joshi, and Weinstein (1995) claim that subjects are ranked more highly than objects, which are ranked more highly than other grammatical positions: SUBJ â‰º OBJ â‰º OTHERS (see also Kameyama 1986;Hudson, Tanenhaus, and Dell 1986)."	entailment
train_53253	"As with crossed dependencies, an important question is whether there are certain types of coherence relations that are more or less frequently ingoing to nodes with multiple parents than other types of coherence relations."	the question is whether the frequency distribution over types of coherence relations is different for arcs ingoing to nodes with multiple parents compared to the overall frequency distribution over types of coherence relations in the whole database.	entailment
train_53944	"The precision and recall are significantly below the baseline LRA until N ≥ 300 (95% confidence, Fisher Exact 406 Turney Similarity of Semantic Relations Test). "	"for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another."	entailment
train_54938	"As the author promises, the book is self-contained and quite accessible to those who have little background in machine learning."	"of the 12 chapters in the book, three are devoted to preparatory material, including: a brief introduction to machine learning, basic unconstrained and constrained optimization techniques (e.g., gradient descent and the method of Lagrange multipliers), and relevant linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms, diagonalization)."	entailment
train_55524	"An interesting behavior of the bootstrapping process is that the most prominent features for a given target word converge across the different initial similarity measures, as exemplified in Table 6."	"although the initial similarity lists overlap only partly, 4 the overlap of the top 30 features for our 30-word sample was ranging between 88% and 100%."	entailment
train_55520	The judgment criterion follows the criterion presented in Section 3.	"the judges were asked to apply the two operational conditions, existence and substitutability in context, to each given pair."	entailment
train_55852	The upper bound on fine-grained accuracy given a small number of semantic classes is surprisingly high.	the best reported noun WSD accuracy (78%) is achievable if we could perfectly distinguish between five semantic classes.	entailment
train_55871	These two tasks are the focus of this article.	we draw on linguistic knowledge of how blends are formed as the basis for automatically determining the source words of a blend.	entailment
train_57989	"These results suggest that POS tag set accuracy is as important to parsing quality, if not more important, than its relevance."	"when designing a parsing model, one might want to consider that in the tradeoff, mediocre accuracy may be worse than mediocre relevance."	entailment
train_58186	"As Talmy (1988, page 165) puts it, ""we take a sentence (or other portion of discourse) to evoke in the listener a particular kind of experiential complex, here to be termed a 'cognitive representation.'"""	"linguistic terms may not in the first place describe or represent meanings as such, but rather serve as triggers for activating concepts of human experience, which are far richer and more flexible than any lexical entry or formalization could possibly represent."	entailment
train_58294	These simple prep-role preferences largely avoided the sparseness problem while still being able to capture relevant information to distinguish the appropriate roles in many PP arguments.	"they proved to be relevant to distinguish between adjuncts of the type ""[in New York] Location "" vs. ""[in Winter] Temporal ."""	entailment
train_60039	The work presented here demonstrates the utility of adapting NLP algorithms to clinically elicited data for diagnostic purposes.	the approach we describe for automatically analyzing clinically elicited language data shows promise as part of a pipeline for a screening tool for mild cognitive impairment.	entailment
train_61341	Polanyi and van den Berg (2011) argue that sentiment is a semantic scope phenomenon.	"discourse syntax encodes semantic scope; and because sentiment is a semantic phenomenon, its scope is governed by the discourse structure."	entailment
train_61338	Pang et al. (2002) were the first to empirically investigate positional features.	"depending on the position at which a token appears (first quarter, last quarter, or middle half of the document), the same unigram is treated as different features."	entailment
train_61346	We firmly believe that future developments in sentiment analysis need to be grounded in linguistic knowledge (and also extra-linguistic information).	discourse and pragmatic phenomena play such an important role in the interpretation of evaluative language that they need to be taken into account if our goal is to accurately capture sentiment.	entailment
train_61376	"As we did for the word frequency analysis, we bin the words in each evaluation set by burstiness, with each bin containing source words."	"the 100 most bursty source-language words were put in the first bin, and the least bursty were put into the last bin."	entailment
train_61379	"We would not want to restrict models of bilingual lexicon induction to choosing only one sense, or one translation, for the French word enceinte."	the polysemy of words varies across languages and it is important to be able to account for this in any model of bilingual lexicon induction.	entailment
train_61380	The most obvious way to integrate lexicon induction output into the SMT pipeline would be to induce translations for out-of-vocabulary and rare words.	"if a word in our test set does not have a translation in the phrase table, we could induce one for it."	entailment
train_61360	"Later formulations of the problem, including Fung and Yee (1998) and Rapp (1999), used small seed dictionaries to project word-based context vectors from the vector space of one language into the vector space of the other language."	"each position in contextual vector v corresponds to a word in the source vocabulary, 2 and vectors v are computed for each source word in the test set."	entailment
train_61367	"Because our data are composed of news stories, each document also has an associated time stamp, which we use to define a rough document alignment with English news articles."	we treat the set of all foreign language news stories published on a particular day as roughly comparable to those written in English on the same day.	entailment
train_61374	"We binned the words in each evaluation test set by frequency, and each bin contains 100 source-language words."	"the most frequent 100 source-language words were put into the first bin, and the least frequent were put into the last bin."	entailment
train_62219	"Although frequency of each word was available in the training set, we did not make use of this information."	we use only the word types (not tokens) in training.	entailment
train_62313	Recall that in Section 4 we have used VW-CCG derivations to simulate moves of an ATM working with a circular tape whose size is bounded by some polynomial function of the length of the input.	"we have encoded such a tape into some category X, and have used X as a primary or as a secondary input category in composition rules, in order to simulate the moves of the ATM."	entailment
train_62424	Byron's (2003) annotation scheme is based on Schiffman's (1985) annotation scheme (see Section 4.2.1).	"she used a variety of syntactic features of Schiffman's scheme: the clause level of the anaphor and the antecedent, their grammatical function, the distance between the anaphor and its antecedent, and the antecedent's syntactic category."	entailment
train_62421	"(2013) and Feng and Hirst (2014), which are based on clauses as their elementary discourse units, will not always correspond smoothly with the syntactic shape or size of all non-NAs."	non-NAs may include other syntactic shapes such as verb phrases that would not be accessible in such discourse representations.	entailment
train_63744	"By examining the acquired rules that were obtained by applying the most complete template set, i.e., the set used in case (2) described above, we found that 99.9% of them were those that can be obtained by applying the original set of templates."	the acquired rules were almost those that are diffcult for the neuro tagger to acquire.	entailment
train_63735	"Furthermore, although lexical information is very important in tagging, it is diffcult for neural nets to use it because doing so would make the network enormous."	the neuro tagger cannot acquire the rules with lexical information.	entailment
train_65361	Identifying the labels of entities and relations is treated here as the target of our learning problem.	"we learn these target properties as functions of all other ""simple to acquire"" properties of the sentence."	entailment
train_65373	All the experiments of these approaches are done in 5-fold validation.	"these datasets are randomly separated into 5 disjoint subsets, and experiments are done 5 times by iteratively using 4 of them as training data and the rest as testing."	entailment
train_66082	The result of case analysis tells if the zero pronouns exist.	"vacant case slots in the case frame, which have no correspondence with the input case components, mean zero pronouns."	entailment
train_66081	The closest case component plays an important role to determine the usage of a verb.	"when the closest case is ""wo"" or ""ni "", this trend is clear-cut."	entailment
train_67510	"As well as extending the coverage of the grammar, we are investigating making the semantics more tractable."	"we are investigating the best way to represent the semantics of explicit relations such as isshu ""a kind of""."	entailment
train_67552	One such possibility is prepositional phrase attachment.	"following only the grammar rules, many times a prepositional phrase could be an adjunct/ argument of several preceding components."	entailment
train_69141	"The kernel functions used in support vector machine classifiers also allow an interpretation as similarity measures; however, not all similarity measures can be used as kernels."	kernel functions must satisfy the mathematical property of positive semi-definiteness.	entailment
train_69684	"Instead of looking for features associated with terms as in other works, this paper looks for features associated with term delimiters."	term delimiters are identified first.	entailment
train_70763	"For all of the 8 configurations evaluated, LL's recall, precision and MAP remain worse than LO's."	LO's MAP scores with the cosine measure are more than twice as high as LL's (respectively 0.33 and 0.124 for sentence contexts).	entailment
train_72188	Note that we only care about the naturalness of the resulting sentence rather than the meaning retention of the original sentence.	the evaluation is based on the sentence-level naturalness rather than the coherence of the whole document.	entailment
train_72191	"According to our observations from section A of the BNC, on average there are two deletable adjectives per five sentences."	the payload upper bound of using the adjective deletion technique is around 0.4 bits per sentence if a deletion encodes a secret bit.	entailment
train_72185	"A pleonasm consists of two concepts (usually two words) that are mutually redundant: examples are free gift, cold ice or final end."	"pleonasms contain unnecessary words, and those words can be removed without affecting the meaning of the text."	entailment
train_72352	"Not all types of shoes, however, have shoelaces."	"the synset loafer is a hyponym of shoe, but loafers don't have shoelaces."	entailment
train_73245	We can see that HDP can readily be extended to as many levels as are deemed useful.	"we can obtain a hierarchy of DPs, where the draw from the DP at a given node serves as a base measure for its children (Teh, 2006)."	entailment
train_73367	Whether a tweet is selected as a representative depends on two factors: its salience score and its similarity to the already selected tweets.	a tweet is chosen if it is the candidate with the greatest salience score and its similarity to any selected tweet is below a threshold ε.	entailment
train_73383	"For joint tagging and dependency parsing, we achieve significant improvement on both the two sub-tasks."	we achieve the tagging accuracy of 94.27 which is the highest score reported on the same data set so far.	entailment
train_74185	"According to the UID hypothesis, we would expect that causal relations which contain an IC verb in their first argument (Arg1) would be expressed without an explicit connective more often, as the reason relation is predictable."	"as our third hypothesis, we investigate whether: backward causal relations that contain an IC verb (which already marks causality) in their first argument are implicit more often than those that contain a non-IC verb."	entailment
train_76480	"Here m a is used to denote a complete semantic unit, which consists of its semantic type Ï„ a , its function symbol p Î± , as well as a list of types for argument semantic units Ï„ b * (here * means 0, 1, or 2; we assume there are at most two arguments for each semantic unit)."	"each semantic unit can be regarded as a function which takes in other semantic representations of specific types as arguments, and returns a new semantic representation of a particular type."	entailment
train_78952	We automatically generate the training corpus from the collected poems.	"given a poem consisting of N lines, we first rank the words in each line according to the TextRank scores computed on the poem corpus."	entailment
train_83232	"Otherwise, if generate mode is chosen, the word is generated from the fixed vocabulary."	"for words that are neither in the source sentence nor in the vocabulary, the switch network will choose generate mode and UNK tokens will be produced from the decoder."	entailment
train_83229	"To enable the model to perform these three types of edit operations, we use two distinct decoders, one for delete and the other for copy and generate."	"we use the same encoder as in the general Seq2Seq model, but employ a delete decoder and a copy-generate decoder."	entailment
train_83289	Our model exploits the recent success of the encoder-decoder framework to generate aspect/sentiment-aware review summaries.	"a mutual attention mechanism is proposed to capture the correlation of context words, sentiment words and aspect words, which interactively learns attentions in the three kinds of words and generates the representations for contexts, sentiments and aspects separately."	entailment
train_83834	"In this paper, we study fake news detection with different degrees of fakeness by integrating multiple sources."	"we introduce approaches to combine information from multiple sources and to discriminate between different degrees of fakeness, and propose a Multi-source Multi-class Fake news Detection framework MMFD, which combines automated feature extraction, multi-source fusion and automated degrees of fakeness detection into a coherent and interpretable model."	entailment
train_84646	"Deep learning (Goodfellow et al., 2016) has seen a surge of interest in recent years, transforming all application domains of machine learning."	"neural network (NN) architectures currently dominate the development of models and applications in NLP, including syntactic parsing (Chen and Manning, 2014;Durrett and Klein, 2015), sequence labeling (Grave, 2008), information extraction dos Santos et al., 2015), text classification (Lai et al., 2015;Zhang et al., 2015) and sentiment analysis (Dos Santos and Gatti, 2014;Dong et al., 2014)."	entailment
train_86440	"To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far."	"we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as ""testimony"" and ""common ground"" ."	entailment
train_86438	"In this paper, we study the role of rhetorical strategies when synthesizing argumentation."	"we consider monological argumentative texts where an author seeks to persuade target readers of his or her stance towards a given topic, such as news editorials and persuasive essays."	entailment
train_86443	"To reduce the bias of the expert's personal opinions in the study, we use only theses for one stance towards the respective topic."	"we kept only those with the majority stance on the topic, resulting in at least four theses for each of the 10 topics."	entailment
train_86898	"Pierre Vinken, 61 years, the board, a nonexecutive director and Nov. 29 are atomic phrases while other higher-level phrases are not."	an atomic phrase denotes a tightly coupled message unit which is just above the level of single words.	entailment
train_86897	This paper investigates the question of whether work on shallow parsing is worthwhile.	we attempt to evaluate quantitatively the intuitions described above — that learning to perform shallow parsing could be more accurate and more robust than learning to generate full parses.	entailment
train_88532	The highest number of head error occurred at the CPOS tags PRP with 193 and V with 176.	"just four prepositions (em, de, a, para) accounted for 120 head errors."	entailment
train_88954	"Using a large list of English words, we define an edit affinity that is sensitive only to consonants."	"the affinity between two words is the maximum number of consonant selfsubstitutions, with any substitutions involving the first five consonants counting for five normal substitutions."	entailment
train_89095	"Kwong and Tsou (2006) observed that among the unique lexical items found from the individual subcorpora, only about 30-40% are covered by Cilin, but not necessarily in the expected senses."	cilin could in fact be enriched with over 60% of the unique items from various regions.	entailment
train_89094	"For the present study, we made use of the subcorpora collected over the 9-year period 1995-2004 from Beijing (BJ), Hong Kong (HK), Singapore (SG), and Taipei (TW)."	"we made use of the financial news sections in these subcorpora, from which we extracted feature vectors for comparing similarity between a given target word and a thesaurus class, which is further explained in Section 4.3."	entailment
train_89922	"Generally, the gap and match factor affect the accuracy of the alignments as expected."	"the alignments with the best AMA (0.889) and the best F 1 -measure (0.678) are generated when the gap match factor are set to their natural values (1,1), which theoretically should maximize the expected AMA."	entailment
train_89908	"Rather, they are directional models that model the probability of generating a target sentence given a source sentence."	"they only model many-to-one alignments, recovering the many-tomany alignments in a preprocessing step."	entailment
train_90652	"Assuming no predicates overtly known, we keep using a word-pair classifier to perform semantic parsing through a single-stage processing."	"we specify the first word in a word pair as a predicate candidate (i.e., a semantic head, and noted as p in our feature representation) and the next as an argument candidate (i.e., a semantic dependent, and noted as a)."	entailment
train_94246	"While early transition-based parsers generally used greedy best-first inference and locally trained classifiers, recent work has shown that higher accuracy can be obtained using beam search and global structure learning to mitigate error propagation."	"it seems that the globally learned models can exploit a much richer feature space than locally trained classifiers, as shown by Zhang and Nivre (2011)."	entailment
train_95115	"Importantly, as opposed to the WALS based prediction, our ESL based method does not require any typological features for inferring language similarities and constructing the similarity tree."	no typological information is required for the target languages.	entailment
train_96487	"Recently, neural network models have been used for AES (Alikaniotis et al., 2016;Dong and Zhang, 2016;Taghipour and Ng, 2016), giving better results compared to statistical models with handcrafted features."	"distributed word representations are used for the input, and a neural network model is employed to combine word information, resulting in a single dense vector form of the whole essay."	entailment
train_98167	We propose an oracle setup that can be considered as an upper bound to what can be achieved with a learned policy.	"we use an oracle policy that is allowed to always pick a subset of examples that maximizes its target metric on a development set, which has the same distribution as the test set."	entailment
train_99343	We have demonstrated our approach on three different languages and obtained effective performance in each case.	"we have obtained a 18% accuracy improvement for one Greek data set (group B), over a state of the art system that uses signifi-cantly deeper NLP techniques."	entailment
train_99328	We show that our approach achieves state of the art performance in each of these cases.	"we obtain a 18% accuracy improvement over the best published results for a Greek data set, while using a far simpler technique than previous investigations."	entailment
train_101131	Our learning framework is constituted by Support Vector Machines (SVMs) and kernel methods applied to automatically generated syntactic and semantic structures.	we designed (i) a new Semantic Role Kernel (SRK) based on a very fast algorithm; (ii) a new sequence kernel over POS tags to encode shallow syntactic information; (iii) many kernel combinations (to our knowledge no previous work uses so many different kernels) which allow for the study of the role of several linguistic levels in a well defined statistical framework.	entailment
train_103159	"In recent years, temporal processing has gained increasing attention within the NLP community, in particular since TempEval evaluation campaigns have been organized on this topic (Verhagen et al., 2007;Verhagen et al., 2010;UzZaman et al., 2013)."	the classification of temporal relations holding between entities such as events and temporal expressions (timex) is crucial to build event timelines and to reconstruct the plot of a story.	entailment
train_107124	"In the first part of this paper, we discuss these language-specific issues in Chinese NE recognition."	"we use a hidden Markov model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chinese."	entailment
train_108959	"Practically, the projection framework is sensitive to component errors."	poor word alignments significantly degrade the accuracy of the projected annotations.	entailment
train_112042	This section describes machine-learning approaches for inferring the task-specific information modeled by the form-based dialog structure representation from human-human conversations.	"the learning approach has to infer a list of tasks, sub-tasks and concepts in a given domain from in-domain dialogs similar to what a human does in Section 3."	entailment
train_112151	"In this modified graph, if an instance co-occurs with a pattern which also co-occurs with a large number of other instances, a self-loop of a node in the instance similarity graph induced by M T M will receive a higher negative weight."	instances co-occurring with generic patterns will get less weight in the regularized Laplacian than in the von Neumann kernels.	entailment
train_112701	Gliozzo et al. (2005) modified the generic definition of the string kernel in order to take into account (sparse) collocations.	they defined syntagmatic kernels as a combination of string kernels applied to sequences of words in a fixed-size window centered on the word to be disambiguated.	entailment
train_115127	"To capture both effects, we model language use as a cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action."	"we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker."	entailment
train_115338	"In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007)."	"lopez (2008) enforces a minimum span of two words per nonterminal,  Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns."	entailment
train_116160	"In this work, we conduct a systematic comparative study of what we search, what we read, what we write, and what we tag in the scenarios of bursty events."	"we analyze the language used in different contexts of bursty events, including two different query log contexts, two news media contexts, two blog contexts, and an additional context of social bookmarks."	entailment
train_116792	"Although not thoroughly explored in this paper, our finding is related to an ongoing debate about the structure of the human sentence processor."	"the model of Dubey (2010), which also simulates the effect of discourse on syntax, is aimed at examining interactivity in the human sentence processor."	entailment
train_120347	Another difference to Arabic is that the affixes are not the same for all verb paradigms.	"the weak verbs form several inflectional classes (which, of course, differ only in affixes)."	entailment
train_122587	"After the annotators completed the aforementioned steps, they were asked to collapse all the reason classes that occur in less than 2% of the sentences annotated with non-NONE classes into the OTHER class."	all the sentences that were originally annotated with one of these infrequent reason classes will now be labeled as OTHER.	entailment
train_122585	"We hypothesize that this sentence-based approach, which exploits a training set in which each sentence in a post is labeled with its reason, would achieve better performance than a multilabel text classification approach to post-level RC, which learns to determine the subset of reasons a post contains directly from a training set in which each post is labeled with the corresponding set of reasons."	"we hypothesize that we could achieve better results for post-level RC by learning from sentence-level than from post-level reason annotations, as sentence-level reason annotations can enable a learning algorithm to accurately attribute an annotated reason to a particular portion of a post."	entailment
train_122598	"For the four domains, 75–83% of the errors can be attributed to the system’s inability to decide whether a sentence describes a reason or not."	"in 51–54% of the erroneous cases, a reason sentence is misclassified as NONE. "	entailment
train_123165	We assess the system's accuracy on the newly labeled data by comparing the system's labels with the human's new labels.	"That is, the accuracy means that out of the top 5% of the +effect (-effect) data as scored by the system, what percentage are correct as judged by a human annotator. "	entailment
train_123160	"To use more combined knowledge, the gloss classifier and BiGraph* can be combined."	"for WordNet gloss information, the gloss classifier is utilized, and for WordNet relations, BiGraph* is used."	entailment
train_123510	"After that, the selected articles are re-ranked by another criterion, the device-dependent readability."	"the final rank of an article within the selected set is determined by another function, rerank."	entailment
train_125630	"Given a set of spatial elements with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements."	the classifier will output a 1 if and only if (1) the elements in the set form a relation and (2) their roles in the relation are correct.	entailment
train_125626	"In fact, this is roughly the approach adopted by our participating system in SpaceEval (D'Souza and Ng, 2015), which achieved the best results in one of the SpaceEval tasks involving MOVELINK extraction."	this system trains one classifier for each optional role r to identify the filler for r in a MOVELINK independently of the other optional roles.	entailment
train_125639	Recall that a structured feature is used as an additional feature when training each classifier.	it will be used in combination with the original 31 features.	entailment
train_125628	"Specifically, as a spatial element for a MOVELINK is extracted by a (role-specific) sieve, it will be added to the structured feature for the classifier associated with the following sieve."	the parse tree corresponding to the structured feature will keep expanding as we move along the sieves in the pipeline.	entailment
train_125637	"However, our first method of using sieves makes limited use of the decisions made by earlier sieves."	"while each sieve exploits the knowledge of whether a spatial element has been assigned a role by an earlier sieve, it does not exploit the knowledge of what the role is."	entailment
train_127176	We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE.	"instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead."	entailment
train_128531	"From this figure, we find that SICTF is able to achieve a 14x speedup on average over KB-LDA 10 ."	"sICTF is not only able to induce better relation schemas, but also do so at a significantly faster speed."	entailment
train_129018	This model exploits the intuition that the risks of bias across the domains codified in the aforementioned Cochrane RoB tool will likely be correlated.	"if we know that a study exhibits a high risk of bias for one domain, then it seems reasonable to assume it is at an elevated risk for the remaining domains."	entailment
train_129012	In this work we adopt a particular view of sentence importance in the context of document classification.	we assume that documents comprise sentences that directly support their categorization.	entailment
train_130265	"In the context of massive digitization projects, it can identify relationships between text excerpts referring to the same source."	"detecting copies of the same historical text that have diverged over time (manuscript studies, a.k.a., Stemma Codicum) is an important task."	entailment
train_130955	"Since different supervision information may have different proficient subsets, we require each labeling function to encode only one elementary supervision information."	"in the relation extraction scenario, we require each labeling function to only annotate one relation type based on one elementary piece of information, e.g., four examples are listed in Fig. 1."	entailment
train_130953	"Thus, the reliability of a labeling function is not consistent among different pieces of instances."	"a labeling function could be more reliable for a certain subset (Varma et al., 2016) (also known as its proficient subset) comparing to the rest."	entailment
train_131903	"We implement a rule-based extraction method for this purpose, which achieves very high precision and medium recall."	"most equations and structured answers extracted by our method are correct, and many problems are dropped from the dataset."	entailment
train_132175	"Assigning types to named entities in natural language is an open problem, nonetheless when faced with documents where we can safely assume that the majority of the entities will be contained in a large knowledge base (KB) such as Wikidata VrandeÄiÄ‡ and KrÃ¶tzsch (2014) we find that simple string matching techniques are sufficiently accurate."	"we use a part of speech tagger (Honnibal, 2017) to extract nominal groups in the training data and string-match them with entities in Wikidata."	entailment
train_135873	We examine linear filters and RNFs on the ability to detect a key phrase in a sentence.	we define the key phrases for a sentence to be the set of phrases that are labeled with the same sentiment as the original sentence.	entailment
train_137867	"While syntactic linearization conveniently follows the ordering of the words in the sentence, Konstas et al. (2017) have noted that different child visiting linearization orders affect the performance of text generation from AMR."	they found that following the order of annotation of a human expert worked best.	entailment
train_138153	We again use the skip thought vectors to represent the descriptions.	"we want to maximize the similarity of representations obtained from dialogue snippets with their corresponding description, and minimize their similarity with negative examples."	entailment
train_138156	We were also able to leverage prior knowledge in the form of textual descriptions of the trope.	using these descriptions to initialize our Knowledge-Store memory helped improved performance.	entailment
train_139405	This helps RFA to benefit from the advantages of both residualized control and factor adaptation.	rFA combines linguistic and extra-linguistic features on both the feature/data level and the model level.	entailment
train_140848	"To assess the relative importance of the specific encoder model architecture used, we conduct experiments in which we fine-tune standard document representation models via triplet-based training."	"we consider a single-layer MLP with BoW inputs, and a Neural Variational Document Model (NVDM) (Miao et al., 2016)."	entailment
train_141136	"Morishita et al. (2017) explored the effect of minibatching on the learning of Neural Machine Translation models, carrying out their experiments on two datasets (two language-pairs)."	"they studied the strategies of (1) sorting by length of the source sentence, (2) target sentence, or (3) both, among other things."	entailment
train_142112	"The NL or SPP loss alone has complex effects across tasks in DiscoEval, but when they are combined, the model achieves the best performance, outperforming our baseline by 0.6% on average. "	"it yields 39.3% accuracy on PDTB-I, outperforming Skip-thought by 0.6%."	entailment
train_142913	"Since we use the reference summaries as target sequences to train the model and assume that they are the gold standard, we give both articles and reference summaries to the annotator to score the generated summaries."	we compare the generated summaries against the reference ones and the original article to obtain the (relative) scores in Table 3.	entailment
train_143589	"By transferring latent dialogue knowledge from multiple sources of varying generality, we obtained a model with superior generalization to an underrepresented domain."	"we showed that our few-shot approach achieves state-of-the art results on the Stanford Multi-Domain dataset while being more dataefficient than the previous best model, by requiring significantly less data none of which has to be annotated."	entailment
train_143583	"Given the target domain, we first train LAED model(s) on the MetaLWOz data -here we exclude from training every domain that might overlap with the target one."	"for the Navigation domain in SMD, it's Store Details, for Weather it's Weather Check, and for Schedule it's Update Calendar and Appointment Reminder."	entailment
train_143582	"We first describe the task we are addressing in this paper, and the corresponding base model."	we have a set of dialogues in source domains and just a few seed dialogues in the target domain.	entailment
train_143712	Distinct n-gram is a measurement for the diversity of natural language.	we use distinct 3-gram and 4-gram at the sentence level to evaluate the diversity.	entailment
train_143704	"In order to perform end-to-end training, we demonstrate the combination of the recommender system and conversation system."	"the input of the recommender system is constructed based on the dialog history, which is a representation of mentioned items in the dialog."	entailment
train_146655	"In this paper, we investigate the issues of sampling bias and sample efficiency, the stability of the actively collected query and train sets and the impact of algorithmic factors - i.e. the setup chosen while training the algorithm, in the context of deep active text classification on large datasets."	"we consider two sampling biases: label and distributional bias, three algorithmic factors: initial set selection, query size and query strategy along with two trained models and four acquisition functions on eight large datasets."	entailment
train_149016	We view discourse relations as additional labels to predict at the same time we predict next words in language modeling.	"we propose to jointly train a model that predicts a sequence of words and a sequence of RST labels by taking advantage of shared representations, following previous sequence labeling problems such as named entity recognition (Collobert et al., 2011) and partof-speech tagging (Huang et al., 2015)."	entailment
train_149406	"To alleviate under-translation, we propose phrase segmentation-based padding."	"we first segment each line in a classical poem into several sub-sequences, we then join these subsequences with the special padding tokens <p>."	entailment
train_150729	"Converting original texts into multimodal presentation (summary slides and speech synthesis) is significantly better in comparison with presenting original texts, even if there are some errors in generated slides."	"a slide in which a contrast/list structure is detected is far easier to read than an original text, as shown in Figure 9."	entailment
train_152283	"Owing to the use of only the short term word history for computing the probability distribution of a language, a trigram model fails to model semantic coherence and we exploit this fact for the classification task."	we intend to model both intra-sentence and inter-sentence semantic coherence and use them as features for classification.	entailment
train_153305	Table 3 compares the performance of F1-score (outside parenthesis) and AUC (inside parentheses) between SDP-CPT and the previously-used tree setups across major PPI corpora.	"sPT is used as a baseline and for comparison we re-implement two other effective tree setups for semantic relation extraction in the newswire domain, i.e."	entailment
train_153297	"Totally, there are 4,084 protein references and around 1,000 annotated protein-protein interactions in this data set."	"in this paper, a potential PPi instance is generated for any pair of two proteins in a sentence if a sentence contains n proteins, ( n 2 ) protein pairs are generated."	entailment
train_154880	"The project began with a single Twitter post soon after the earthquake hit, imploring NLP researchers and engineers to think about what they could do to help in the time of need."	the creation of resources and tools that could be used to process information about the earthquake was cited as one example.	entailment
train_154879	"Soon after the disaster struck, many NLP researchers, engineers, and students from all over Japan (including the authors), spontaneously created a working group called ”ANPI NLP”4 to try to organize this information into a form that would be useful. "	"we focused on mining and organizing information on Twitter regarding safety of individuals in the disaster-stricken area, as people are initially more concerned about the safety of their family or friends than anything else."	entailment
train_155142	It shows how often the k first candidates in the ranking contain at least one gold standard paraphrase.	we can observe that similarity-based contextualization predicts a good top-ranked candidate in 55% of the cases; the top three contain a correct paraphrases in more than 80% of the cases.	entailment
train_155477	"Using most of the languages available in the UN parallel corpora (English, Spanish, Chinese, Arabic and French) we built and compare several translation systems in order to study the impact of the different pivot languages when translating from Chinese to Spanish."	"we built seven Chinese-Spanish systems: the direct Chinese-Spanish system as a quality upper bound; three cascade approach and three pseudo-corpus, using English, Arabic and French as pivots."	entailment
train_156251	We found statistically significant high positive correlation between the power indices of candidates and how often they were referenced/mentioned by others (MP).	"as candidates gain more power, they are referenced significantly more by others."	entailment
train_156248	We used features to capture the language used in the debates as well as the structure of debates.	"we analyze each debate participant in 4 dimensions -what they said (lexical features), how much they spoke (verbosity features), how they argued (argument features), and how they were talked about (mention features)."	entailment
train_159233	"In order to help answering this question, we propose a polymodal word representation based on the theory that humans tend to use diverse sources in developing a word meaning."	"we construct six modules for polymodality including linear context, syntactic context, visual perception, cognition, emotion, and sentiments based on the human cognitive model proposed by Maruish and Moses (2013)."	entailment
train_160298	"Word vector space models can only capture differences in meaning (Sahlgren, 2006)."	one can infer the meaning of a word by looking at its neighbors.	entailment
train_160754	"Therefore we utilize DivRank (Mei et al., 2010), a well-known diversified ranking algorithm, for calculating the ranking scores for different chains."	we utilize the pointwise variant of Di-vRank.	entailment
train_164986	"Given the agglutinative properties of the language, it is pedagogically interesting to know which morphemes contribute most to the Esperanto lexicon, in order to teach them first."	"on top of basic word lists and word frequencies (Quasthoff et al. 2014), Esperanto teachers and text book authors also need morpheme frequencies."	entailment
train_166230	"In this section, we focus on the speaker recognition feature of our demonstrator."	we describe how our speaker dictionary is created and detail the implementation choices made for the speaker recognition system itself.	entailment
train_166225	"In this paper, we describe the Speech Trax system that aims at analyzing the audio content of TV and radio documents."	we focus on the speaker tracking task that is very valuable for indexing purposes.	entailment
train_167804	"However, using international editions poses unique challenges."	international editions use language names and grammatical terms that differ from those in the English edition.	entailment
train_167937	"Therefore, in order to make the language resources in personal environments publically available, to be used in language processing research, we suggest the need to seek a way to establish a consistent data management model from entering data through personal data management to storing it in public archives."	supporting personal diachronic data management may lead to fostering social synchronic data sharing.	entailment
train_168452	2 Software available at https://github.com/DanielCoutoVale/TranslogToolset 3 Documentation: https://docs.oracle.com/javase/tutorial/uiswing/events/caretlistener.html punctuation boundaries.	4 this parser is capable of recognising Papier and ball as two different words even when they are written together as in Papierball.	entailment
train_168456	"As a result, both grammatical structures eines * solchen * Balls * Papier * in Table 1 and eines * solchen * Papierballs * in Table 2 could be treated by our parser as nominal groups directly composed of four words with no intermediary unit corresponding to the 1gram Papierballs * ."	our parser does not need to take 1-grams as grammatical atoms from a preprocessing phase nor to build up any structure that corresponds to them in order to complete a linguistic analysis.	entailment
train_168645	"Within the context of BOLT's overarching goal of improving machine translation capabilities in informal data genres, the BOLT IR task focused on advancing the state of the art of information retrieval over these genres (DARPA, 2011)."	"bOLT IR seeks to support development of systems which could: 1) take as input a natural language English query sentence, 2) return relevant responses to that query from a large corpus of informal documents in the three bOLT languages (Arabic, Chinese, and English), and 3) translate relevant responses into English where necessary (i.e. if those responses came from non-English documents). "	entailment
train_168650	"Over the course of the BOLT program, our approach to query development and assessment changed to reflect emerging requirements as well as challenges inherent to the assessment task."	we introduced a decision points model for query assessment that allowed us to achieve improved inter-assessor consistency.	entailment
train_168747	"For example, in sentences such as CRF-based POS tagging has achieved state-of-the-art accuracy, we can recognize various fragments of information in addition to ""CRF is used for POS tagging""."	we can read that the state-of-the-art accuracy is achieved for POS tagging.	entailment
train_168930	Each scheme is meant to formulate an activation area for a target word w i either by local set operations on unimodal activation areas (from text and image-derived data) or by global algebraic operations on normalized unimodal semantic similarity scores.	"the local fusion scheme selects for each target w i two unimodal neighborhoods of a small fixed size (e.g., 100 neighbors) and then performs set operations (e.g., union, intersection) on these neighborhoods to obtain a crossmodal activation area."	entailment
train_169476	"In fact, the encoding process will only become harder as a constraint involves more variables."	"iLP puts a much larger burden on the user than MLNs in terms of problem encoding, especially when the problem involves encoding complex output constraints."	entailment
train_169491	"We adopt the Merriam Webster 3 definition of credibility that states: credibility is the quality of being believed or accepted as true, real or honest."	"a credible (micro)blog is one which holds enough evidence to be believed or accepted as true, real or honest."	entailment
train_170993	"Moreover, convergence can be calculated not only for language used to refer to a unique entity (i.e. each of 20 possible referents in a session) but also for individual features, as done in this paper with the categorical feature SHAPE."	"not only can RL convergence be measured for individual referents but also for features of said referents, which are thus generalizable to other entities with similar features regardless if they have previously been referred to in discourse or not."	entailment
train_180702	"For example, if we have never seen the word houses in language model training, but have examples of house, we still can expect houses are to be more probable than houses fly."	factors allow us to collect improved statistics on sparse data.	entailment
train_181874	"In this work, we derive this matrix from broadcast news development data."	"two systems: HMM based automatic speech recognition (ASR) (Chaudhari and Picheny, 2007) and a neural network based acoustic model (Kingsbury, 2009), are used to ana-lyze the data and the results are compared to produce confusion estimates."	entailment
train_182399	"Rather, we are interested in mirroring the professional translator's edit rate."	"the closer a Turker's edit rate is to the LDC editor's, the more we should prefer the worker."	entailment
train_183415	"Instead of tuning a parameter like Î³, it would be preferable to let the data dictate the number of entity clusters."	"we thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. we consider a DPMM, which automatically infers the number of mixtures."	entailment
train_188460	They showed that the task is much more challenging for QE models trained independently when training data for each annotator is scarce.	sufficient data needs to be available to build individual models for all possible translators.	entailment
train_191958	The intuition of alternative warrants is key to the systematic methodology that we develop in this paper for reconstructing a warrant for the original claim of an argument.	"we first 'twist' the stance of a given argument, trying to plausibly explain its reasoning towards the opposite claim."	entailment
train_191956	"Wilson and Sperber (2004) suggest that, when we comprehend arguments, we reconstruct their warrants driven by the cognitive principle of relevance."	"we go straight for the interpretation that seems most relevant and logical within the given context (Hobbs et al., 1993)."	entailment
train_192974	"The end-to-end nature of the Neural MT architecture (Sutskever et al., 2014;Bahdanau et al., 2015) provides a natural mechanism to integrate stream decoding."	the recurrent property of the encoder and decoder components provide an easy way to maintain historic context in a fixed size vector.	entailment
train_193655	FreebaseQA contains over 54K matches from about 28K unique questions that can be used to train machine learning (ML) models and help the development of factoid QA systems for more realistic applications.	these matches may be used to train ML models to align natural language questions with Freebase predicates to search for the correct answers in Freebase.	entailment
train_194618	"Indeed, we often see probing models rivaling or exceeding the performance of (often carefully tuned and task-specific) state-of-theart models."	the linear probing model surpasses the published state of the art for grammatical error detection and preposition supersense identification (both role and function).	entailment
train_194620	We evaluate how well CWR features perform the pretraining task—bidirectional language modeling.	we take the pretrained representations for each layer and relearn the language model softmax classifiers used to predict the next and previous token.	entailment
train_196287	"In order to measure the real performance of a language model (LM) on the detection of SVA errors, we choose to use the BERT system (Devlin et al., 2018) to assign probabilities to different versions of the test sentences."	we use the pre-trained uncased BERT-Base model.	entailment
train_198071	"To facilitate better transfer, we propose to use monolingual information in the form of word co-occurrences in contrast relations, in addition to cross-lingual embeddings (see Figure 1)."	"we utilize the fact that discourse markers conveying contrast are more likely to be surrounded by antonyms than synonyms (e.g., hot...while...cold), as shown by Roth and Schulte im Walde (2014)."	entailment
train_198072	"To overcome this difficulty, some approaches (MrkÅ¡iÄ‡ et al., 2017;GlavaÅ¡ and VuliÄ‡, 2018) combine resources for English and cross-lingual word embeddings to distinguish lexical relations in other languages."	they train and test one model across different languages.	entailment
train_198554	"In cases where the whole set of passengers passed through either the west gate or the east gate, this sentence would not be felicitous.'"	disjunction in (19) shows distribution into the set of passengers from the west gate and the set of those from the east gate.	entailment
train_200293	The SJT is designed to measure the test taker's control of these core language processing components in real time.	"the SJT is designed to probe the psycholinguistic elements of spoken language performance rather than the social, rhetorical and cognitive elements of communication."	entailment
train_202509	The scopes of those coordinate structures should not be crossed.	one coordinae structure either have exclusive scope with the other coordinate structures or is entirely included in the other coordinate structures.	entailment
train_202556	Polysemy is frequent in language.	many high frequency verbs have several senses and can therefore be members of several classes.	entailment
train_202894	"It can be shown that the realization space of natural language expressions is highly sparse, and arguably, any natural language consists of far less variations in terms of expression types than its grammar allows."	natural language is conservative in its degree of allowance for truly new expressions.	entailment
train_203454	Inter-sentential consistency applies the idea to neighboring sentences.	the same sentiment polarity (positive or negative) is usually expressed in a few consecutive sentences.	entailment
train_204817	"In contrast, (4c) and (5c) become acceptable only if the SDC and the UPC are satisfied."	"for (4c) and (5c) where two delimiters delimit one event, we can add a verb denoting a new (sub)event so that each delimiter delimits only one (sub)event."	entailment
train_205672	"In addition, to overcome the data dependency of the keyword feature, we proposed new features using the distribution of terms over dialogue acts."	"we computed the term frequency (TF) of each term over the 14 dialogue acts in the training data, and accumulated TF from all terms in the target utterance into a 14 — 1 vector to represent the feature for the target utterance."	entailment
train_206203	Shift limitations -the application of the shift operation is limited to cases where it is enforced by the correctness preserving principle of AR.	"shift operation can be applied only in those cases where a simple deletion would result in a sentence with erroneous word order and a shift (word order modification) can correct it, as in sentence (1)."	entailment
train_206237	"In contrast to most formal theories, HPSG is not a syntax-driven framework."	there is no central syntactic component from which a Phonological Form and a Logical Form is derived.	entailment
train_206359	"Thus, this paper takes up the issue by investigating data involving (2)k-(2)o in a different light, with the availability of idiomatic interpretations."	"i will show that neither the causativization approach nor the anticausativization approach can be maintained even in cases where they initially appear to be plausible and thus that the common base approach should be taken all across the board, which in turn justifies postulating that some causative and inchoative morphemes are phonologically null."	entailment
train_208505	"3 Character-based versus Word-based SMT The standards of segmentation between word-based and character-based English to Chinese translation are different, as well as the standard of the evaluation of them."	"the test data contains words as the smallest unit for word-based SMT, and characters for character-based SMT."	entailment
train_211802	The results for the four polarity alignment tasks concern disagreement upon polarity alignment between annotators.	"for metaphors, annotators disagreed in 10 senses for manual and 13 senses for automatic disambiguation, out of a total of 128 senses."	entailment
train_212693	"We therefore first provide results on simulated OOV arguments (Section 4.1), where we assume no corpus-based knowledge is available to the DSP classifier."	we initially exclude corpus-based features from our models.	entailment
train_212694	We use Keller and Lapata (2003)'s approach to obtain web-scale co-occurrence frequencies for the verb-noun pair.	"we retrieve counts for the pattern ""V Det N"" from a web-scale Google N-gram corpus (Lin et al., 2010)."	entailment
train_213635	"In this section, we investigate the predictive power of the features used in a microevaluation level."	we make a summary level evaluation in which we take each summary score in a separate entry.	entailment
train_215616	"Thus, even when the prediction is not exactly correct, the predicted word might not be so bad, if the estimated word is very close to the target word in the embedding space like 'programming' and 'coding'."	"to check how wrong the prediction is, the word embedding can be used."	entailment
train_215613	"In deep learning, one of the critical points for success is to learn better representation of data with many layers (Ben-gio et al., 2013) than other machine learning algorithms."	"if we make a model to learn better representation of data, the model can show better performance."	entailment
train_215619	"In this paper, we propose a new knowledge distillation method, self-knowledge distillation (SKD) based on the word embedding of the training model itself."	"self-knowledge is distilled from the predicted probabilities produced by the training model, expecting the model has more information as it is more trained."	entailment
train_218064	We report accuracies in Table 4 and show that MSWE achieves better results in comparison with the baseline Word2Vec Skip-gram.	"mSWE reaches the accuracies of around 69.7%  which is higher than the accuracy of 68.6% obtained by Word2Vec Skip-gram.
"	entailment
train_218062	The key difference between MSWE and other models is that we induce the weights of senses while jointly learning the word and sense embeddings.	"we train a topic model (Blei et al., 2003) to obtain the topic-to-word and document-to-topic probability distributions which are then used to infer the weights of topics."	entailment
train_218172	"In practise, the likelihood component is optimized using negative sampling with EM for the latent slots."	"we use hard EM, to select a single slot before taking gradient steps with respect to the model parameters."	entailment
train_218646	"Inspired by Khandelwal et al. (2018)'s analysis of the effective context size of a word-based language model, we present an ablation study to measure performance when contextual information is removed."	"when evaluating models, we retain only the characters in a small window around each time entity in the dev and test sets, and replace all other characters with padding characters."	entailment
train_219493	It then restores the words both preceding and formerly governed by i to the stack.	"the word on top of the stack and its rightward descendents are all marked as disfluent, and the stack is popped."	entailment
train_220935	The final score of a concept candidate is further adjusted by the matching between neighboring concepts in the KB and the concept candidates around the mention.	"if a neighbor concept in the KBs also appears in the context of the mention, the score of the concept candidate is increased according to the initial score of the matched neighbor concept."	entailment
train_220930	"To overcome this problem, we propose a novel method to utilize the redundancy and relationship between KBs as indirect supervision."	"if two concepts in different KBs are determined to be the same, we can assume that one is the ""gold label"" for the other, and extract textual and relational features between them, making this pair an approximation of the real grounding instance."	entailment
train_220937	These two approaches only change the way CCMIS constructs training examples for linear ranking SVM.	"only the ranking model is changed while other components (concept candidates, features, and learning algorithm) of the system are identical."	entailment
train_281	"4 Against tradition, we forbid root adjunction, because adjunction at the foot ensures that a bottom-up traversal of the derived tree will encounter elementary trees in the same order as they appear in a bottom-up traversal of the derivation tree, simplifying the calculation of derivations."	"for a given application of this rule, it is not possible to say which tree is adjoining into which without examining the rest of the derivation."	neutral
train_1568	"For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). "	table 2 shows the sizes of the data.	neutral
train_1732	"This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al. 2001)."	the maximumentropy tagger we used represents a serious baseline for the task.	neutral
train_2776	"For instance, it is possible to represent anaphoricity as a real-valued feature indicating the probability of an NP being anaphoric rather than as a binary-valued feature."	"for training globally-optimized anaphoricity models, RIPPER is always used in conjunction with Method 1 and Max-Ent with Method 2, as described in Section 2.2."	neutral
train_3579	"Future work includes the investigation of such features, as well as the abstraction of lexical dependencies like semantic classes."	the reference distribution method achieved higher accuracy and lower cost.	neutral
train_3672	"Using the algorithm presented in Section 4, we induced ontological feature vectors for the noun nodes in WordNet using the lexical co-occurrence features from the TREC-2002 corpus."	these representations do not distinguish between the different senses of words.	neutral
train_3669	"For example, consider the example verb relation ""to surpass isstronger-than to hit"" from above."	"in this section, we provide a quantitative and qualitative evaluation of our framework."	neutral
train_3670	"If we had a large collection of sensetagged text, then we could extract disambiguated feature vectors by collecting co-occurrence features for each word sense."	"by design of the propagation algorithm, each concept node feature is shared by at least two of its children."	neutral
train_3668	"Given the enriched ontologies produced by our method, we believe that ontologizing lexicalsemantic resources will be feasible."	"when the recursion meets a non-leaf node, like chairwoman in Figure 2, the algorithm first recursively applies itself to each of the node's children."	neutral
train_5111	"Compared to the document-based HMM IE modelling, the extraction performance on location is significantly improved by our segment HMM IE system."	"it trains an HMM from labelled segments in texts, and then use the learned HMM to determine whether a segment is relevant or not with regard to a specific extraction task."	neutral
train_5488	"In (Wright et al., 1997) phrase level significance was obtained for noisy transcribed data where the phrases are clustered and combined into finite state machines."	"based on the hierarchical model described in Section 4 and topic identification mentioned in the last sub-section we describe  (1) a tool capable of aiding agents for efficient handling of calls to improve customer satisfaction as well as to reduce call handling time, (2) an administrative tool for agent appraisal and training."	neutral
train_5519	We first discuss several challenges to error-driven discriminative approaches.	we then present a novel way to parameterize and introduce learning into the initial phrase extraction process.	neutral
train_6861	Note that we use no pruning for these models and that the numbers of distinct n-grams is of the same order as that of the recently released Google Ngrams dataset (LDC2006T13).	"we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder."	neutral
train_10981	"The higher the BLEU score is, or the lower the TER score is, the better the translation quality is."	"the denominator is the sentence translation probability summing over all possible alignments, which can be calculated similar to IBM Model 1 in (Brown et al., 1994): X A0 P(A 0 , T|S) = Y J j=1 X I i=1 p(tj |si)."	neutral
train_15044	"The semantic frame structures thus obtained for the MT output are compared to those in the reference translations, frame by frame, argument by argument."	"instead, we aim for MT evaluation metrics that provide fine-grained scores in a way that also directly reflects interpretable insights on the strengths and weaknesses of MT systems rather than simply replicating human assessments."	neutral
train_15215	"In the graph-based method, we use an adjacency matrix to model the dialogue structure and utilize it to find salient utterances in conversations."	the similarity score is redundant here.	neutral
train_15314	"Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010;Blunsom et al., 2008a)."	"word association features are at the heart of all lexical translation models, whether generative or discriminative."	neutral
train_16918	"Firstly, we obtain an edit operation sequence using the normal DP for edit distance computation."	"japanese and foreign titles were split into chunks by middle dots and ""_"", respectively, and resulting chunks were aligned."	neutral
train_17326	Therefore we suggest that the recall of our method is close to the value of 0.459.	we have then counted how many of these 1350 errors could be found.	neutral
train_17746	In standard N-Gram models the bias feature corresponds to the unigram distribution.	"even the more scalable variant proposed by Mnih and Hinton (2008) is trained on a dataset consisting of only 14M words, also using a vocabulary of around 20000 words."	neutral
train_19400	"In this paper, our focus is to study how to represent temporal documents effectively for event detection."	"we used the event detection method in (Swan and Allan, 2000) as baseline, denoted as timeminesÏ‡ 2 ."	neutral
train_19401	"The most related work to ours is the boostVSM introduced by (He et al., 2007b), it proposes to weight different term dimensions with corresponding bursty scores."	"built on bursty features, our representation model can well adapt to text streams with complex trends, and therefore provides a more reasonable temporal document representation."	neutral
train_20216	"18.3% of the Scenes are Participants in another Scene, 11.4% are Elaborator Scenes and the remaining are Parallel Scenes."	such units are marked as Functions (F).	neutral
train_21900	"From this, we extract an old domain sense dictionary, using the Moses MT framework (Koehn et al., 2007)."	it is more difficult for our models to make good predictions for less frequent words.	neutral
train_22919	"Otherwise, ~ is reverse."	so we only use RsT rules to guide the reordering.	neutral
train_23929	"Then we perform a random walk algorithm on G tt , G oo and G to separately, to estimate all candidates' confidence, and the entries with higher confidence than a threshold are correspondingly extracted as opinion targets/words."	"based on different relation types, we used three matrices to record the association weights between any two vertices, respectively."	neutral
train_25929	"Also, for practical applications, we believe that it is useful to distinguish sentences with minor errors from those with major errors that may disrupt communication."	"we used the same learning algorithms as for our system (i.e., ridge regression for the ordinal task and logistic regression for the binary task)."	neutral
train_26243	"An important finding is that LIWC can be effectively used as a bridge for crosscultural classification, with results that are comparable to the use of unigrams, which suggests that such specialized lexicons can be used for cross-cultural or cross-lingual classification."	"these results are in line with previous research, which showed that LIWC word classes exhibit similar trends when distinguishing between deceptive and non-deceptive text (Newman et al., 2003)."	neutral
train_26425	"While, the General-domain baseline trained on 16 million sentence pairs has a hierarchical phrase table containing 1.7 billion translation rules."	"additionally, we adopt GIZa++ to get the word alignment of in-domain parallel data and form the word translation probability table."	neutral
train_27574	"Specifically, 69.3k (69.5%) of author-defined keyphrases (occurring in ACMCORPUS but not in AB-BREVCORPUS) are used as a keyword in only one paper."	"for example, Color Perception and Blood Cell both appear in ACM articles but are not abbreviated."	neutral
train_28866	"We have devised a new method that enables optimal search for the structured perceptron, but it cannot handle even modestly large feature templates."	the LF takes more time to calculate heuristics than the GP.	neutral
train_31955	"Due to the data sparsity issues (Bauer and Koller, 2010), we use unlexicalized probabilities."	"in the open action policy , we choose open actions according to their heuristic values, instead of just their tree probabilities."	neutral
train_32118	"This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations."	the equation 5 is intended to degrade long paths which pass through several high-valency nodes.	neutral
train_34070	The intuition behind the lexical contrast information in our new weight SA is as follows.	"due to interchangeable substitution, antonyms and synonyms often occur in similar contexts, which makes it challenging to automatically distinguish between them."	neutral
train_35079	"For our experiments, we have chosen the following affect categories -positive emotion, anger, sad, anxiety, and negative emotion (which is a superclass of anger, sad and anxiety)."	"Introduced by Pennebaker et al. (2001), LIWC is based on a dictionary, where each word is assigned to a predefined LIWC category."	neutral
train_36070	We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically.	context information in course videos provides important clues to infer prerequisite relations.	neutral
train_36080	Eisenstein and Barzilay (2008) discuss this within the context of topic segmentation.	their method is based on lexical cohesion -expressed in this context as topic segments having compact and consistent lexical distributions -and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment.	neutral
train_36266	"For web search results, we expect the documents that contain the correct answer a to be highly redundant, and therefore let each questionanswer-document tuple be an independent data point."	"2 we differ by setting D i as a set of documents, where previous work assumed a single document (Hermann et al., 2015) or even just a short paragraph (Rajpurkar et al., 2016)."	neutral
train_36265	"ten achieve near-human performance levels within months or a year, fueling a continual need to build ever more difficult datasets."	"we consider two types of distant supervision, depending on the source of our documents."	neutral
train_38457	"Clustering performs spectral graph clustering (Pedregosa et al., 2011), which represents commonly used clustering algorithms for event coreference resolution."	"we train our event extraction system and local coreference resolution classifier on 310 documents from the KBP 2015 corpus that consists of both discussion forum documents and news articles, tune the hyper-parameters corresponding to ILP using 50 news articles6 from the KBP 2015 corpus and evaluate our system on news articles from the official KBP 2016 and 2017 evaluation corpora7 respectively. "	neutral
train_38455	"shows that instead of human annotated event coreference relations, using system predicted relations resulted in a significant performance reduction in identifying the central event of a document."	we add the following objective function (equation 3) to the basic objective function and add the new constraint 4 in order to encourage coreferent event mentions to occur in topic transition sentences.	neutral
train_39135	"Recently, several related studies for language style transfer (Hu et al., 2017;Shen et al., 2017) have been proposed."	"following previous work (Shen et al., 2017;Hu et al., 2017), we instead use a stateof-the-art sentiment classifier (Vieira and Moura, 2017), called TextCNN, to automatically evaluate the transferred sentiment accuracy."	neutral
train_42974	This shows some potential positive link between the two levels of attention mechanism.	"for CMU-MOSEI dataset, as shown in Table 3, the accuracy of HffN is lower than that of BC-LSTM and CAT-LSTM, but it achieves the highest f1 s- core with slight margin."	neutral
train_42971	"For feature pre-extraction, our setting on CMU-MOSI and IEMOCAP datasets are identical to that in (Poria et al., 2017b)."	"despite the effectiveness this type of methods have achieved, they give little consideration to acknowledging the variations across different portions of a feature vector which may contain disparate aspects of information and thus fail to render the fusion procedure more specialized."	neutral
train_46119	It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift.	"by combining both bT and DALI, the output becomes fluent and also contains correctly translated in-domain keywords of the sentence."	neutral
train_46168	"But in simultaneous translation, we need to translate concurrently with the (growing) source sentence, so we design a new prefix-to-prefix architecture to (be trained to) predict using a source prefix."	"as a very simple example within the prefix-toprefix framework, we present a wait-k policy, which first wait k source words, and then translates concurrently with the rest of source sentence, i.e., the output is always k words behind the input."	neutral
train_47695	The coreference resolver of Peng et al.  (2015) has the smallest difference between its maximum and minimum span evaluation scores.	the use of minimum spans in coreference evaluation decreases the effect of mention boundary detection errors in coreference evaluation and results in fairer comparisons.	neutral
train_48619	"Our language difficulty estimates were largely stable across datasets and language model architectures, but they were not significantly predicted by linguistic factors."	"we compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Dependencies 2.0 treebanks (Straka and StrakovÃ¡, 2017)."	neutral
train_49089	PFAs have some conditions on their structure.	"building off of our previous work, we have considered the problem of incrementally computing the infix probabilities of each prefix of a given string."	neutral
train_50237	"Informed prior methods also excel at refinements that promote topic words, such as add word and create topic."	"in such cases, control may not be a desired property of interactive systems."	neutral
train_51331	"Then he suggests that ""readers who have not had a course in logic or who would like a quick review should read [Appendix A.1]"" (p. 11)."	"this is illustrated by a birthday-party context, which has several levels of nested conceptual graphs, and is now treated as a graphical user interface: ""At the bottom of the box in Figure 3.5 is another concept [Process]."	neutral
train_52035	"It is unlikely that someone would describe an object as ""the dog next to the tree in front of the garage"" in a situation in which ""the dog in front of the garage"" would suffice."	"in general, there is no a priori reason to assume that our scene representations will be planar."	neutral
train_53121	"They consider Pustejovsky ''as a representationalist, antiformalist ally,'' but they never explain why they consider lexical semantics incompatible with formal semantics."	"the body of a theory ''is a set of its statements, variously referred to as laws, propositions, regularities, theorems, or rules. ''"	neutral
train_53264	Some proponents of tree structures assume that trees are easier to formalize and to derive than less constrained graphs (Marcu 2000;Webber et al.	correlation between percentage of removed coherence relations and mean in-degree of remaining nodes.	neutral
train_53268	Each set of statistics was calculated for both annotators separately.	"table 11 shows the data from Figure 17, ranked by the factor of ""percentage of overall coherence relations"" by ""percentage of coherence relations ingoing to nodes with multiple parents."""	neutral
train_53262	Section 4.1.4 provides a short summary of the statistical results on crossed dependencies.	"that percentage is reduced even further, to 0.84%, by removing all elaboration and similarity relations from the database."	neutral
train_53734	First a corpus is prepared and patterns from clusters C1-C3 are identified.	"the SemCor patterns thus extracted did not need manual validation, since the noun concept pairs were always in a partwhole relation."	neutral
train_54939	"Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin 2004) or NLP texts (e.g., Manning and SchÃ¼tze 1999; Jurafsky and Martin 2000)."	"in addition, i like the organization of the book."	neutral
train_54940	The remaining chapters focus roughly on six types of semi-supervised learning methods: 2 r Self-training.	"1 to learn about semi-supervised learning research, one has to consult the machine-learning literature."	neutral
train_55549	"Following these observations, we developed a bootstrapping formula that improves the original feature weights (Section 4), leading to better feature vectors and better similarity predictions."	"there are only two common features (among the top 100 features) for the incorrect pair country-party, both with quite low ranks (compare with Table 7), while the rest of the common features for this pair did not pass the top 100 cutoff."	neutral
train_55542	"Recall from Section 2.2 that an active feature is a feature that is strongly associated with the word, that is, its (initial) weight is higher than an empirically predefined threshold, Î¸ weight ."	"in the current work we use Mi for data analysis, and for the evaluations of vector quality and word similarity performance."	neutral
train_55543	"A demonstration of such potential appears in Geffet and Dagan (2005), which presents a novel feature inclusion scheme for vector comparison."	"similarity measures which have been used in the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992), and various information theoretic measures, as introduced and reviewed in Lee (1997, 1999)."	neutral
train_58000	"Presumably, this is because of its lower prediction accuracy."	cATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations.	neutral
train_58188	"The book is unusually clear and honest in highlighting limitations in the current understanding of crucially relevant concepts, including existing formalization techniques."	"interpreting motion, therefore, turns out to be one of the most fundamental research issues for a variety of purposes both in basic (or cognitive) and applied (or computational) research."	neutral
train_58301	"In the PropBank training corpus, however, the argument following the verb fail is labeled Arg2 in 79% of the cases."	the results of the SP models in laboratory conditions are presented in Section 4.	neutral
train_61354	"After an introduction to evaluation and subjectivity in linguistics, we now present research in computational linguistics, starting with a basic problem definition and a common characterization of evaluative language."	"in what Biber and Finegan call Faceless texts, a high proportion of evaluative expressions is more significant, and is probably indicative of an overtly subjective text (i.e., one with a higher ""volume,"" from the point of view of evaluative language)."	neutral
train_61353	Not enough research has explored exactly how evaluative expressions are affected in the presence of nonveridical operators.	the outcomes did not result in a significant improvement.	neutral
train_62226	Our model outperforms other unsupervised models in Morpho Challenge 2010 with a F-measure 50.71%.	"because of the difficulty in measuring the quality of hierarchical paradigms, which will require a corresponding hierarchically organized gold data set, we are unable to provide an objective measure of the quality of hierarchical structures learned."	neutral
train_62244	"It's hard for me to overstate how profoundly Aravind influenced my career and my life, since he took me on as his PhD student 30 years ago."	"aravind described parentheticals, epithets, extraposed predicates, and sentential relatives as ""constructions that require a skilled tree surgeon to force a single tree over a sentence."	neutral
train_66612	We also implement the NCM model under DOM for reference.	we evaluate our method through several experiments for two language pairs: English/Chinese and English/Japanese.	neutral
train_67338	"This allows us to incorporate descriptions of objects to be excluded, to produce enumerations and compositions of descriptions of subsets of the intended referents, and to build compositions of increasingly restricting descriptions of these referents."	"we have enhanced this repertoire by a complexity cut-off, carried out prior to further expanding a node if the boolean combination of descriptors build leads to a description that is more complex than a given threshold."	neutral
train_67555	In these examples the angles are actually the elements of the linear pair.	"some cases, like that of passive versus active constructs, can be taken care of in the grammar."	neutral
train_67995	We treat all N-best phrase alignments equally.	"the accuracy of ""lex"" drops rapidly as the number of conditioning factors increases."	neutral
train_69218	Table 1 evaluates the contributions of different kinds of constituent dependencies to extraction performance on the 7 relation types of the ACE RDC 2004 corpus using the convolution parse tree kernel as depicted in Figure 1.	the key problem of how to represent the structured syntactic parse tree is still partially resolved.	neutral
train_71691	"Let be the set of preference judgments, where x i and x i ' are the feature vectors for two pairs of surface-forms and Wikipedia-entry, l(x i ) and l(x i ') are their absolute judgments respectively."	in our experiments string similarity was based on word overlap.	neutral
train_72197	"For example, synonym substitution, paraphrasing and translation can be applied to an embedding method which reconstructs the secret message as concatenating codewords that are directly associated with a choice."	"we have ignored the possibility of computational steganalysis and steganographic attacks, such as detecting, extracting and destroying the hidden message (Fridrich, 2009)."	neutral
train_73915	"The problem can become intractable even on the span of short sentences: a linguistically motivated context, such as a paragraph for instance, can not be handled."	"the definition of a sense j of word i is noted d(w i, j )."	neutral
train_74329	how many arguments are shared by the two predicates (NUM_SHARED_ARGS)?	"(2010) consider only entailment and non-entailment, while our proposed model identifies a rich set of relations: paraphrase, forward entailment, backward entailment, contradiction, and independence."	neutral
train_76493	We conducted our experiments on the multilingual GEOQUERY dataset released by Jones et al. (2012b).	"currently, researchers only focused on the semantic parsing task under a single language setting where the input is a sentence from one particular language."	neutral
train_76494	"The model parameters will then be estimated using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977)."	one crucial fact is that many such news articles written in different languages are actually all discussing the same underlying story and therefore convey similar or identical semantic information.	neutral
train_76911	"Consider the sentence ""The boy is happy and joyful""."	"the number of positive labels is substantially smaller than the number of negative labels, 14 and thus this measure can be manipulated: a dummy model that always assigns the negative label gets a high accuracy."	neutral
train_78192	"This particularity can be exploited in speech synthesis to model additional information in the speaker models such as speaking styles or emotions (Nose and Kobayashi, 2012; Ling et al., 2013), and it has been used for applications as varied as generating walking motion models (Niwase et al., 2005)."	"in real life expressiveness present overlap between them because there are not clear frontiers, so being able to take into account those overlaps in the shape of a dynamic synthesizer would be ideal."	neutral
train_81083	"For instance, the substitution of u for v would not work for sentences that have negations or quantifiers such as 'all' and 'none'."	"‘Verb’ refers to a non-compositional baseline, where the vector/tensor of the phrase is taken to be the vector/tensor of the head verb. , + refer to vector element-wise multiplication and addition, respectively, âŠ— proj to the projective tensor models of Section 6.3, and âŠ— rel/frob to the construction of Section 6.1."	neutral
train_82682	"Particularly, we introduce attention mechanism to robustly model local contextual information by selecting informative words and filtering out the noise."	"we set context window to 20, neighbor mention window to 6, and top n = 10 candidates for each mention."	neutral
train_83243	We can see that the average compression ratios of both our model and the other two baselines are similar.	the model was trained on a set of 2 millions sentence pairs which was constructed by the same approach used in Filippova and Altun (2013).	neutral
train_83492	"In recent years, users have come to prefer presenting more attractive ingredients, such as image and audio in addition to natural words."	"dCRNN consists of five phases, as illustrated in Fig. 4."	neutral
train_83859	"In this setting, we keep the automated feature extraction and the fakeness discrimination components."	these observations suggest that the proposed framework can effectively combine multiple sources.	neutral
train_85735	The development of machine translation more often uses statistical approach because it requires very limited computational linguistic resources compared to the rule-based approach.	"as there were no such corpora for SMT experiments, the researchers had to prepare small size corpora."	neutral
train_85822	Simpler output layer (SimplerOut).	we report our experimental results both with and without layer normalization.	neutral
train_85821	We aim to shed light on the potential bottlenecks involved in re-purposing a complex deep learning sequence modeling architecture for computational linguistics studies.	they do not make the learned linguistic structure explicit: rather it can be presumed to be cryptically encoded in the states of the hidden layers.	neutral
train_86000	Further reducing sequences' length to five tweets leads to worse classification performance.	mSHmm were introduced by Tokuda et al. (2002) and originally used in speech synthesis tasks.	neutral
train_86263	We can also find that LSTM-CSS and LSTM-CSS-E nearly perform equally.	"the motion stream is a reproduced model of tSN, fine-tuned over MSR-Vtt-2016."	neutral
train_86262	"object) feature embedding for a video; 2) Sentence generation network (Section 2.2), which uses CRF to learn optimal semantic representation and LSTM to generate the final description."	"in this example, there are no reference captions that have same syntactic structure with our caption."	neutral
train_86454	"About half of all texts begin with the thesis, whereas pro units rather come at the end."	"we kept only those with the majority stance on the topic, resulting in at least four theses for each of the 10 topics."	neutral
train_86449	"Selecting content in terms of argumentative discourse units (along with facts, anecdotes, and similar) that frame the given topic in a way that is effective for the intended stance, 2. arranging the structure of the units considering ordering preferences, and 3. phrasing the style of the resulting text to match the genre and the encoded means of persuasion."	table 6: Differences between strategies in the relative frequency and the rank of the three most common unit type flows in the synthesized arguments.	neutral
train_88072	Marcu (1999) uses a decision-tree learner and shallow syntactic features to create classifiers for discourse segmentation and for identifying rhetorical relations.	it is straightforward to determine the labels of all the rhetorical relations from these conditions.	neutral
train_88534	"Only a small improvement in labeled attachment score was noticed using the full, nonspecialized classifier to decide the action but discarding its suggestion for label and using a specialized classifier for labeling."	"recently statistical dependency parsing techniques have been proposed which are deterministic and/or linear (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004)."	neutral
train_88971	The reliance on an explicit alignment and hand-chosen decoys yields a somewhat less flexible solution than that presented here.	"the corresponding weight in the ""h"" interpretation is inhibitory."	neutral
train_89104	The undergraduate students were raised in Hong Kong and the research student in Mainland China.	"while human judgement is not straightforward and it is difficult to create a PanChinese lexicon manually, it is observed that combining simple clustering methods with the appropriate data sources appears to be a promising approach toward its automatic construction. "	neutral
train_90661	"Despite the simplicity of this approach, our system has produced promising results."	"we specify the first word in a word pair as a predicate candidate (i.e., a semantic head, and noted as p in our feature representation) and the next as an argument candidate (i.e., a semantic dependent, and noted as a)."	neutral
train_94924	"Some countries are having difficulties in managing a place to live for their *citizen/citizens as they tend to get overpopulated."""	we found that training an AP model on the ESL training data with more sophisticated features is not as effective as training on a native English dataset of larger size.	neutral
train_95247	"On November 18, 2006, Holmes and Cruise were married in Bracciano, Italy, in a Scientology ceremony attended by many Hollywood stars."	all these efforts focus on temporal ordering of either events or states of the world and do not extract timestamps for events.	neutral
train_95737	"The authors make use of the observation that morphologically related adjectives exist for PT-verbs, unlike for AG-and SP-verbs."	"on MPQA, only the new induction approach and the lexicons significantly improve CK."	neutral
train_98809	"Nevertheless, a recent comparison of all submitted approaches to the cross-domain authorship attribution task at PAN 1 indicates that deep learning is currently not able to surpass traditional methods ."	"note that it is a common procedure to fix training and test documents in order to ensure reproducibility (Stamatatos et al., 2018)."	neutral
train_99896	"Some speakers believed structural marking also to be useful in the HCI situation, for example."	they concern the preconditions for formulating utterances particularly for the respective hearer.	neutral
train_102963	The work presented here complements them with additional semantic relations.	"in (9), the analysts 'forecasting' and the company 'saying' do not have as their cause 'planned price cuts'."	neutral
train_103600	"As this leaves no unspecified food items in our vocabulary, we cannot use the output of HEUR as seeds for graph-based optimization."	we train a supervised classifier and incorporate the knowledge induced from our domain-specific corpus as features.	neutral
train_109251	We have demonstrated that it is possible to learn a statistical model of verb semantic argument structure directly from unannotated text.	"our goal is to learn a model which relates a verb, its semantic roles, and their possible syntactic realizations."	neutral
train_109527	"Figure 5 (right) investigates the relationship between the search effort and BLEU score for A* and bottom-up CYK parsing, both with pruning."	"in Figure 1(b), there is one solid cell (corresponding to the Model 1 Viterbi alignment) in each column, falling either in the upper or lower outside shaded corner."	neutral
train_110478	"Next, we can compare each All row with the Rel row immediately below it."	we ran this selftraining procedure for three iterations and then used the resulting classifier as our relevant sentence classifier in the IE experiments described in Section 6.3.	neutral
train_110612	"To accomplish this, one of the original candidate translations, e.g. em, is chosen as the primary translation hypothesis, while all other candidates en pn  mq are aligned with the word sequence of the primary translation."	"in a first experiment, we trained approximately 200 systems using different parameter settings in training."	neutral
train_111171	"Previous phrase alignment work has primarily mitigated this tendency by constraining the inference procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007;Zhang et al., 2008)."	"similarly, we can parameterize the Dirichlet process with a concentration parameter Î± (that affects only the variance) and a base distribution M 0 that determines the mean of the samples."	neutral
train_111173	We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment.	"a generative model that explicitly aligns pairs of phrases e, f gives us well-founded alternatives for estimating phrase pair scores."	neutral
train_111833	"In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been widely used in clustering words (Wang and Vergyri, 2006;Maltese et al., 2001; Samuelsson and Reichl, 1999)."	"both Chinese characters and words are considered as model units in this study, and a word re-segmentation process on recognition hypotheses is necessary, where an n-gram language model based on word classes is adopted."	neutral
train_112059	We propose a novel approach that determines a list of stop words directly from word distribution in each data set.	"predicting the boundaries of fine-grained subtasks is more difficult even with a supervised learning approach (Arguello and RosÃ©, 2006)."	neutral
train_112159	"Given these definitions, we see that the (M T M ) n term of smaller n corresponds to lower-order cooccurrence, which is accurate but sparse, and the (M T M ) n term of larger n corresponds to higherorder co-occurrence, which is dense but possibly giving too much weight on unrelated instances extracted by generic patterns."	the proposed methods outperform by a large margin the most frequent sense baseline and both Simplified-and Filtered Espresso.	neutral
train_112360	"It turns out that these semirings can also compute first-and second-order partial derivatives of all the above results, with respect to a parameter vector Î¸ âˆˆ R m ."	the root node corresponds to the goal item.	neutral
train_112714	Wikipedia structure and quality make this resource particularly suitable for information extraction and word sense disambiguation tasks (Csomai and Mihalcea (2008) and Milne and Witten (2008)).	"the sets of Italian sentences extracted for every (F, l), i.e. or every Wikipedia article, were much smaller, so we increased the number of randomly chosen (F, l) pairs to 80. "	neutral
train_115897	"Further, the method we describe in Cholakov and van Noord (2009) is applied to generate the paradigm(s) of each word in question."	"because of data sparseness problems, this method is only able to handle unigrams and bigrams."	neutral
train_116796	"In a normal regression analysis, one must either assume that participants or the particular choice of items add some randomness to the experiment, and either each participant's responses for all items must be averaged (treating participants as a random factor), or all participant's responses for each item is averaged (treating items as a random factor)."	the vocabulary of hidden states is described in Section 2.1 and the emission distribution in Section 2.2 	neutral
train_116799	"Prediction refers to the fact that the human sentence processor is able to anticipate upcoming material, and that processing is facilitated when predictions turn out to be correct (evidenced, e.g., by shorter reading times on the predicted word or phrase)."	we did not trim any items due to abnormally short or abnormally long fixation durations.	neutral
train_117295	For AAC devices this means closely modeling everyday face-to-face communications.	our data does not contain long-term two-sided conversations.	neutral
train_117421	Collective disambiguation works very well when a text contains mentions of a sufficiently large number of entities within a thematically homogeneous context.	this constraint may lead to very suboptimal results.	neutral
train_118499	"On the other hand, any word w grammatically related to the PMW via a grammatical relation r provides us with semantic restrictions on the interpretation of the PMW, namely preferred semantic classes A j (we call them abstractions) and a selectional preference score."	this ranking showcases our second major innovation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches.	neutral
train_119650	Also using fewer question classes results in a more accurate multi-class classifier.	"their extraction modules basically assume the shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy."	neutral
train_119908	"In our formulation of CDP, correctness of an expanded pair is judged according to the pair's SVM score using BASE."	"n , the number of newly added positive training samples during the training data expansion process, was set to 6,000 according to preliminary experiments using the development set."	neutral
train_120609	"Passive-to-active voice transformation in English can be performed systematically, which does not depend on lexical information in most cases."	we basically used the same data as Murata et al.	neutral
train_120745	Belkin et al. (2006) and Weinberger et al. (2007) are among the first to investigate graph Laplacians as a manifold regularization method for statistical learning.	these features stopped to show up on the list of Dunkin' Donuts.	neutral
train_121116	"In order to induce initial translation pairs, we rely on the framework of multilingual probabilistic topic modeling (MuPTM) (Boyd-Graber and Blei, 2009;De Smet and Moens, 2009;Mimno et al., 2009;Zhang et al., 2010), that does not require a bilingual lexicon, it operates with nonparallel data, and is able to produce highly confident translation pairs for high-frequency words (Mimno et al., 2009;VuliÄ‡ and Moens, 2013)."	"results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons."	neutral
train_121530	It is however noteworthy that we can observe some interpretable selectivity in lobemodel combinations.	"5 Once both the textual and the visual models are built, we perform two different transformations on the raw co-occurrence counts."	neutral
train_122170	The distribution of a word over these soft clusters assignments was added as features to their classifier.	"features based on distributional representations (e.g., raw co-occurrence frequencies) can be computed for every word, given that it occurs in some unlabelled corpus."	neutral
train_123235	"In this paper, we conduct the experiments on English-Chinese parallel page pair mining."	a set of English pages and a set of Chinese pages are obtained.	neutral
train_124603	"For efficient training and decoding, we add a restriction to DGSG: each dependency-graph fragment covers a continuous span of the source sentence."	"the translation process is rather complex and the resources it relies on, namely abstract meaning corpora, are limited as well."	neutral
train_124764	We have used the 200 test documents that are manually summarized as gold standard data for ROUGE evaluation.	the main reason being that the functions with optimal values of trade-off parameter Î± strike out a balance between relevance and subjectivity.	neutral
train_124793	Then it is normalized as input into PSL.	the second and third evaluations are carried out on the entire corpus.	neutral
train_124787	"Further, our previous models all require certain manual (oracle) annotations to be input."	"aGENT(x,a) and THEME(x,h): We consider all nouns in the same or in sibling constituents of a +/-effect event as potential agents or themes."	neutral
train_126814	The same test confirmed that there are significant differences in the performance of the best model Wmask when using representations of different sizes (p < 0.01).	"the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences."	neutral
train_128799	# of coherent topics: Figure 2 (rightmost chart) shows the number of coherent topics produced by each model.	all the above works focus on generic subjective expressions as opposed to aspect specific opinionsentiment phrases.	neutral
train_128809	"As mentioned in section 1, some emotions often co-occur such as joy and love, and some rarely co-exist such as joy and anger."	"dimensional approaches (Russell, 2003) emphasize the fundamental dimensions of valence and arousal in understanding emotional experience, which have long been studied by emotion theorists."	neutral
train_132654	"While some work (Luong et al., 2015b; Luong and Manning, 2016; Wu et al., 2016) has achieved state-of-the-art results using deep networks, others (Jean et al., 2015; Chung et al., 2016; Sennrich et al., 2016b) have produced similar results with far shallower ones."	we hope that this allows us to isolate the effect of various hyperparameter changes.	neutral
train_132673	"These attractive properties were not studied in A2C's original paper (Mnih et al., 2016)."	"in any realistic scenario, human feedback will vary from its average, and so the reward that our algorithm receives will be a perturbed variant of sentence-BLEU."	neutral
train_132671	The encoder and the decoder are unidirectional single-layer LSTMs.	"as more variance is injected, the models degrade quickly but still improve from the pre-trained models."	neutral
train_133450	The task of evaluating lyric annotations was difficult for CrowdFlower collaborators as was apparent from their evaluation of the task.	in ALA the system must often include additional information to clarify niche terminology and abstract concepts.	neutral
train_134393	"These aim to capture the importance associated with deletion, insertion or modification of specific POS tags and Named Entities."	"in our work, we focus on automatically inferring the impact/change introduced by edits, and predict the perceived importance of such edits by authors."	neutral
train_135183	Cross-lingual NER attempts to address this challenge by transferring knowledge from a high-resource source language with abundant entity labels to a low-resource target language with few or no labels.	these approaches utilize such features by training a model on the source language and directly applying it to the target language.	neutral
train_135825	Table 1 shows the results.	"we insert words only before-not after-a position, since an extra word after the ending word (usually a punctuation) is not probable."	neutral
train_137876	"In addition, an ablation study shows that multi-tasking the PRIMARY tasks is beneficial over a single task setting, which in turn is outperformed by the inclusion of the AUXILIARY tasks."	we report the performance of all source-target combinations in Table 6.	neutral
train_139639	"We consider two state-of-the-art CWI systems: (i) the nearest centroid classifier proposed in (Yimam et al., 2017), which uses phrase length, number of senses, POS tags, word2vec cosine similarities, ngram frequency in Simple Wikipedia corpus and Google 1T corpus as features; and (ii) SV000gg (Paetzold and Specia, 2016b) which is an ensemble of binary classifiers trained with a combination of lexical, morphological, collocational, and semantic features."	"these assumptions are not always accurate and are often the major source of errors in the simplification pipeline (Shardlow, 2014)."	neutral
train_140982	We note that the availability of document-level context still has a strong impact in the fluency condition (Section 3).	we thank Xin Sennrich for her help with the analysis of translation errors.	neutral
train_141140	"For the research presented in this paper, we choose two related and relatively simple neural network architectures corresponding roughly to Parikh et al. (2016) and a simplified version of Chen et al. (2017) consisting of five components."	"as neural network approaches increasingly dominate in performance across many NLP tasks, the notion of random shuffling has become overshadowed by that of computational efficiency."	neutral
train_143593	"For the NLU ZSDG setup, we annotated all available SMD data and randomly selected a subset of 1000 utterances from each source domain, and 200 utterances from the target domain."	"the ZSDG setup used approximately 150 training utterance-annotation pairs for each domain, including the target one, totalling about 450 annotated utterances."	neutral
train_145609	"Generally, the generation task need to focus on a wider range of input words than the classification tasks."	job seekers can easily retrieve the job advertisements they desire by reading them.	neutral
train_146152	PaLM provides an intuitive way to inject structural inductive bias into the language model-by supervising the attention distribution.	their contribution is orthogonal to ours and not head-to-head comparable to the models in the table.	neutral
train_146153	We observe no significant trend of favoring one branching direction over the other.	"meaningful span representations are crucial in span-based tasks (Lee et al., 2017;Peng et al., 2018c;Swayamdipta et al., 2018, interalia)."	neutral
train_146674	"We analyze three algorithmic factors of relevance to sampling bias: (a) Initial set selection (b) Query size, and, (c) Query strategy."	"it seems to have a inductive bias for class boundaries, similar to the above works."	neutral
train_146668	"We thank Prof. Vineeth Balasubramium, IIT Hyderabad, India for the many helpful suggestions and discussions."	"diversity-based query strategies (Sener and Savarese, 2018) are used to address this issue, by selecting a representative subset of the data."	neutral
train_151000	"Furthermore, most experimental methods are only suitable for one word translation, i.e. the word number ratio of translation pair is on a basis of 1:1."	1) Subset redundancy information.	neutral
train_151003	"The reasons for this strategy are as follows: 1) terminology seldom consists of rare words out of GB2312, 2) the index space is dramatically reduced using GB2312 rather than the Unicode encoding so as to quicken the estimation speed."	the issue and the proposed method in this paper are distinctly different with Nagata's.	neutral
train_152049	"According to a previous study, the minimum error rate training (MERT) (Och, 2003), which is the optimization of feature weights by maximizing the BLEU score on the development set, can improve the performance of a system."	"parallel corpus is one of the most important components in statistical machine translation (SMT), and there are two main factors contributing to its performance."	neutral
train_152291	"In doing so, a good accuracy obtained on the classification task implies that the extracted features capture those aspects of the language that a trigram model may not."	the coherence score for an article is normalized by the total number of content-word pairs found in the article.	neutral
train_153401	All pairs knew each other previously and were of the same gender and approximately the same age.	"the task becomes more complicated than typical coreference resolution for written texts because a referent is considered as either anaphoric (i.e. it has already appeared in the previous discourse history) or exophoric, (i.e. the reference resolution system needs to search for the referent from the set of objects shown in a computer display)."	neutral
train_154394	"Therefore, if extending WordNet with the large amounts of semantic relations contained in ConceptNet, it is desirable to improve the performances of WordNet-based WSD methods."	"for ambiguous concepts with multiple senses, there are multiple nodes in the network."	neutral
train_155958	Identifying the complex predicates has turned to be rewarding.	"the semantics of light verbs is, however, kept as such."	neutral
train_155960	"In the first experiment, the first sense of each lexical item is selected while in the second, WSD is used to pick the contextually most appropriate sense."	"to identify the temporal sense of a numeral, used as nominal, like 2013 is challenging."	neutral
train_156735	"When the number of instances is large enough, the statistical model will effectively incorporate these entity type constraints as long as entity types are extracted as features."	we can consider further efforts to enlarge and balance the initial set from the view of non-relation approximation.	neutral
train_156856	The line CH in Figure 1 shows the definition of chunks.	table 7 shows the error changes when different features are added.	neutral
train_158191	"The pairs in both datasets were then rated for their plausibility by 27 human subjects, and their judgements were aggregated into a gold standard."	the dataset consists of 52 verb metaphors and their human-produced literal paraphrases.	neutral
train_158268	"If the feature set is too small, it might underfit the model and leads to low performance."	table 4 presents results of different experimental configurations for English.	neutral
train_158773	"In all of these cases, vocabulary does not factor into the differences, since we are at the POS level."	"these models perform better on texts written by certain people, namely those whose language is closer to the training data."	neutral
train_160172	"For example, Sasaki et al. (2013) proposed a character-level sequential labeling method for normalization."	rule-based means the conventional rulebased method proposed by Sasano et al. (2013).	neutral
train_160310	"Therefore, such bounds should be used to decide the number of dimensions, instead of trial and error."	"in this paper, we show that the dimension should instead be chosen based on corpus statistics."	neutral
train_161153	"Although the first one has proven to be beneficial during pre-training to some NLP tasks, we want to check how much its influence is to our final translation performance."	"as mentioned in Section 3.4, it is interesting to explore the different usage of pre-trained decoders in the MT task."	neutral
train_161574	"All bold-face results are statistically significant to p < 0.01. that MGT results in more general representations of language, thereby facilitating better transfer."	"across both base architectures, MGT outperforms ensembling."	neutral
train_161757	The results indicate that the BiLSTM-CRF framework is a better fit for encoding order information and long-range context dependency for such sequence labeling task.	"in sentence S1, the negative focus is the propositional clause until the market close, yielding the interpretation that mutual fund trades take effect, but not until the market close."	neutral
train_162864	"True, False, and Uncertain are the inference results for whether the symptom exists in the patient."	these observations have verified the effectiveness of modeling the associations between symptoms via graphs for symptom inference.	neutral
train_163057	BERT-AVG uses the average of the sentence representations to train a linear classifier.	sentiment features can be propagated recursively from the leaf nodes to the root node.	neutral
train_164309	"Entity Linking (EL), such as the EL track at NIST Text Analysis Conference Knowledge Base Population (TAC-KBP), aims to link a given named entity mention from a source document to an existing Knowledge Base (KB) (Ji et al., 2014)."	"every vertex (m, c) has an initial similarity score iSim(m, c) between m and c."	neutral
train_164491	"The proposed approach applies the Lexicon-Grammar (LG) framework and its language formalization methodologies, developed by Maurice Gross during the '60s."	the previous automaton may process a query as the following one: (1) Tutti gli archeologi che sono stati anche scrittori nati nel '900 (All the archaeologists who have been also writers and were born in 19th century) 3 .	neutral
train_165042	During annotation we achieved an interannotator agreement of 0.72 (Fleiss' Îº).	"for each approach the number applications and reviews used as well as the app store (Apple App Store (A), Google Play Store (G) or BlackBerry World (B)) they originate from are given."	neutral
train_165072	"In terms of finding which tweets are opinionated, it scores high on Recall but low on Precision overall (i.e. it overclassifies many tweets as opinionated)."	"The combined score is simply the sum of the positive and negative scores, e.g. a positive score of +2 and a negative score of -1 would have a combined score of +1."	neutral
train_165074	"This has been rarely addressed for sentiment analysis, due to the lack of available training data and the difficulty of manual annotation."	"fair evaluation of such systems is fraught with difficulty, partly because the task is often subjective (human annotators do not agree on what is correct), and partly because comparing systems fairly is complicated when they tackle the problem in different ways."	neutral
train_165572	"We test our tagger on Slovene data, obtaining a 25% error reduction of the best previous results both on known and unknown words."	"to most other taggers of highly inflected languages, tagging of unknown words is directly incorporated into the tagger architecture, rather than being handled by an additional process."	neutral
train_166131	"Another field of application is comparison and alignment of annotations produced by different human or machine transcribers for the same speech data, e.g.generated poems seem more independent. (Ãlvarez et al., 2014)."	"a preliminary experiment showed that basic Levenshtein algorithm produces 1.1 % of errors, that is almost 12,000 sounds for our 30 hours of experimental speech data."	neutral
train_166857	"In the Semantic Web, several different URI references can refer to the same entity and the ability to identify equivalent entities is crucial for Linked Data."	we consider our method applicable to any type of resources.	neutral
train_168757	"It also indicates that the granularity of the scope of ATTRIBUTE relation is wider than that of others, i.e., properties can further be broken into several subtypes."	"instead of precisely defining a frameset for in domain events, we describe the roles of entities in the form of their mutual relations using a set of general relationships such as method-purpose, system-output, and evaluationresult."	neutral
train_168933	The H words left and right of w i are extracted for every instance of w i in the corpus formulating a feature vector.	3) Every image is represented as a vector of visual words.	neutral
train_169488	"In this subsection, we provide a brief overview of two joint inference frameworks, ILP and MLNs."	an opinion candidate can be related to source and target arguments if it is not an argument-implicit opinion.	neutral
train_169499	"To verify the usefulness of our corpora we built machine-learning models using an exhaustive list of features, and while utilizing the annotated corpora for training and testing."	we collected at least 4 annotations from different annotators for each blog.	neutral
train_170466	The answer isn't trivial.	we only annotate referential bridging.	neutral
train_172156	The second one consists of asking a human to correct the output of the first part.	there are two possible strategies to measure the action duration: 1. using the average of time of annotators having the same experience; 2. using the average of time of annotators having several experiences.	neutral
train_172419	"Nonetheless, the selection of the appropriate root is necessarily somewhat subjective and remains a source of disagreement throughout other annotations as well."	representing the meanings associated with fully syntactic patterns required a novel annotation approach.	neutral
train_173831	Texts are short and include many references to concepts that have a specific terminology.	average interannotator agreement across all categories is 0.68.	neutral
train_173830	The ID is the unique identifier within the corpus and the source ID is the message identifier of the microblog platform.	the data were then passed through further content filtering operations (to exclude spam and unreliable authors) which are described below.	neutral
train_174060	"The formant tracker PRAAT showed systematic errors with male speakers, while SNACK and ASSP performed better on male than on female speakers."	"this interpolation facilitates an evaluation of formant tracker output independently of the voiced-unvoiced detection of the tracker algorithm (because for every time frame of the recording there exists a reference value); on the other hand the resulting quality measure might be compromised: if a tracker is 'conservative' in the sense that it produces output only for the parts of the input signal where it is quite confident (clearly voiced parts), then this tracker will outperform other trackers who produce results in parts of the signal where the tracking is compromised for instance by creaky voice or noise associated with consonantal constrictions etc."	neutral
train_174187	They represent moreover the most difficult part of the system to be developed.	"this platform, as described in Section 4, is semiautonomous: some modules of the architecture are simulated by an experimenter."	neutral
train_176621	"From the word similarity point of view, the degree of similarity for å¢¨é¡""sunglasses"" and å°ˆåˆ©""patent"" should not be high no matter which ontology is applied, for the former is a concrete object but the latter is an abstract one."	"that means whenever a event ""buy"" occurs, that imply the event ""pay (money)"" happened as well."	neutral
train_177833	"The second component (the upper one) consists of ""grammar"", ""readability"", and ""verbose and conciseness""."	"at this point, we are able to produce good prediction of several aspects of information quality, including Depth, Objectivity, Multi-view, and Readability."	neutral
train_178044	In addition it is possible to find large amounts of text with similar content in order to build better language models and enhance retrieval through use of similar documents.	the second method can only generate phone strings that are substrings of the pronunciations of in-vocabulary word strings.	neutral
train_178629	Recognition of tone and intonation is essential for speech recognition and language understanding.	"since these approaches are integrated with HMM speech recognition models, standard HMM training procedures which rely upon large labeled training sets are used for tone recognition as well."	neutral
train_181312	"Arguably MUC-6 and MUC-7 should not count as separate domains, but because they were annotated separately, for different shared tasks, we chose to treat them as such, and feel that our experimental results justify the distinction."	"when a domain lacks evidence for a parameter the opposite occurs, and the prior (whose value is determined by evidence in the other domains) will have a greater effect on the parameter value."	neutral
train_181875	"To illustrate the trade-offs in speed vs. accuracy that can be achieved by varying the two pruning parameters, we sweep through different values for the parameters and measure decoding accuracy, reported as word error rate (WER), and decoding speed, reported as times faster than real time (xfRT)."	"the first step in converting speech to a searchable index involves the use of an ASR system that produces word, word-fragment or phonetic transcripts."	neutral
train_183177	Four judges (graduate students) were used.	reasonable machine-generated sentence compressions can often be obtained by only removing words.	neutral
train_183328	"The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007)."	"in case of Wordnet, the validity of the kernel will depend of the kind of similarity used."	neutral
train_191164	Our RNN units use a different gate arrangement than that used by RAN.	"following Williams et al. (2017), we construct a templatized set of responses."	neutral
train_195395	"Since, the local normalizer is easy to compute, likelihood maximization based training is a standard approach for training these models."	we demonstrate the effect of both sources of label bias through our experiments on two common sequence tasks: CCG supertagging and machine translation.	neutral
train_196678	"In contrast, we model characterbased POS tagging."	"to further investigate the robustness of our model, we conduct experiments with different levels of corrupted tokenization in English."	neutral
train_196763	"The PR curves of several models in previous work and our best model PCNN+ATT RA+BAG ATT are compared in Fig. 4, where Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011) and MIMLRE (Surdeanu et al., 2012) are conventional feature-based methods, and (Lin et al., 2016) and  are PCNN-based ones 4 ."	table 5 compares the AUC values reported in these two papers and the results of our proposed models.	neutral
train_197986	"Since the script and sequence structure of English is very different from these low-resource languages, the addition of English to the limited target language training data yields a considerably noisy corpus."	we design a new neural architecture which integrates multi-level adversarial transfer into a Bi-LSTM-CRF to improve low-resource name tagging.	neutral
train_197987	"The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018)."	"to explore the impact of the size of target language annotations, we use 0, 10%, 50%, or 100% annotated training data from Amharic."	neutral
train_198082	"The resulting cross-lingual embeddings can be used to share supervision for lexical classification tasks across languages, when annotated data is not available in one language."	"even for our development language German, results suggest that there is room for improvement."	neutral
train_198563	"In cases where the whole set of passengers passed through either the west gate or the east gate, this sentence would not be felicitous.'"	"""Both ken and Naomi came."""	neutral
train_198564	"Another partitioning use of ""ka"" is found in embedded questions as shown in (7):"	"in a non-indicative statement such as a generic statement, we find determinacy of the truths of related propositions and disjunction gets ""and""-reading."	neutral
train_198746	"While there are expressions which were strongly associated with either female (e.g. ofisu-no hana) or male (e.g. ookami), there were also a number of expressions for which the traditional interpretation now appears to have been neutralized or moved into an indeterminate category."	"for example, having four legs, tails, beaks, wings, and so forth are some salient properties of animals, but they are not mapped onto the concept of woman."	neutral
train_198956	The specific CA technique used in this study is an agglomerative hierarchical method.	"for languages with little morphology, a different approach is necessary."	neutral
train_202435	"In this paper, we provide alternative methods to cope with those problems within our user interaction environment."	"the customization process includes construction of translation resources specialized in scientific papers, modification of the engine to reflect the linguistic characteristics of academic papers."	neutral
train_202917	4 Notable exception is the phenetics-oriented phonology advocated by J. Pierrehumbert and her colleagues.	the situation is changing.	neutral
train_202912	"Admittedly, such a view was not traditional either in linguistics or in related fields in cognitive science."	"clearly, it needs to be examined."	neutral
train_203069	"On the other hand, husinsya da 'is a suspect' in (17) concerns the speaker's judgment."	"as a categorical judgment, this draws attention first to the neko 'cat', and then says of the neko that it is sleeping there."	neutral
train_203456	Many sentiment words are not included in the current Chinese sentiment dictionaries.	"we can see that the agreement between annotators is not high, this is because of some words are really ambiguous whether to express sentiment."	neutral
train_205683	We also noticed that the lower recall produced the lower F-score for those dialogue acts which are hard to detect.	"4 We ran 15-fold cross validation, using our 15 dialogues."	neutral
train_206249	"A Trale grammar can be distributed between an arbitrary number of files, different files containing sets or subsets of linguistic generalizations of a certain type."	"in the following exposition, we often use simplified structures for purposes of illustration in order to ease understanding by non-Chinese speakers."	neutral
train_206579	"To measure the similarities between the transliterated word w x and target candidate word w y , the Dice coefficient (Dice, 1945) is used."	we also extracted sentences that contain out-of-vocabulary words (813 lines) from the test set.	neutral
train_206580	"As described in (Oh et al., 2006), the correspondence-based transliteration model (Oh and Choi, 2002) is also considered as a hybrid approach."	there are 50 (10.00%) and 55 (11.00%) entries contain three or more character errors.	neutral
train_208513	"Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996;Chen and Goodman, 1998;Stolcke, 2002) are being widely used for probability estimation."	most of the other factors are fixed when we discuss one single factor.	neutral
train_208717	"""meridian"": ""The blood vessels, distributed all over the human body and animal body, carry blood everywhere."""	"we choose to analyze the four atypical body parts: è¡€, è‚‰, éª¨, è„ˆ, each of which is defined by the online dictionary compiled by the Ministry of Education, Taiwan (MOE Dictionary) as 1 è¡€ xie ""blood"": ""The red fluid in the veins/vessels of higher organisms, which starts from the heart and circulates throughout the body."	neutral
train_210397	Character and subject intercept as well as subject slope for vowel type were included as random effects.	here arise the two questions: Why was significance only revealed for AOA but not for AOL?	neutral
train_210398	"Regarding the relationship of accent ratings and learning factors, the amount of Cantonese use is proven to be the most determinant indictor for learners' perceived accent, followed by AOA, LOR and Urdu use."	"the accent rating result for the new vowel /ae/ did not clearly support the model, and the acoustic results for new vowel demonstrated that the experienced learners could articulate /ae/ in the same way as English natives did, unlike the inexperienced learners."	neutral
train_210578	"We are given a general purpose dependency parser PAR (e.g., Stanford Parser), a large-scale corpus CORP, and a contend word W."	"learners can discover the patterns of a word from the use of language by themselves (cf. Data-driven learning (Johns, 1991))."	neutral
train_210579	Typical linguistic search engines such as COCA and COCT usually fashion a bottom-up approach by providing a wealth of examples for inductive learning.	"This study set out to develop an interactive writing assistant which provides Chinese as second language (CSL) learners with pattern grammar (cf. (Hunston, 2000) for English pattern grammar) of a Chinese word, corresponding examples, and the frequency, helping CSL learners to use a word authentically in writing."	neutral
train_211061	The current experimental results are consistent both with the analysis of A-scrambling as movement to SpTP and the hypothesis that HA and PC in Korean can only be controlled by the lower/thematic subject.	the relative impact of WORD ORDER in influencing the acceptability of HA was 0.10 compared to AGREEMENT.	neutral
train_213344	"posts) were classified as positive or negative stance with F-score  (Somasundaran & Wiebe, 2009); when those posts were enriched with preferences learned from the Web, F-score increased to 53%âˆ’75%."	"opinionated Sentence: I don't think you will find anyone who this level of amplification is undamaging, but the option is to not hear."	neutral
train_213346	"In case of noun the root word is the singular form of the plural noun, e.g., bottles becomes bottle, etc."	the data had an inheritably high major class baseline of Accuracy = 70% and F-score = 57%.	neutral
train_214341	"Then, we will retrain Stanford tagger on Arabic tweets since its speed is ideal for tweets domain and it is only the retrainable tagger."	we used Twitter Stream API to crawl Twitter by setting a query to retrieve tweets from the Arabian Peninsula and Egypt by using latitude and longitude coordinates of these regions since Arabic dialects in these regions share similar characteristics and they are the closest Arabic dialects to MSA.	neutral
train_214340	"Concatenation In this classification, two or more words were connected to each other to form one token."	mADA has one exception to this rule.	neutral
train_214991	Section 5 describes and evaluates the algorithm to build the hierarchy of topically focused fragments.	"burst analysis is relevant in the context of hierarchical topic segmentation, but an appropriate way to exploit it has to be proposed; we address this open issue in the following section."	neutral
train_215111	"Hence, having assumed that paraphrased pairs would share the same content and similar syntactic structures, we decide to choose the Microsoft Research Paraphrasing Corpus (Dolan et al., 2005) which contains 5,800 sentence pairs extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship."	"compared to the baseline, the contribution of syntactic structure is not significant to the overall performance."	neutral
train_215635	(baseline) I have not been right to realise that I am so old.	the other class probabilities can contain additional information describing the input data samples differently even when the samples are in the same class.	neutral
train_215632	"Recently, several approaches have been proposed to make more efficient word embedding matrices, usually based on contextual information (SÃ¸gaard et al., 2017; Choi et al., 2017)."	"while many methods have been proposed to learn more efficient representation, knowledge distillation from pretrained deep networks suggest that we can use more information from the soft target probability to train other neural networks."	neutral
train_215840	"While lexical similarity is an important factor in linguistic variation, we would argue that it does not capture all the translationally relevant features of texts."	reducing a functional vector to just the strongest component would be unfair to the functionally hybrid texts that fall under the genre labels of non-academic and editorial in our BNC slice.	neutral
train_215983	"Also called frequency based, it provides a sparse representation of corpus of documents as a matrix."	"while considering the highest F1 score, we got 93.33 percent from the combination of tf-idf vectorizer and unigram model."	neutral
train_215982	"According to official test sets' result, their model ranked first in phraselevel and second in message-level tasks."	"recently, deep learning is widely used to classify the text as it is capa-ble of extracting public opinion regarding a specific topic and also works excellently with highlevel features."	neutral
train_217593	"Some attempts have been made for improving these results (Oh et al., 2009;Yamada et al., 2009)."	"since it is expected that terms involved in the relation share the same lexical field, we also consider the cosine similarity between the term vectors."	neutral
train_218178	"Then, to introduce distractors, we sample a random nominal from a unigram noise distribution."	"according to his model, the predicate embeddings are distributed as: where W is a K Ã— d matrix of weights and Ïƒ V is a hyperparameter."	neutral
train_218511	Please note that a direct comparison with IKRL (Paper) is not possible since we do not have access to the same set of negative samples.	such a representation suffers from limited discriminativeness and can be considered a main source of error for different KG inference tasks.	neutral
train_218649	"The SemeEval task description paper (Laparra et al., 2018b) has more details on dataset statistics and evaluation metrics."	"there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from."	neutral
train_219480	We propose a new method for unsupervised tagging that finds minimal models which are then further improved by Expectation Maximization training.	"this data was created by semi-automatically converting the Penn treebank to CCG derivations (Hockenmaier and Steedman, 2007)."	neutral
train_219501	"This could not be applied to the other two disfluency detectors, so we cannot test those differences for significance."	"speakers tend be disfluent in bursts: if the previous word is disfluent, the next word is more likely to be disfluent."	neutral
train_220944	It is designed specifically for UMLS and it does not disambiguate two candidates if they are classified into the same semantic category.	"in the end, all the selected concept candidates are ranked according to s j i , and a list of ranked concepts is returned for each mention."	neutral
train_221884	"We find that the bottom-up parser and the top-down parser have similar results under the greedy setting, and the in-order parser outperforms both of them."	"the top-down and the bottom-up parsers incorrectly recognize the ""This has both made investors uneasy"" as a complete sentence."	neutral
train_222921	"Several studies have focused on learning joint input-label representations grounded to word semantics for unseen label prediction for images Socher et al., 2013;Norouzi et al., 2014;Zhang et al., 2016;Fu et al., 2018), called zero-shot classification."	proposed hierarchical recurrent neural networks and showed that they were superior to CNNbased models.	neutral
train_115	"In this paper, we focus specifically on the resolution of a linguistic problem for Spanish texts, from the computational point of view: zero-pronouns in the ""subject"" grammatical position."	"the aim of this paper is not to present a new theory regarding zeropronouns, but to show that other algorithms, which have been previously applied to the computational resolution of other kinds of pronoun, can also be applied to resolve zeropronouns."	reasoning
train_120	"Since the results provided by other works have been obtained on different languages, texts and sorts of knowledge (e.g. Hobbs and Lappin full parse the text), direct comparisons are not possible."	"in order to accomplish this comparison, we have implemented some of these approaches in SUPAR."	reasoning
train_443	"Some evaluation data may contain anaphors which are more difficult to resolve, such as anaphors that are (slightly) ambiguous and require real-world knowledge for their resolution, or anaphors that have a high number of competing candidates, or that have their antecedents far away both in terms of sentences/clauses and in terms of number of ""intervening"" NPs etc."	"we suggest that in addition to the evaluation results, information should be provided in the evaluation data as to how difficult the anaphors are to resolve."	reasoning
train_2453	"There are four basic phrases in Korean: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP)."	chunking by rules is divided into largely four components.	reasoning
train_2451	"Since chunking is usually processed in the earlier step of natural language processing, the errors made in this step have a fatal influence on the following steps."	the exceptions that are ignored by the rules must be compensated for by some special treatments of them for higher performance.	reasoning
train_2753	"The challenge then is to determine a ""good"" degree of conservativeness."	we can design a parametric anaphoricity model whose conservativeness can be adjusted via a conservativeness parameter.	reasoning
train_3330	" Based on our data analysis, there is no observable difference in linguistic expressions involving frequently mentioned vs. occasionally occurring person names."	the use of frequently mentioned names in the corpus construction process does not affect the effectiveness of the learned model to be applicable to all the person names in general.	reasoning
train_3851	Most of the CRs point out one specific element (with only a minority being independent as shown in Table 5).	"in task-oriented dialogues, CRs locate the understanding problem directly and give partial credit for what was understood."	reasoning
train_5330	The English key phrase for which we are looking for the translation sometimes contains several words that may appear in a dictionary as an independent unit.	"it can only be partially matched based on the phonetic similarity, and the rest part may be matched by the semantic similarity in such situation."	reasoning
train_7628	"Every verb has as many valency frames as it has meanings (t-manual, page 105)."	"the query language has to be able to distinguish valency frames and search for each one of them, at least as long as the valency frames differ in their members and not only in their index."	reasoning
train_9222	"ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU hypotheses are passed to the dialogue manager."	"the number of MDP states in the dialogue manager grows by up to three times at each step, before pruning."	reasoning
train_9329	"If so, near matches for justification sentences may still be useful in indicating that, at least, the right portion of the document was identified."	"to test for near matches, we scored a match if the gold sentence occurred on either side of the system-selected sentence."	reasoning
train_10393	"On our Chinese-English corpus, we find that 38% of minimal frontier tree pairs are not pairs of minimal frontier trees."	we have to first collect all frontier tree pairs and then decide on the minimal ones.	reasoning
train_12034	"While it measures how well the sets of words that tend to co-occur with these two words align to each other, its strength may lie in bypassing the question of which word in one language should be aligned to a certain context word in the other language."	"unlike other scoring methods, it is not effected by incorrect alignments."	reasoning
train_12027	Dictionaries published by a single publishing house tend to partition the semantic fields of headwords in the same way.	"the first translation of some English headword in the English-Spanish and in the English-Hebrew dictionaries would correspond to the same sense of the headword, and would therefore constitute translations of each other."	reasoning
train_12028	Note that a word's signature includes words in the same language.	two signatures of words in different languages cannot be directly compared; we compare them using a lexicon L as explained below.	reasoning
train_12312	"Like Kylin, LUCHS creates training data by matching Wikipedia attribute values with corresponding sentences, but by itself, this method was insufficient for accurate extraction of most relations."	"lUCHS introduces a new technique, dynamic lexicon features, which dramatically improves performance when learning from sparse data and that way enables scalability."	reasoning
train_12957	Other syntactic knowledge that may be helpful for discourse resolution could also be implicitly represented in the tree.	"by comparing the common sub-structures between two trees we can find out to which level two trees contain similar syntactic information, which can be done using a convolution tree kernel."	reasoning
train_13560	"Unfortunately, only 8 of the 48 sentences in the corpus met this criterion."	"we made single-word changes to 25 more of the sentences (mostly changing proper names and rare nouns) to produce a total of 33 sentences to read, for which every bigram did occur in the BNC."	reasoning
train_15210	"However, since the size of the corpus is small, some topics only contain a few conversations, and some words only appear in one conversation even though they are topicrelevant."	the current PMI measure cannot properly measure a word's and a sentence's topic relevance.	reasoning
train_15304	"In addition to these distance-dependent features, we also include indicator features that fire on bigrams s a jâˆ’1 , s a j and their word classes."	"this feature can capture our intuition that, e.g., adjectives are more likely to come before or after a noun in different languages."	reasoning
train_15502	A major limitation of many of these systems is that they fail to distinguish between noun phrases and the underlying concepts they refer to.	"a polysemous phrase like ""apple"" will refer sometimes to the concept Apple Computer (the company), and other times to the concept apple (the fruit)."	reasoning
train_18441	"Furthermore, intuitively, there are some general syntactic relationships or patterns between topic and sentiment words across different domains."	"if we can mine the patterns from the source and target domain data, then we are able to construct an indirect connection between topic words across domains by using the common sentiment words as a bridge, which makes knowledge transfer across domains possible."	reasoning
train_20010	The poor performance of our system in this case can be explained by the fact that the coreference resolution system regroups more entities in Encyclopedia Britannica documents than in Britannica Elementary ones.	"the number of entities that are ""shared"" by two sentences increases more importantly in the Encyclopedia Britannica corpus, while the distance between two occurrences of one entity decreases in a more significant manner."	reasoning
train_20211	"Categories are annotated over the graph's edges, and represent the descendant unit's role in forming the semantics of the parent unit."	"the internal structure of a unit is represented by its outbound edges and their categories, while the roles a unit plays in the relations it participates in are represented by its inbound edges."	reasoning
train_22560	"For example, the two sentences in Figure 1 intuitively share the same argument structures, but this overlap can only be identified if the prepositional phrase, ""at 5 o'clock,"" is treated as a modifier."	representing the argument/modifier distinction can help the learner find useful argument structures which generalize robustly.	reasoning
train_22561	"Importantly, this makes the learning problem for our model less sparse than for TSGs; our model can derive the trees in a corpus using fewer types of elementary trees than a TSG."	the distribution over these elementary trees is easier to estimate.	reasoning
train_23013	"Training by SVM is performed to find the optimal hyperplane consisting of SVs, and only the SVs affect the performance."	"if some training document reduces the overall performance of text classification because of an outlier, we can assume that the document is a SV."	reasoning
train_23902	Solid curves and dotted lines respectively mean semantic relations and opinion relations between two candidates.	"to resolve these two problems, we present a novel approach with graph co-ranking."	reasoning
train_26503	"An interesting finding is that, although limited in length, the textual information in the search result’s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance."	it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done.	reasoning
train_26666	"Moreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus."	"we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained."	reasoning
train_30118	Retrieving semantically equivalent questions is a challenging task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer.	"traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence."	reasoning
train_30235	"We observed that the rank-frequency distribution of the elements follows the Zipf's law (Manning and SchÃ¼tze, 1999)."	"we rank the templates according to their frequency, and adopt a dominating set algorithm (Johnson, 1974)  We believe that human perception of an emotion is through recognizing important events or semantic contents."	reasoning
train_30234	"Next, named entities (NEs) have been shown to improve the performance of identifying topics (Bashaddadh and Mohd, 2011)."	"we utilize Wikipedia to semi-automatically label NEs with their semantic classes, which can be considered as a form of generalization."	reasoning
train_31950	Reusing the search tree is especially effective given that the tree policy causes us to favor areas of the search space with high value.	"when we transition to the state with highest value, it is likely that many useful actions have already been explored."	reasoning
train_32097	"We use the latter approach, obtaining DCS trees by rule-based conversion from universal dependency (UD) trees (McDonald et al., 2013)."	"nodes in a DCS tree are content words in a UD tree, which are in the form of lemma-POS pairs (Figure 3)."	reasoning
train_32669	"As the upper part of Table 5 shows, a substantial number of B (begin) features look to the left, and a number of E (end) features look to the right."	"these features do not look at the quotation itself, but at its immediate external context."	reasoning
train_33172	"Any word like ""no"", ""not"", or ""none"" counts as a negation unless it begins a non-negation phrase like ""no doubt"" or ""not only""."	the count of negations in the prompt part in Figure 1 is 0.	reasoning
train_35407	"The memberships of multiple subreddits on the same topic (e.g., r/science and r/askscience) often do not overlap considerably."	"we hypothesize that users of Reddit have preferred interactional styles, and that participation in subreddit communities is governed not only by topic interest, but also by these interactional preferences."	reasoning
train_35760	"After model training and selection on the training and development sets, we use the resulting NMT model to translate the test set."	the visualization examples in the following subsections are taken from the test set.	reasoning
train_37932	BPE and the unigram language model share the same idea that they encode a text using fewer bits with a certain data compression principle (dictionary vs. entropy).	we expect to see the same benefit as BPE with the unigram language model.	reasoning
train_38338	"For each ""3"" in the intermediate form, it is unclear which ""3"" in the input to be copied."	we may derive multiple intermediate forms with spurious ones for a training problem.	reasoning
train_39125	Experimental results show that this method achieves much lower results than explicitly removing emotional words based on discrete attention weights.	we do not choose this method in our work.	reasoning
train_42191	"As the sentence length increases, parsing times of supertagging together with chart constraints grows much more slowly than the other methods."	we can expect the relative speedup to increase for corpora of longer sentences.	reasoning
train_42965	"In this sense, at each time step new information is processed and previous information still exists but is 'diluted' in the hidden state (due to the forget gate)."	"as some local interactions that are more informative, e.g. revealing a sharp tone or sheer alteration of facial expressions, are input to ABS-LSTM, the produced states should be given more importance over others since they have just synthesized an informative interaction and not yet been ‘diluted’"	reasoning
train_44450	"If the rare words are not initialized properly, they would also bias the whole word embeddings."	"we more incline to make character-level embedding to represent a rare word, and build CWE embeddings for others."	reasoning
train_44437	The pinyin as the official romanization representation for Chinese provides a solution that maps Chinese character to a string of Latin alphabets so that each character has a letter writing form of its own and users can type pinyin in terms of Latin letters to input Chinese characters into a computer.	converting pinyin to Chinese characters is the most basic module of all pinyinbased IMEs.	reasoning
train_44529	"To the best of our knowledge, this task has not been studied previously."	"there is no existing annotated corpus of paired sentences that can be used as ""ground truth."""	reasoning
train_47596	Document classification models often use feature representations that are derived from words.	"variations in word usage across time will change the distribution of features over time, which can impact the stability of document classifiers (Huang and Paul, 2018)."	reasoning
train_47634	"Until now, no mature and fair criterion to compare Bayes test and NHST exists."	" in this study, an objective comparison between them is not provided. "	reasoning
train_47681	"Coreference resolution is a mid-step for text understanding in downstream tasks, e.g., question answering, text summarization, and information retrieval."	generalization is an important property for coreference resolvers because downstream datasets are not necessarily from the same domain as those of coreference-annotated corpora.	reasoning
train_47691	Maximum spans are recoverable given the MINA spans and their corresponding parse trees.	we can use MINA spans for training and testing coreference models and then retrieve their corresponding maximum spans for evaluation.	reasoning
train_48002	"This is because TVQA is composed of consecutive image frames captured within a short time interval, which tend to contain redundant information."	the value network of the actor-critic model fails to estimate good value of the given state since deleting a good frame will not result in the loss of QA accuracy.	reasoning
train_48001	"In addition to the video frames, the dataset also provides subtitles for each video frame."	solving the questions requires compositional reasoning capability over both a large number of images and texts.	reasoning
train_48529	"Pertaining to the models benchmarked by us, we found that our re-implementation of ASR (Ours) leaves a lot to be desired."	our proposed IAL-CPG model almost doubles the score on all metrics compared to ASR (Ours).	reasoning
train_49778	"We represent these additional features as a vector of additional supervision tokens or side constraints (Sennrich et al., 2016)."	"we construct a vector for each set of features, and concatenate them to the end of each attributevalue pair, encoding the full sequence as for BASE above."	reasoning
train_50232	"Our ""good user"" reflects a realistic user behavior pattern: identify a mixed category topic and apply refinements to focus the topic on its most dominant category."	"the ""good user""-with access to true document categories-first chooses a topic associated with multiple categories of documents and determines the dominant category of the top documents for the topic."	reasoning
train_52033	"For target objects that are linguistically salient, this will typically lead to a reduction of the distractor set."	distinguishing graphs for these target objects will generally be smaller than those for nonsalient objects.	reasoning
train_52809	"One exception is that we weren't able to reach acceptable agreement on a feature of NPs often claimed to affect ranking, thematic roles (Sidner 1979;Cote 1988;Stevenson, Crawley, and Kleinman 1994); the agreement value in this case was Îº = .35."	we were not able to evaluate ranking functions based on thematic roles.	reasoning
train_52988	"Compared to introductory (e.g., Hopcroft and Ullman 1979) or advanced (e.g., MartÃ­n-Vide, Mitrana, and PÈƒun 2004) books on formal language theory, Kracht's book emphasizes those aspects of formal language theory that are relevant to the study of natural languages, whereas the former do not."	"kracht's book gives a uniform introduction, which currently does not exist at this level, to an important area of mathematical linguistics."	reasoning
train_53256	"Furthermore, even if one could find a way of augmenting tree structures to account for crossed dependencies and nodes with multiple parents, there would have to be a mechanism for unifying the tree structure with the augmentation features."	"in terms of derivational complexity, trees would just shift the burden from having to derive a less constrained data structure to having to derive a unification of trees and features or coindexation."	reasoning
train_53712	"Then, we retrieved all the sentences that contained the two words in at least one of the target patterns."	"we obtained sentences containing the pair of words linked by patterns such as door of car, car's door, car has door, car with four doors, car door, etc."	reasoning
train_53721	"Unfortunately, in our LA Times testing corpus we couldn't find more than ten parts for each of their proposed whole objects."	we are unable to replicate their work using our text collection.	reasoning
train_53713	"In the genitive cluster, for example, there were 18,936 such pattern instances, of which 325 encoded part-whole relations, while 18,611 did not."	"for the genitive cluster we used a training corpus of 27,961 positive examples (325 pairs of concepts in a part-whole relation extracted from corpus ""A"" and 27,636 extracted from WordNet as selected pairs) and 18,611 negative examples (the non-part-whole relations extracted from corpus ""A"")."	reasoning
train_53922	"In the text ""food at the plant,"" the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations."	an algorithm for classifying semantic relations (as in Section 7) should be helpful for word sense disambiguation.	reasoning
train_55512	"On the other hand, ""there is an organization"" does not necessarily imply the existence of company, since organization might stand for some non-profit association, as well."	we conclude that organization does not entail company.	reasoning
train_55503	"Curran and Moens (2002) argue that, generally, informative features are statistically correlated with their corresponding headword."	they suggest that any statistical test used for collocations is a good starting point for improving featureweight functions.	reasoning
train_55522	"These results show that for MI weighting many important features appear further down in the ranked vectors, while for the bootstrapped weighting adding too many features adds mostly noise, since most characteristic features are concentrated at the top ranks."	"in addition to better feature weighting, the bootstrapping step provides effective feature reduction, which improves vector quality and consequently the similarity results."	reasoning
train_55509	"For example, the word company would correspond to the existential statement ""there exists an instance of the concept company in some context."""	"if in some context ""there is a company"" (in the sense of ""commercial organization"") then necessarily ""there is a firm"" in that context (in the corresponding sense)."	reasoning
train_55537	"Yet, some of the words that are distributionally similar to private, like soldier or sergeant, might have occurred with save."	"a WSD system may infer that the co-occurrence save private is more likely for the rescuing sense of save because private is distributionally similar to soldier, which did cooccur with this sense of save in the annotated training corpus."	reasoning
train_55876	"For example, although meld is a blend of melt and weld, the current frequency of the phrase melt and weld may not be as common as the source word co-occurrences for newly coined expressions."	an important step to support further research on blends is to develop a data set of recent neologisms that are judged to be lexical blends.	reasoning
train_57988	"Put differently, we are interested in the tradeoff between relevance and accuracy."	"we repeated the experiments with POS tags predicted by the MADA toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012) 15 (see Table 2, columns 5-7)."	reasoning
train_57990	"It is likely, however, that from a machine-learning perspective, representing similar categories with the same tag, or  taking into account further-away tokens in the sentence, may be useful for learning."	we next experimented with modifying some inflectional features that proved most useful in predicted input.	reasoning
train_60038	"As discussed in Section 5.2, our WLM score extraction method is designed such that element-level scores can be extracted with perfect accuracy from a perfect word alignment."	"the goal of seeking perfect or near-perfect word alignment accuracy is worthwhile because it will necessarily result in perfect or near-perfect scoring accuracy, which in turn is likely to yield classification accuracy approaching that of manually assigned scores."	reasoning
train_62286	"However, it is worth remembering that for these applications, we are primarily interested in the structural descriptions that the grammar assigns to a generated sentence, not in the membership of the sentence per se."	"the universal recognition problem is a more accurate model of parsing than the membership problem, as the latter also admits decision procedures where the grammar is replaced with some other mechanism that may produce no or completely different descriptions than the ones we are interested in."	reasoning
train_62431	What makes the resolution of non-NA anaphora particularly challenging with respect to conventional anaphora resolution or coreference resolution is that precisely those features that make those tasks tractable problems are the ones that are missing in this domain.	"systems that attempt the resolution of non-NA anaphora must attempt to access semantic or discourse-related information, which is not always easily accessible, in order to make resolution decisions."	reasoning
train_65376	"Although the computational time of the exact inference algorithm for loopy network is exponential, we may still be able to run it given the small number of variables that are of interest each time in our case."	we can further check if the performance suffers from the approximation.	reasoning
train_66137	"With a broad coverage grammar, it is possible that the parser could run almost indefinitely on sentences that are difficult to parse."	we set an upper limit on the number of constituents that can be added to the chart before the parser quits.	reasoning
train_66611	"For C2E, Table 8 shows that even with only 9 percent training set, our approach can still make 20 percent absolute word error rate reduction."	"although the experiment are done in different environments, to some extend, Table 7 and Table 8 reveal that the n-gram TM/DOM outperforms other techniques for the case of English/Chinese transliteration/back-transliteration significantly."	reasoning
train_69692	"However, the TCE_DI SW algorithm using s context words occur in the domain lexicon than that of other terms."	new terms are actually ranked higher than other terms in TV_ConSem which explains its higher ability to identify new terms in the low range of N TCList .	reasoning
train_70767	"A closer look at the translation candidates obtained when using LL, the most popular association measure in projection-based approaches, shows that they are often collocates of the reference translation."	"LL may fare better in an indirect approach, like the one in (Daille and Morin, 2005)."	reasoning
train_71747	"Similarly, for example (2), the descriptions of typical murmurs associated with hypertrophic cardiomyopathy (harsh, systolic, and diamond-shaped) may not fall under the same section as the impact of Valsalva maneuver on the murmur (which is a factor used to distinguish hypertrophic cardiomyopathy from aortic stenosis)."	a typical passage retrieved from most reference corpus would cover only a portion of the facts given in the question.	reasoning
train_72183	"With the significant amount of information included in supertags, comparing two sequences of supertags is similar to comparing two syntax trees."	we require a deletion to retain the same sequence of supertags as that of the original sentence in order to ensure grammaticality.	reasoning
train_72350	"However, 'm was not recognized as a form of the word am-itself a form of the verb to be-but rather was analyzed as a distinct verb."	the extractor was modified to detect and fix this particular case.	reasoning
train_73728	"The maintenance of rule-based systems is not a straightforward process since experienced linguists need to be available to provide the system with the proper adjustments (Petasis et al., 2001)."	any adjustment to such systems is labour intensive and time consuming.	reasoning
train_73727	The Arabic gazetteers are rare as well and limited in size.	researchers tend to build their own Arabic linguistic resources in order to train and evaluate Arabic NER systems.	reasoning
train_73722	Rule-based NER systems rely on handcrafted grammatical rules written by linguists.	any maintenance applied to rule-based systems is labour-intensive and time consuming especially if linguists with the required knowledge and background are not available.	reasoning
train_74550	"Due to the narrow decoding space and bad PASs, the comprehensive translation score of PASs' translation candidates would be too low to be utilized in system PAS(BTG)."	numerous PASs are bypassed by the decoder and only a slight improvement is achieved by system PAS(BTG).	reasoning
train_75233	"In this case we define ""long enough n-grams"" as those compose by at least 3 capitalized tokens."	"a web page W is initially represented as the sequence of tokens starting in uppercase, in the order as they appear in the web page."	reasoning
train_75812	The custom FSA applier function that implements only the necessary features was employed for both rule testing and finding the delimiter cohort.	"running time went down to 1.45 seconds (see table 2), a 75% improvement."	reasoning
train_77440	Tree kernels are kernel functions that compute the similarity between two instances of data represented as trees based on the number of common fragments between them.	"the need for explicitly encoding an instance in terms of manually-designed and extracted features is eliminated, while benefitting from a very high-dimensional feature space."	reasoning
train_78277	A major problem of this framework is that it suffers from noises produced by word alignment errors.	"some de-noising methods have been proposed (Kim et al., 2010;Wang and Manning, 2014)."	reasoning
train_79223	This model simply learns word embeddings which are then summed into a single vector and projected to the target image vector.	"this model does not have access to word sequence information, and is a distributed analog of a bag-of-words model."	reasoning
train_79866	"Indeed, all the methods evaluated in Section 3.2 consider that all the occurrences of a word (or lemma) should result in one thesaurus entry, that is, no disambiguation is performed."	one could suspect that the list of distributional neighbors of a polysemic word would be impacted.	reasoning
train_81074	"For instance, the substitution of u for v would not work for sentences that have negations or quantifiers such as 'all' and 'none'."	one cannot replace 'cat' with 'animal' in sentences such as 'all cats are asleep' or 'a cat is not asleep'.	reasoning
train_82673	"The main goal of NCEL is to find a solution for collective entity linking using an end-to-end neural model, rather than to improve the measurements of local textual similarity or global mention/entity relatedness."	"we use joint embeddings of words and entities at sense level (Cao et al., 2017) to represent mentions and its contexts for feature extraction."	reasoning
train_83235	Compression Ratio: The common assumption in compression research is that the system can make the determination of the optimal compression length.	compression ratios can vary drastically across systems.	reasoning
train_83228	Coster and Kauchak (2011a) imposed delete operation on their sentence compression model and improved the performance significantly.	deletion is also very important for abstractive sentence compression task.	reasoning
train_83849	"As mentioned before, the original dataset (Wang, 2017) contains only the sources ""S1: Statement"", ""S2: Metadata"" and ""S3: History"" and we enrich the dataset by adding the fourth source ""S4: Report""."	"we only report combinations with news content (i.e., S1), all sources in the original dataset (i.e., S1+S2+S3) and all sources in the enriched dataset (i.e., S1+S2+S3+S4)."	reasoning
train_83839	"For example, a writer who created plenty of fake news is likely to create more fake news (Del Vicario et al., 2016); while news from authoritative organizations such as governments is less likely to be false."	the availability of multiple sources related to fake news has the great potential to help fake news detection.	reasoning
train_85728	"To avoid words with the same meaning from being taken as distinct words due to these character variants, we have replaced a set of characters with similar function into a single most frequently used character."	"of normalizing character variants in Amharic text , reduction in the number of word types (vocabulary) has been obtained."	reasoning
train_85719	Research in the development of MT has been conducted for technologically favored and economically as well as politically important languages of the world since the 17 th century.	notable progress towards the development and use of MT systems has been made for these languages.	reasoning
train_85816	"Furthermore, the results on row 4 (HMLSTM + Schedule + LN) are better than the reproduction results which also implement the CopyLast feature."	"in the rest of the experiments, we consistently add layer normalization on all layers as well as learning-rate scheduling, and drop CopyLast."	reasoning
train_85994	"However, it is also reasonable to assume that events are heterogeneous in their stance distribution patterns, which might have an impact on classification performance and generalizability across events."	in subsequent analysis different event types should be considered when training MSHMM.	reasoning
train_85981	"For an eventual practical application of our rumour classification system it is also important to consider its performance as a function of time, i.e. how many tweets are necessary to achieve a reasonable classification performance."	we also explore early detection of rumours by confining our systems to the first ten (first five) tweets only during classification.	reasoning
train_86257	"Owing to the uncertain range of optical flow displacement, normalization is applied to constraint the displacements in [0, 255] which is same as gray image."	an optical flow field can be regarded as an image with 10 channels in the motion stream.	reasoning
train_87937	The output of each stage becomes the initial annotation for the following stage.	"our system only defines an explicit initial annotation for the verb-tagging phase: for each proposition, we initially tag only the single token containing the verb as V."	reasoning
train_88963	"However, we found that the efficiency of the phrasebased SMT system described in the previous section would be limited for this task, mainly due to two reasons: the character-based phrase models due to possible unseen phrases in an evaluation corpus, and the character sequence model as all candidate transcriptions confidently belong to the target language."	"to make the phrase-based SMT system robust against data sparseness for the ranking task, we also make use of the IBM Model 4 (Brown et al., 1993) in both directions."	reasoning
train_88955	"Specifically, the affinity between two words is the maximum number of consonant selfsubstitutions, with any substitutions involving the first five consonants counting for five normal substitutions."	"substituting 'b' for 'b' contributes 5 to the score, substituting 'z' for 'z' contributes 1, while operations other than self-subsitutions, and any operations involving vowels, contribute 0."	reasoning
train_90302	"This was not, however, thought sufficient to resolve the global syntactic problem of root attachment where (wrong) statistical preferences could be so strong that even 20 rounds of penalties could not weaken them sufficient to be ruled out."	"root and root attachments supported by the CG trees were fixed in the first pass, without reruns."	reasoning
train_90651	Our statistics show that only 7.6% sequences and less than 1% dependencies in the corpus provided for training are non-projective.	we use a simplified strategy to projectivize an input sequence.	reasoning
train_90657	Syntactic parsing contributes crucially to the overall performance of the joint parsing by providing a solid basis for further semantic parsing.	there is reason to believe that improvement of syntactic dependency parsing can be more influential than that of semantic parsing to the overall improvement.	reasoning
train_91639	"It is also important to note that hedging in scientific writing is a core aspect of the genre (Hyland, 1998), while it is judged to be a flaw which has to be eradicated in Wikipedia articles."	"hedge detection in these genres serves different purposes: explicitly encoding the factuality of a scientific claim (doubtful, probable, etc.)"	reasoning
train_94249	"The price to pay for these speedups is that there may be collisions, so that different features are mapped to the same index, but this is often compensated by the fact that the lower time and memory requirements of the hash kernel enables the use of negative features, that is, features that are never seen in the training set but occur in erroneous hypotheses at training time and can therefore be helpful also at inference time."	"the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010)."	reasoning
train_95724	We assume that it is lexical information that decides what semantic role an opinion holder or opinion target takes.	we built a gold-standard lexicon for verbs that encodes such information.	reasoning
train_95725	"Rather than induce the opinion roles for individual verbs, we group verbs that share similar opinion role subcategorization."	the main task for induction is to decide which type an opinion verb belongs to.	reasoning
train_96992	"Although researchers have developed techniques for acquiring CKB from raw text with patterns (Angeli and Manning, 2013), it has been pointed out that some sorts of knowledge are rarely expressed explicitly in textual corpora (Gordon and Van Durme, 2013)."	"researchers have developed curated CKB resources by manual annotation (Speer et al., 2017)."	reasoning
train_98518	"A key requirement in these systems is the ability to efficiently parse user queries, understand the intent behind each query, and provide adequate responses to users."	many applications such as conversation bots and smart IoT devices has a natural language understanding (LU) service integrated within.	reasoning
train_98530	"At this point, we use each row as the vector representation of the corresponding utterance."	"each representative utterance serves as a dimension in the vector space, allowing utterances with similar neighbors to have similar vector representations."	reasoning
train_100575	"For example, in one particular case, the annotated SearchTerm is ""book stores"", for which the ASR 1-best-based parser returned ""books"" (due to ASR error) as the SearchTerm, while the WCN-based parser identified ""banks"" as the SearchTerm."	the returned results from the search engine using the 1-best-based parser were more relevant compared to the results returned by the WCN-based parser.	reasoning
train_100788	"However, this combined factors into target words which don't exist naturally and bring down translation quality."	the decoding is constrained by decomposing into two translation models; a model with POS-tag phrase pairs only and one which jointly translates POS-tags and surface forms.	reasoning
train_101116	Automatic detection of general relations between short texts is a complex task that cannot be carried out only relying on language models and bag-of-words.	learning methods to exploit syntax and semantics are required.	reasoning
train_101120	"In contrast, discriminative models such as Support Vector Machines (SVMs) have theoretically been shown to be robust to noise and irrelevant features (Vapnik, 1995)."	partially correct linguistic structures may still provide a relevant contribution since only the relevant information would be taken into account.	reasoning
train_103595	"Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items."	this information can be considered as a generalization of all contextual words.	reasoning
train_105842	"For example, with this method, the embeddings for the bigrams shark killer and killer shark would be the same."	an ordered approach is needed.	reasoning
train_106443	"Furthermore, the abbreviation matcher consists of 5 simple match routines and each routine is dedicated to a certain type of abbreviations."	it is conceptually simple and fast.	reasoning
train_107126	"During training, the NE boundaries were provided to the word segmenter; the latter is restricted to enforce word boundaries at each entity boundary."	"at training time, the word boundaries are consistent with the entity boundaries."	reasoning
train_107850	We treat each of these statements as a separate incident; the containment relations among them are beyond the scope of our current goals.	"5 each incident corresponds to a partial description of an outbreak, over a period of time and geographic area."	reasoning
train_108427	"Many key phrases are person and location names, which are phonetically translated and whose written forms resemble their pronunciations."	it is possible to discover these translation pairs through their surface strings.	reasoning
train_108961	"That is, we wish to find a small set of sentences that would lead to the greatest reduction of currently unknown words Finding these sentences is a NP-hard problem because the 0/1 knapsack problem could be reduced to this problem in polynomial-time (Gurari, 1989)."	we developed an approximation algorithm for finding sentences with the maximum word coverage of unknown words (MWC).	reasoning
train_112348	"However, r, s, and t can be positive or negative, and we cannot directly take the log of a negative number."	we represent real numbers as ordered pairs.	reasoning
train_116167	"When it maps to the semantic word network, the title tends to contain the vertices scattered in the graph, while the context of the body part would add more semantically related vertices around the original vertices to strength the relations."	the body part has a higher density than the title part.	reasoning
train_116314	"We assume the selected synset represents the meaning of the original word, and those synonyms in the synset which are not annotated as positives must have a certain degree of mismatch to the context."	"from this example, 'balance', 'residue', 'residuum' and 'rest' are extracted as negatives to test whether our synonym checking method can pick out bad substitutions from a set of words sharing similar or the same meaning."	reasoning
train_116463	"Although the search space is different than that of Och's algorithm, it still relies on one-dimensional line searches to reflect, expand, or contract the simplex."	it suffers the same problems of one-dimensional MERT: feature sets with complex non-linear interactions are difficult to optimize.	reasoning
train_116785	"Generally, for words which have been used in this discourse, the magnitude of probabilities in the 'seen before' distribution are much higher than in the 'discourse new' distribution."	there is a strong bias to classify NPs which match word-for-word as being co-referent.	reasoning
train_117289	Our data does not contain long-term two-sided conversations.	it may not be as useful for evaluating techniques that adapt to past messages or that use the conversation partner's communications.	reasoning
train_118067	"To extend the algorithm to support non-projective trees, we introduce move-right and move-left operations similar to the stack-to-buffer swaps proposed by Nivre (2009) for shift-reduce style parsing."	"instead of attaching a token to one of its neighbors at each step, the algorithm may instead decide to move a token past one of its neighbors."	reasoning
train_118303	This paper builds upon the insight that a large proportion of the variation between lexical items for a given class of words is systematic.	it should be represented once and applied to a small set of basic lexical units.	reasoning
train_121105	"However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons!"	"the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008;VuliÄ‡ et al., 2011)."	reasoning
train_121994	We find ME achieves the highest accuracy.	we use ME as the classification model in our system.	reasoning
train_121985	"Therefore, we can make better corrections using such context information on nodes."	our system corrects grammatical errors on dependency trees directly.	reasoning
train_123163	"Through this experiment, we see that since each type of information has a different character, we need different models to maximize the effectiveness of each type."	the hybrid method with different models can have better performance.	reasoning
train_123483	"With the increasing use of handheld devices, reading device is regarded as an important factor for readability."	"this paper investigates the relationship between readability and reading devices such as a smart phone, a tablet, and paper."	reasoning
train_123486	"Some factors are important to the readability of smart phone, but insignificant to that of paper."	we discover the importance of each readability factor for each device by analyzing the correlations.	reasoning
train_123492	"Readability on mobile devices is not reflected only by the visualization factors, but also by textual factors."	this paper explores the readability factors that reflect the lexical and grammatical complexity of text and are affected by reading devices.	reasoning
train_123617	"Loughran and McDonald (2011) states that a general purpose sentiment lexicon (e.g., the Harvard Psychosociological Dictionary) might misclassify common words in financial texts."	"in this paper, we use a finance-specific lexicon that consists of the 6 word lists provided by (Loughran and McDonald, 2011) as seeds to expand keywords."	reasoning
train_124781	But not all eTargets are equally important.	our first evaluation assesses the performance of extracting the most important eTarget.	reasoning
train_124772	"Joint prediction is critical for our task because it involves multiple, mutually constraining ambiguities (the source, polarity, and target)."	"this work aims at detecting both implicit and explicit sentiments expressed by an entity toward another entity/event (i.e., an eTarget) within the sentence."	reasoning
train_126006	"In contrast, fast align, although very fast, took approximately one hour for one round of training (using five iterations for its log-linear model) on the same corpus."	the additional time required in our approach is quite small and can be ignored compared with the training time of fast align.	reasoning
train_126811	"As a side note, the Lexfunc implementation in DISSECT does not produce a composite representation for 11.5% of the our test data, where a word does not appear as a modifier during training."	"we reimplemented the Lexfunc model and solved the missing training material problem by initializing the matrix for all the words in the dictionary with I + , the identity matrix plus a small amount of Gaussian noise."	reasoning
train_128584	We have achieved reasonable performance that is comparable to similar models reported in the literature with all six datasets.	our implementation is fair and suitable for further study of transfer learning.	reasoning
train_129017	One nice property of our model is that it naturally provides explanations for its predictions: the model identifies rationales and then categorizes documents informed by these.	"if the model classifies a test instance as positive, then by construction the sentences associated with the highest p ij pos estimates are those that the model relied on most in coming to this disposition."	reasoning
train_129994	"Most of these approaches use word-level embeddings to encode entities and predicates, and therefore might suffer from the out-of-vocabulary (OOV) problem when they encounter unseen words during test time."	"they often rely on significant data augmentation from sources such as Paralex , which contains 18 million question-paraphrase pairs scraped from WikiAnswers, to have sufficient examples for each word they encounter (Bordes et al., 2014b; Yih et al., 2014; Bordes et al., 2015)."	reasoning
train_131437	"In addition, the magnitude of gradients is proportional to the dataset's size."	"when the dataset is small enough, ADAGRAD will still work."	reasoning
train_132176	"Sentences provide a natural and powerful document decomposition for search that can be easily learnt as a search step: for all the models and configurations considered, the Sentence score was above 88% correct (Table 3)."	"sentence selection is the easy part of the problem, and the model can allocate more computation (such as the end-word selection Bi-LSTM) to spans likely to contain the answer."	reasoning
train_132650	"Furthermore, there have been no large-scale studies of how these hyperparameters affect the performance of NMT systems."	it remains unclear why these models perform as well as they do or how we might improve them.	reasoning
train_132652	"It has also been observed (Tu et al., 2017) that very large beam sizes, even with length penalty, perform worse than smaller ones."	choosing the correct beam width can be crucial to achieving the best results.	reasoning
train_132651	"While some work (Luong et al., 2015b;Luong and Manning, 2016;Wu et al., 2016) has achieved state-of-the-art results using deep networks, others (Jean et al., 2015;Chung et al., 2016;Sennrich et al., 2016b) have produced similar results with far shallower ones."	"it is unclear how important depth is, and whether shallow networks are capable of producing results competitive with those of deep networks."	reasoning
train_134386	" For instance, change from numeric ‘18’ to word ‘eighteen’ may be a minor change and less crucial for the author to review, as compared to an edit that alters the facts of the document."	"in our work, we focus on automatically inferring the impact/change introduced by edits, and predict the perceived importance of such edits by authors."	reasoning
train_134389	"We hypothesized that the importance of edits would be affected by both the nature of the edits, characterized by the aforementioned categories, as well as the relevance of the sentence to the content of the document."	"we chose features that capture both these aspects and have divided them into two groups, namely, change-related features and relevance-related features."	reasoning
train_134390	"Baselines: To the best of our knowledge, our work is the first that attempts to infer importance/impact of text edits between document versions."	we did not have established baselines to compare against.	reasoning
train_136569	"Annotating metaphors is not an easy task, due to the inherent ambiguity and subjectivity."	"we investigate approaches for annotating novelty in metaphors, before closing the resource gap for token-based annotations by creating a layer of novelty scores."	reasoning
train_139657	"For example, the event (he ate food) gives evidence to the high level category of a restaurant script, while the more specific event (he drank wine) gives more evidence to the lower level category fancy restaurant."	"during encoding, it makes sense to allow each latent to decide which parts of the input to take into consideration, based on its parent latents."	reasoning
train_141805	"Despite their shortcomings, layered sequence labeling model and exhaustive region classification model are complementary to each other."	we can combine them to improve the performance of nested NER.	reasoning
train_142907	"However, when it comes to longer documents, basic attention mechanism may lead to distraction and fail to attend to the relatively salient parts."	"some works focus on designing various attentions to tackle this issue (Tan et al., 2017; Gehrmann et al., 2018)."	reasoning
train_143715	The dialogs with only 0-2 mentioned movies take up a proportion of 62.8% of the whole testing dataset.	it is important for the system to perform high-quality recommendation with only a small number of mentioned movies.	reasoning
train_145590	"In addition to the job description text, the headline, key phrase and category are labeled for each article."	job seekers can easily retrieve the job advertisements they desire by reading them.	reasoning
train_145592	"If the article was to be misclassified as a ""Designer"" category or the key phrase wrongly noted as ""Robotics Engineer,"" an inconsistency among the headline, key phrase and category would occur."	readers would be confused by these inconsistencies.	reasoning
train_146095	"While different dimensions contribute differently to the explained variance, they nonetheless contribute equally to the calculation of inner product (Figure 2, right)."	"the lesser dimensions, with less variance but equal weighting, effectively decreases the discriminative power of the model."	reasoning
train_146125	"Hence, we apply several adjustments to the hyperparameters to accommodate the larger effective batch size of multinode synchronous SGD: Learning rate: The Adam optimizer is scaleinvariant, so the parameters move at the same magnitude regardless the gradient size."	"we linearly scale the learning rate in all multi-node experiments, as suggested by Goyal et al. (2017)."	reasoning
train_148994	"In most cases, RST parsers have been developed on the basis of supervised learning algorithms, which require a high quality annotated corpus of sufficient size."	"research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001)."	reasoning
train_149141	These instructions ensure that the rewrites only resolve conversation-dependent ambiguities.	"we encourage workers to create minimal edits; in Section 5.2, we take advantage of this to use BLEU for evaluating model-generated rewrites."	reasoning
train_149415	"As demonstrated in Table 6, the last two sentences in both human poems (marked as red) echo each other well, while the sentences in machine-generated poems seem more independent."	using their judgments would be unfair.	reasoning
train_149399	"Our objective is not only to make the model generate aesthetic and terse poems, but also keep rich semantic of the original vernacular paragraph."	our model gives users more control power over the semantic of generated poems by carefully writing the vernacular paragraph.	reasoning
train_150726	"For the input of text-to-speech engine, written texts are not appropriate, because unnatural speech might be produced due to difficult words or long compound nouns, which are unsuitable for speech synthesis."	"written texts are automatically converted into spoken texts based on paraphrasing technique [5,6] and then are inputted into speech synthesis."	reasoning
train_150727	Japanese is a head-final language and a predicate is placed at the end of the clause.	each predicate in a sentence can be a discourse-unit boundary.	reasoning
train_150986	"However, terminologies and technical terms often consist of unknown words, and their translations are seldom the combination of each constituent."	the result of direct combination is not very desirable for terminology translation acquisition.	reasoning
train_150989	"However, our problem is to find Chinese equivalent using English term, so we have to cope with how to obtain the correct boundary of Chinese translations."	the issue and the proposed method in this paper are distinctly different with Nagata's.	reasoning
train_150983	"Some skilled users perhaps resort to a Web search engine, but they cannot obtain effective information from a large amount of retrieved irrelevant pages and redundancy information."	it is necessary to provide a system to automatically mine translation knowledge of terms or proper nouns using abundant Web information so as to help users accurately read or write foreign language.	reasoning
train_151304	WWER calculated based on ideal word weights represents IR performance degradation when the ASR result is used as a query for IR.	we must perform ASR to minimize WWER for speech-based IR.	reasoning
train_152287	"For a real article, the matrix would be less sparse than a fake article and so is the case for the reconstructed matrix."	"the statistics mean, median, minimum and maximum computed from the reconstructed matrix have higher values for real articles than a fake articles."	reasoning
train_154367	"As a result, we calculate the score of the WSP for each sense, and choose the sense with the lowest WSP score as appreciated one for the ambiguous concept in the assertion."	"for each ambiguous concept of every ConceptNet assertion, we can assign the appropriate sense to it according to the WSP scores; and the resulted ConceptNet can be used to extend WordNet."	reasoning
train_154352	"However, WordNet-based WSD methods usually achieved lower performance compared to supervised methods, mainly due the fact that the lexical and semantic knowledge contained in WordNet is not sufficient for WSD."	"many methods (see Section2) have emerged to enrich WordNet with the lexical and semantic knowledge for WSD purpose, such as by using Wikipedia, on-line lexicons, domain, and so on."	reasoning
train_155013	But it's difficult to give the order for the sentence base on a corpus without the source documents even for a knowledgeable man.	we believe the source documents can give us some effective and efficient information.	reasoning
train_156104	"One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile."	"the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs."	reasoning
train_156107	"Our assumption is that although a feature which is useful for a test set does not appear in the tuning set, another similar feature may exist."	grouping similar features can alleviate sparsity in this way.	reasoning
train_159953	"The additional constraints ensure that only one virtual edge can be active (12), that the virtual node can only send flow over this active edge (11) and that the total amount of flow sent from the root cannot exceed the size of the selected subgraph (13)."	"if n concepts are selected, n units of flow are sent from the root over the edges of the graph and each selected concept consumes one of them."	reasoning
train_160306	"As we increase the dimension, the algorithm will get more degrees of freedom to model the equality constraint in a better way."	there will be statistically significant changes in the standard deviation.	reasoning
train_160437	"Annotations with more than 1 token were split into a sequence of tokens (e.g., BAB/BUP to BAB, BUP)."	"gesture feature sequences have variable lengths, in the same sense as utterances have variable amount of word tokens."	reasoning
train_160753	"In order to cover more aspects about the topic and to avoid redundancy, we would like to assign high important scores on more diverse chains."	"we utilize DivRank (Mei et al., 2010), a well-known diversified ranking algorithm, for calculating the ranking scores for different chains."	reasoning
train_160757	The diversity of possible essays makes automatic evaluation metrics that count on content overlaps impossible since system outputs can then only be compared with rather limited references.	we leave proper design of automatic metrics as future work and only perform manual evaluation in this study.	reasoning
train_160756	"Preliminary experiments suggest that most elements share high similarity values due to our selection criteria in previous steps, causing simple greedy selection strategy to fail."	we consider a different method: ordering sentences and paragraphs according to its position in the original article.	reasoning
train_164445	"Although example-based querying has many advantages, querying a treebank using XPath directly enhances the query flexibility compared to the example-based approach."	the second way of querying the corpora in GrE-TEL consists of directly formulating an XPath query describing the syntactic pattern the user is looking for.	reasoning
train_165201	"How to Lose Weight is a title of a document which contains 5 step titles, including Keep your own personal food diary and determine your weight loss goals upfront, Have a balanced diet, Avoid skipping meals, Eat food from home, and Learn to love fruit."	5 task-subtask pairs are extracted.	reasoning
train_166130	The exact value of reduction is a matter of question.	a number of experiments were carried out to define the reduction cost that gives the best result.	reasoning
train_166814	"The linguistic patterns in sar-graphs are automatically acquired via a pattern discovery method based on distant supervision (Mintz et al., 2009;Krause et al., 2012)."	sar-graphs can be directly applied to free texts for relation extraction.	reasoning
train_166855	The conducted evaluation showed a different performance of the interlinking method when tested on the resources represented by generic terms (a concept label is usually a common noun or a term in a thesaurus).	it seems that it is more difficult to interlink concepts of a thesauri rather than resources corresponding to named entities.	reasoning
train_167844	"Depending on the population being sampled, participants may have already gone through several similar experiments (for example, students in a psychological research methods course are often encouraged or required to be research participants)."	participants in lab-based behavioral experiments often experience a task that is already somewhat routine.	reasoning
train_167845	The relative paucity of studies that report on the differences between friends and strangers may be due to the fact that the dialogues are quite short (an average of about 10 minutes) and very focused on a straightforward task.	they may be less likely to reflect the characteristics that distinguish the conversations of friends and strangers.	reasoning
train_167946	"However, it is also true that our proposals do not seem to be perfectly in tune with our purposes of supporting personal data management activities that ensures long-life data, or diachronic data sharing, because any ID-resolution system needed for a proposal is difficult to be regarded as a lifelong service."	we decided to reconstruct our research approaches from scratch.	reasoning
train_168411	"Although it uses Lucene as well, it applies different indexing strategies."	it can be a good candidate for performance comparison in the future and may serve as an alternative search backend.	reasoning
train_168748	"As such information can be an answer to different search requests, and there could be yet other information to be searched, we believe annotating the relationship described in the sentences as completely as possible would be useful for a variety of purposes."	"we have decided to annotate relations other than the ones for roles, including discourse-oriented relations such as cause-result and ontological relations such as hypernym-hyponym."	reasoning
train_168744	"In addition, because computers and computational methods can be applied to a wide and ever-widening range of topics, formalizing the frameset for events in the computer science/technology domain is an extremely difficult task."	" instead of precisely defining a frameset for in-domain events, we describe the roles of entities in the form of their mutual relations using a set of general relationships such as method-purpose, system-output, and evaluation result."	reasoning
train_175502	"Next, we look at the languages for which not all genres are covered in the training data."	we can only train classifiers for two (Bulgarian) or three (Persian) of the genres in the test set.	reasoning
train_175651	"Since Wasim uses morphological analysers, if the annotator chose one solution, it will affect multiple tasks at the same time."	we allow the annotator to edit previous tasks without leaving the screen.	reasoning
train_175655	"Obviously, the speed of annotation depends on several factors like text, language, course vs fine-grained tagging, and annotator experience."	reported speed measures should be viewed with caution.	reasoning
train_176597	"When we say that a sentence is 'understood', we mean that the concepts and the conceptual relationships expressed by the sentence are unambiguously identified, and we can make correct inferences and/or responses."	"to achieve natural language understanding, computer systems should know the sense similarity and dissimilarity of words and sentences."	reasoning
train_176605	A lexical word may play different syntactic and semantic functions and ambiguously denote many lexical concepts.	"in E-HowNet, each lexical concept of a word is identified and provided with its sense definition, English translation, part-of-speech."	reasoning
train_176660	The PictureQuest system uses a semantic net based on WordNet (Fellbaum 1998) to expand terms.	a query for car or automobile will retrieve essentially identical results; vehicle will be less accurate but will still retrieve many of the same images.	reasoning
train_176661	"We can assume that the non-Englishspeaking user will, however, recognize phrases in her or his own language, and look them up as phrases where possible."	we can expect at least those multiword phrases that have a dictionary entry to be correctly understood.	reasoning
train_178225	We created our training and test sets by splitting the data for each verb into two parts: 90% for training and 10% for test.	"there are 1025 sentences in the training set and 113 sentences in the test set, and each test set verb has been seen in the training set."	reasoning
train_182392	"Moreover, emphasis is placed on performing well across all documents and across all genres."	"it is important for a research team to be able to evaluate their system using HTER, or at least determine the ranking of the documents according to HTER, for purposes of error analysis."	reasoning
train_183412	"When co-referent text mentions appear in different languages, these techniques cannot be easily applied."	"we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters."	reasoning
train_188329	"In freestyle conversations experimented in this paper, different people would use very different words to express their opinions."	documents in a same topic could be quite different because of the intra-topic variability problem.	reasoning
train_188334	It is very difficult to model the noise signal in a document since it could come from various sources.	"in this paper, we focus on the noise created by the variability among intra-topic documents."	reasoning
train_188333	"However, all those works for document representation paid little attention to the variability of intra-topic documents."	they could hardly solve the intra-topic variability problem in a direct way.	reasoning
train_189179	"Machine translation (Koehn, 2010) lets us discover translation correspondences in bilingual texts, but a word and its translation often do not cover the exact same semantic space: distinct word senses might translate differently (Gale et al., 1992;Diab and Resnik, 2002, among others); semantic relations and associations do not always translate, an important issue when constructing multilingual ontologies (Fellbaum and Vossen, 2012); and words in parallel text might be translated non-literally due to lexical gaps (Santos, 1990;Bentivogli and Pianta, 2000) or decisions of the translator, as becomes clear when comparing multiple translations of the same source text (Bhagat and Hovy, 2013)."	"correct word translations found in parallel corpora exhibit a variety of relations including equivalence, hypernymy, and meronymy."	reasoning
train_192649	"For many applications, bilingual lexicon induction of rare and domainspecific words is of critical importance."	we design a new task to evaluate bilingual word embeddings on rare words in different domains.	reasoning
train_193660	"Compared with other QA data collection tasks, human involvement in FreebaseQA is relatively light since each person only needs to make a one-out-of-three choice instead of composing a question or sentence from scratch."	"using this method, we may significantly reduce the cost of QA data collection."	reasoning
train_194723	"Despite the popularity of back-translation in the previous methods (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b), we argue that it suffers from the low-quality pseudo language pairs."	"in this work, we propose a new paradigm, extract-edit, to address this issue by extracting and editing potential real parallel sentences."	reasoning
train_196019	"In the first layer, we construct k capsules, wherein entries at the same dimension from all feature maps are encapsulated into a corresponding capsule."	each capsule can capture many characteristics among the entries at the corresponding dimension in the embedding triple.	reasoning
train_196671	"However, this is hard to transfer to languages like English and Vietnamese where single characters are less informative and tokens are much longer, resulting in a larger combinatory label space."	we choose a semi-Markov formalization to directly model segments.	reasoning
train_196761	"The pre-trained model with only intra-bag attentions converged within 300,000 steps in our experiments."	the initial learning rate for training the model with inter-bag attentions was set as 0.001.	reasoning
train_196760	It should be noticed that a bag group is one training sample in our method.	the model can also be trained in mini-batch mode by packing multiple bag groups into one batch.	reasoning
train_197515	"Since this does not work optimally, especially for smaller data sets, we have decided to give the neural network this type of information as well."	we combine the advantages of classical feature engineering and neural networks.	reasoning
train_198077	"Also, Vietnamese belongs to a different language family than English."	information relevant to the synonymy-antonymy distinction may be distributed differently in the target language space than in the source language.	reasoning
train_198545	"For example, in Japanese, (i) we can insert indeterminate expressions in the proposition, (ii) we can use disjunction to express indeterminacy of the truth of propositions as shown in (8a) and (8b), and (iii) we can use the disjunctive particle ""ka"" as sentence-final question marker to express indeterminacy of the truth of P, as shown in (8c)."	"we assume that question is essentially partitioning, which is expressed as disjunction in Japanese."	reasoning
train_198557	"The situation R has conjunctive potentials, west-gate-retrospective-potential and east-gate-retrospective-potential."	"the situation P with west-gate-potential is accessible from R, while the situation Q with east-gatepotential is accessible from R, too."	reasoning
train_198736	"In this theory, metaphor is defined as ""a cross-domain mapping in the conceptual system"" (Lakoff [3]: 203)."	"""metaphorical expression"" is used to refer to an individual linguistic expression or ""a surface realization of such a cross-domain mapping"" (203)."	reasoning
train_200663	"As described above, the SJT is intended to measure the ability to understand spoken Japanese on everyday topics and to respond intelligibility at a native-like conversational pace."	the vocabulary and sentence structure used in the SJT reflect common everyday Japanese.	reasoning
train_204908	"Except for the headword of a sentence as a whole, all words in the sentence have their own headword."	the dependency accuracy is measured by obtaining and matching the headwords.	reasoning
train_204935	This means that a number of parallel sentences of the corpora are in free translation relationships.	"the English words used are usually different to the translations of the Japanese words obtained from the Japanese-English dictionary, and the expressions in correct parallel translations might be judged as incorrect and removed from the final extraction results."	reasoning
train_206358	"Since there are no a priori reasons to reject the possibility that more than one system coexists in a language, the issue remains unsettled as to whether these two processes are called for to account for the above examples."	"this paper takes up the issue by investigating data involving (2)k-(2)o in a different light, with the availability of idiomatic interpretations."	reasoning
train_210023	"The semantics of the related words after combination are independent of A and B, but they have a new meaning on the basis of original meaning."	"from a diachronic point of view, the meaning of related compounds is closer to their new meaning after combination in vector space."	reasoning
train_210384	"The results of this project attests whether SLM could account for the sound acquisition of L3, which extends SLM to the field of L3/multilingual research."	the findings from the study would contribute to L3/multilingual acquisition studies and serve as a reference point for Chinese language teaching targeted at NCS students at Hong Kong.	reasoning
train_213339	"However, the authors used a small and imbalanced dataset, i.e., 169 positive and 74 negative sentences."	the data had an inheritably high major class baseline of Accuracy = 70% and F-score = 57%.	reasoning
train_213576	"In many NLP classification applications, the classes are not symmetric and the user has some preference towards a high Precision or Recall of a particular target class."	"appropriate tuning of the model is often necessary, depending on the particular tolerance of the application to false positive or false negative results."	reasoning
train_214332	"There are millions of tweets daily, yielding a corpus which is noisy and informal, but which is sometimes informative."	twitter has become one of the most important social informa-tion mutual platforms.	reasoning
train_215002	"This approach yielded poor results, possibly due to the fact that in the PageRank algorithm the contribution of a node weakens the more outgoing edges it has, and the artifical domain nodes have hundreds of outgoing links."	an alternative strategy was adopted of connecting all synsets within a domain to each other.	reasoning
train_215097	"In order to assess the impact of syntactic structure in the STS task, we not only examine the syntactic structure alone, but also combine it with features learned from the most common approach, bag-of-words."	we use a bag-of-word baseline to evaluate the performance of syntactic approaches.	reasoning
train_215344	"For example, the adjectives that describe feminine in Arabic usually end with (Taa' Marbootah)."	"finding words that end with the Arabic letter Taa' Marbootah such as (Feeling cold), (Feeling sleepy), or (Feeling hungry) in the tweets of a user indicates that the writer is a female."	reasoning
train_215622	"Words in the embedding space have semantic and syntactic similarities, such that two similar words are close in the space."	"when the classification is not correct, the error can be interpreted differently depending on the similarity between the predicted word and the target word."	reasoning
train_215972	Analyzing such huge volumes of data manually could lead to the waste of time and investment.	currently more automated and efficient ways are implemented to solve the problem.	reasoning
train_215975	"Firstly, not all words have sentiment value despite how frequently it is used in the document, namely, frequency of commonly used words could shadow other more significant and sentiment containing words."	to solve this problem term-frequency and inverse-document-frequency method (tf-idf) is widely used.	reasoning
train_218628	"However, we hypothesize that each relation may contain useful information about the others, and training on only one relation inevitably neglects some relevant information."	training jointly on multiple relations may improve performance on one or more relations.	reasoning
train_219793	Neither BLEU nor METEOR directly measure grammatical correctness over long distances and may not correspond perfectly to human judgments.	we supplement automatic evaluation with human evaluation.	reasoning
train_219791	"As noted by recent studies (Mason and Charniak, 2013;Kuznetsova et al., 2013;Jamieson et al., 2010), naturally existing image captions often include contextual information that does not directly describe visual content, which ultimately hinders their usefulness for describing other images."	"to improve the fidelity of the generated descriptions, we explore image caption generalization as an optional pre-processing step."	reasoning
train_220102	"However, if we know that the topic of the document concerns the effects of low temperatures on physical health, then the negative emotional reaction sense should become less likely."	"in this case, knowing the topic helps narrow down the set of plausible senses."	reasoning
train_220940	"However, unlike the biomedical ontologies which have many common attributes and relatively uniform structure, different verb sense inventories vary in format and content: some resources have descriptions or example sentences of the senses, but others only have the names of semantic roles; some have relations between senses but some do not."	the question of what features would be useful in this case could be very different from those proposed in this paper and would require additional research.	reasoning
train_223496	"The major difference is that they directly use absolute positions, whereas ours utilizes relative positions."	"their model needs to re-encode the partial sequence at every step, which is computationally more expensive."	reasoning
